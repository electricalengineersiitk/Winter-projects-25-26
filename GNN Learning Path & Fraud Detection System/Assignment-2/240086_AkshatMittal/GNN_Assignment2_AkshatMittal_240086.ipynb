{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjlbWcXd3j2y"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class LinearSVM:\n",
        "    def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=1000):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.lambda_param = lambda_param\n",
        "        self.n_iters = n_iters\n",
        "        self.w = None\n",
        "        self.b = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Initialize w (zeros) and b (zero)\n",
        "        self.w = np.zeros(n_features)\n",
        "        self.b = 0\n",
        "\n",
        "        # Ensure y values are converted to {-1, 1} if they are essentially {0, 1}\n",
        "        # If y contains 0, it becomes -1. If y is already -1, it stays -1.\n",
        "        y_ = np.where(y <= 0, -1, 1)\n",
        "\n",
        "        # Loop for n_iters\n",
        "        for _ in range(self.n_iters):\n",
        "            # Iterate through every sample x_i, y_i\n",
        "            for idx, x_i in enumerate(X):\n",
        "                # Check the Margin Condition: y_i(w . x_i - b) >= 1?\n",
        "                # Note: The formula in the image is explicitly (w . x_i - b)\n",
        "                condition = y_[idx] * (np.dot(x_i, self.w) - self.b) >= 1\n",
        "\n",
        "                if condition:\n",
        "                    # If True (Correct & Safe):\n",
        "                    # Update w only based on the regularization term.\n",
        "                    # Gradient of lambda||w||^2 is 2 * lambda * w\n",
        "                    self.w -= self.learning_rate * (2 * self.lambda_param * self.w)\n",
        "                else:\n",
        "                    # If False (Misclassified or Inside Margin):\n",
        "                    # Update w and b based on the gradient of both terms.\n",
        "\n",
        "                    # 1. Gradient for w:\n",
        "                    # derivative of regularization: 2 * lambda * w\n",
        "                    # derivative of hinge loss (1 - y(wx - b)): -y_i * x_i\n",
        "                    # Total gradient = 2 * lambda * w - y_i * x_i\n",
        "                    dw = 2 * self.lambda_param * self.w - y_[idx] * x_i\n",
        "                    self.w -= self.learning_rate * dw\n",
        "\n",
        "                    # 2. Gradient for b:\n",
        "                    # The hinge loss term inside max is (1 - y_i * w * x_i + y_i * b)\n",
        "                    # Derivative with respect to b is +y_i\n",
        "                    db = y_[idx]\n",
        "                    self.b -= self.learning_rate * db\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Compute the linear output: f(x) = w . X - b\n",
        "        linear_output = np.dot(X, self.w) - self.b\n",
        "\n",
        "        # Return the sign of the result (-1 or 1)\n",
        "        return np.sign(linear_output)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# 1. Define Sigmoid Activation Function and its Derivative\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# 2. XOR Dataset\n",
        "X = np.array([[0,0], [0,1], [1,0], [1,1]]) # Inputs\n",
        "y = np.array([[0], [1], [1], [0]])         # Correct Answers (Labels)\n",
        "\n",
        "# 3. Initialize Weights and Biases randomly\n",
        "np.random.seed(42) # For consistent results\n",
        "\n",
        "# Weights from Input to Hidden Layer (2 inputs -> 2 hidden neurons)\n",
        "weights_input_hidden = np.random.uniform(size=(2, 2))\n",
        "bias_hidden = np.random.uniform(size=(1, 2))\n",
        "\n",
        "# Weights from Hidden to Output Layer (2 hidden -> 1 output neuron)\n",
        "weights_hidden_output = np.random.uniform(size=(2, 1))\n",
        "bias_output = np.random.uniform(size=(1, 1))\n",
        "\n",
        "learning_rate = 0.5\n",
        "epochs = 10000\n",
        "\n",
        "# 4. Training Loop (Forward + Backward Pass)\n",
        "for i in range(epochs):\n",
        "\n",
        "    #  Forward Pass\n",
        "    # Hidden Layer Calculation\n",
        "    hidden_layer_input = np.dot(X, weights_input_hidden) + bias_hidden\n",
        "    hidden_layer_output = sigmoid(hidden_layer_input)\n",
        "\n",
        "    # Output Layer Calculation\n",
        "    output_layer_input = np.dot(hidden_layer_output, weights_hidden_output) + bias_output\n",
        "    predicted_output = sigmoid(output_layer_input)\n",
        "\n",
        "    # Backward Pass (Backpropagation)\n",
        "\n",
        "    # Calculate Error\n",
        "    error = y - predicted_output\n",
        "\n",
        "    # Calculate Gradients (How much to change weights)\n",
        "    # 1. Output Layer Gradient\n",
        "    d_predicted_output = error * sigmoid_derivative(predicted_output)\n",
        "\n",
        "    # 2. Hidden Layer Error Contribution\n",
        "    error_hidden_layer = d_predicted_output.dot(weights_hidden_output.T)\n",
        "    d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_layer_output)\n",
        "\n",
        "    # --- Update Weights (Gradient Descent) ---\n",
        "    weights_hidden_output += hidden_layer_output.T.dot(d_predicted_output) * learning_rate\n",
        "    bias_output += np.sum(d_predicted_output, axis=0, keepdims=True) * learning_rate\n",
        "\n",
        "    weights_input_hidden += X.T.dot(d_hidden_layer) * learning_rate\n",
        "    bias_hidden += np.sum(d_hidden_layer, axis=0, keepdims=True) * learning_rate\n",
        "\n",
        "# 5. Final Prediction Test\n",
        "print(\"Final Hidden Weights:\\n\", weights_input_hidden)\n",
        "print(\"Final Output Weights:\\n\", weights_hidden_output)\n",
        "print(\"\\nPredictions after training:\")\n",
        "print(predicted_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWZvRVLt5TJd",
        "outputId": "ba5e18b8-e7f0-4811-c3a7-fd34e4f5a24e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Hidden Weights:\n",
            " [[4.59244504 6.47246975]\n",
            " [4.5971031  6.49153682]]\n",
            "Final Output Weights:\n",
            " [[-10.32676834]\n",
            " [  9.62121009]]\n",
            "\n",
            "Predictions after training:\n",
            "[[0.01890475]\n",
            " [0.98371361]\n",
            " [0.98369334]\n",
            " [0.01686123]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RUh_lJI9EWD0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}