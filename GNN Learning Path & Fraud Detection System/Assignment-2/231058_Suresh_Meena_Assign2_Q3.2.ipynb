{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IAC86PCF1Omu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class MLP_XOR:\n",
        "    def __init__(self, learning_rate=0.1, n_iters=10000):\n",
        "        self.lr = learning_rate\n",
        "        self.n_iters = n_iters\n",
        "\n",
        "        # Network architecture:\n",
        "        # 2 input → 2 hidden → 1 output\n",
        "        self.W1 = np.random.randn(2, 2)      # Input to hidden\n",
        "        self.b1 = np.zeros((1, 2))\n",
        "        self.W2 = np.random.randn(2, 1)      # Hidden to output\n",
        "        self.b2 = np.zeros((1, 1))\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        return x * (1 - x)   # derivative of sigmoid output\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        y = y.reshape(-1, 1)\n",
        "\n",
        "        for _ in range(self.n_iters):\n",
        "\n",
        "            # ------------ Forward Pass ------------\n",
        "            z1 = np.dot(X, self.W1) + self.b1\n",
        "            a1 = self.sigmoid(z1)\n",
        "\n",
        "            z2 = np.dot(a1, self.W2) + self.b2\n",
        "            a2 = self.sigmoid(z2)    # final output\n",
        "\n",
        "            # ------------ Backward Pass ------------\n",
        "            error = a2 - y\n",
        "\n",
        "            # Output layer gradients\n",
        "            d_a2 = error * self.sigmoid_derivative(a2)\n",
        "            d_W2 = np.dot(a1.T, d_a2)\n",
        "            d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
        "\n",
        "            # Hidden layer gradients\n",
        "            d_a1 = np.dot(d_a2, self.W2.T) * self.sigmoid_derivative(a1)\n",
        "            d_W1 = np.dot(X.T, d_a1)\n",
        "            d_b1 = np.sum(d_a1, axis=0)\n",
        "\n",
        "            # ------------ Update Weights ------------\n",
        "            self.W1 -= self.lr * d_W1\n",
        "            self.b1 -= self.lr * d_b1\n",
        "            self.W2 -= self.lr * d_W2\n",
        "            self.b2 -= self.lr * d_b2\n",
        "\n",
        "    def predict(self, X):\n",
        "        a1 = self.sigmoid(np.dot(X, self.W1) + self.b1)\n",
        "        a2 = self.sigmoid(np.dot(a1, self.W2) + self.b2)\n",
        "        return (a2 > 0.5).astype(int)\n",
        "\n",
        "\n",
        "# -------------- Testing on XOR --------------\n",
        "if __name__ == \"__main__\":\n",
        "    X = np.array([[0,0],\n",
        "                  [0,1],\n",
        "                  [1,0],\n",
        "                  [1,1]])\n",
        "\n",
        "    y = np.array([0, 1, 1, 0])\n",
        "\n",
        "    mlp = MLP_XOR(learning_rate=0.1, n_iters=10000)\n",
        "    mlp.fit(X, y)\n",
        "\n",
        "    print(\"Predictions:\", mlp.predict(X).flatten())\n"
      ]
    }
  ]
}