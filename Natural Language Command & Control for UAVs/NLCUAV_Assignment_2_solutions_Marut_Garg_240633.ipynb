{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c920f3a",
   "metadata": {},
   "source": [
    "# Natural Language Command & Control for UAV's\n",
    "\n",
    "## Assignment 2 Solutions\n",
    "\n",
    "**Marut Garg (240633)**  \n",
    "**IIT Kanpur â€“ Electrical Engineering**  \n",
    "**December 24, 2025**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3594994",
   "metadata": {},
   "source": [
    "## Q1. Computational Graphs & Gradients\n",
    "\n",
    "**Gradient Accumulation in PyTorch**\n",
    "\n",
    "In PyTorch, gradients are accumulated by default. When we call `loss.backward()`, the computed gradients are **added** to the existing values stored in the `.grad` attribute of leaf tensors. The old gradients are not overwritten automatically.\n",
    "\n",
    "In a standard training loop, `optimizer.zero_grad()` is used to reset all gradients to zero before calculating new gradients. If we forget to do this, gradients from previous iterations will keep accumulating.\n",
    "\n",
    "Mathematically, this means that parameter updates will be based on the sum of gradients from multiple steps, which leads to incorrect updates and unstable training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12baba81",
   "metadata": {},
   "source": [
    "## Q2. Tensors: View vs Reshape\n",
    "\n",
    "Both `.view()` and `.reshape()` are used to change the shape of a tensor in PyTorch.\n",
    "\n",
    "The main technical difference is related to memory layout. The `.view()` method works only when the tensor is stored in a contiguous block of memory. If the tensor is non-contiguous, calling `.view()` will result in an error.\n",
    "\n",
    "On the other hand, `.reshape()` is more flexible. If the tensor is not contiguous, `.reshape()` automatically creates a new contiguous tensor if needed.\n",
    "\n",
    "Therefore, `.reshape()` is safer to use when we are unsure about the memory layout or stride of the tensor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d56853d",
   "metadata": {},
   "source": [
    "## Q3. Device Management (CPU vs GPU)\n",
    "\n",
    "If we try to perform an operation such as addition between a tensor on the CPU and a tensor on the GPU, PyTorch gives an error. This happens because PyTorch does not allow operations between tensors that are on different devices. Both tensors must be on the same device.\n",
    "\n",
    "When a model is moved to the GPU using `model.to('cuda')`, all the model parameters are moved to the GPU. However, any new tensors created inside the `forward` method do not automatically move to the GPU. These tensors must be explicitly created on the same device, otherwise a device mismatch error will occur.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107ea35d",
   "metadata": {},
   "source": [
    "## Q4. Tensor Manipulation (The Image Mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df03c0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def process_images(images):\n",
    "    masked_images = torch.where(images < 0.5, 0.0, 1.0)\n",
    "    return masked_images\n",
    "\n",
    "images = torch.rand(4, 28, 28)\n",
    "output = process_images(images)\n",
    "\n",
    "output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1bde8e",
   "metadata": {},
   "source": [
    "## Q5. The \"Safe\" Autograd (Gradient Checking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f9bc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Initialize x with gradient tracking\n",
    "x = torch.tensor(4.0, requires_grad=True)\n",
    "\n",
    "# Define the function y = x^3 + 2x\n",
    "y = x**3 + 2*x\n",
    "\n",
    "# Backward pass to compute gradient\n",
    "y.backward()\n",
    "\n",
    "# Print gradient computed by PyTorch\n",
    "print(\"Gradient from PyTorch:\", x.grad)\n",
    "\n",
    "# Manual gradient calculation: dy/dx = 3x^2 + 2\n",
    "manual_grad = 3*(4**2) + 2\n",
    "\n",
    "# Sanity check\n",
    "if x.grad.item() != manual_grad:\n",
    "    raise ValueError(\"Gradient does not match manual calculation\")\n",
    "\n",
    "print(\"Manual gradient:\", manual_grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda53f4c",
   "metadata": {},
   "source": [
    "## Q6. Class Interaction (Custom Datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21811868",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NumberDataset(Dataset):\n",
    "    def __init__(self, numbers):\n",
    "        self.numbers = numbers\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.numbers)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_tensor = torch.tensor(float(self.numbers[idx]))\n",
    "        target_tensor = torch.tensor(float(self.numbers[idx] * 2))\n",
    "        return input_tensor, target_tensor\n",
    "\n",
    "dataset = NumberDataset([1, 2, 3, 4])\n",
    "\n",
    "print(len(dataset))\n",
    "print(dataset[1])\n",
    "print(dataset[3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b7b8a8",
   "metadata": {},
   "source": [
    "## Q7. Model Encapsulation (nn.Module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342305c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(10, 5)\n",
    "        self.fc2 = nn.Linear(5, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "model = SimpleClassifier()\n",
    "dummy_input = torch.rand(2, 10)\n",
    "output = model(dummy_input)\n",
    "\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56bef7a",
   "metadata": {},
   "source": [
    "## Q8. The Training Loop (Logic Integration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a21b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, inputs, targets, optimizer, criterion):\n",
    "    # Forward pass\n",
    "    predictions = model(inputs)\n",
    "    \n",
    "    # Loss calculation\n",
    "    loss = criterion(predictions, targets)\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Return scalar loss value\n",
    "    return loss.item()\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create model\n",
    "model = SimpleClassifier()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Dummy inputs and targets\n",
    "inputs = torch.rand(4, 10)\n",
    "targets = torch.rand(4, 1)\n",
    "\n",
    "# Run one training step\n",
    "loss = train_step(model, inputs, targets, optimizer, criterion)\n",
    "\n",
    "print(\"Loss:\", loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
