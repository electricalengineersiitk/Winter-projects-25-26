{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 141314,
     "status": "ok",
     "timestamp": 1770655576935,
     "user": {
      "displayName": "Pradeep Meena",
      "userId": "07726394773264331103"
     },
     "user_tz": -330
    },
    "id": "p18NFkxwN18G",
    "outputId": "cf0fc40e-ac74-4e04-f77b-66e94e56a76c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MAML Training...\n",
      "Epoch 0: Loss = 0.3441\n",
      "Epoch 200: Loss = 0.2955\n",
      "Epoch 400: Loss = 0.2968\n",
      "Epoch 600: Loss = 0.3351\n",
      "Epoch 800: Loss = 0.3127\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "# 1. CONFIGURATION & DATA GENERATION\n",
    "\n",
    "# Hyperparameters\n",
    "K_SHOT = 10              # Support set size [cite: 12]\n",
    "Q_QUERY = 10             # Query set size for evaluation\n",
    "INNER_LR = 0.1           # Step size for inner loop\n",
    "META_LR = 0.001          # Step size for outer loop\n",
    "EPOCHS = 2000            # Meta-training epochs [cite: 18]\n",
    "TASKS_PER_BATCH = 32     # Meta-batch size\n",
    "\n",
    "# Dataset Parameters\n",
    "RADIUS = 2.0             # Fixed radius [cite: 8]\n",
    "INPUT_RANGE = [-5, 5]    # Input space limits [cite: 7]\n",
    "CENTER_RANGE = [-3, 3]   # Center uniform sampling range [cite: 8]\n",
    "\n",
    "def get_circle_task():\n",
    "    \"\"\"Sample a random center (cx, cy) from [-3, 3].\"\"\"\n",
    "    cx = np.random.uniform(CENTER_RANGE[0], CENTER_RANGE[1])\n",
    "    cy = np.random.uniform(CENTER_RANGE[0], CENTER_RANGE[1])\n",
    "    return np.array([cx, cy])\n",
    "\n",
    "def sample_points(task_center, num_points):\n",
    "    \"\"\"Generate x in [-5, 5] and labels based on distance to center.\"\"\"\n",
    "    x = np.random.uniform(INPUT_RANGE[0], INPUT_RANGE[1], (num_points, 2))\n",
    "    # Equation: Dist < radius => Label 1, else 0 [cite: 11]\n",
    "    dists = np.sqrt(np.sum((x - task_center)**2, axis=1))\n",
    "    y = (dists < RADIUS).astype(np.float32).reshape(-1, 1)\n",
    "    return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# 2. MODEL DEFINITION (FUNCTIONAL MLP)\n",
    "\n",
    "# We use a functional approach to allow manual gradient updates (MAML requirement)\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        # Architecture: 2 -> 40 -> 40 -> 1 [cite: 16]\n",
    "        self.w1 = nn.Parameter(torch.randn(2, 40) / np.sqrt(2))\n",
    "        self.b1 = nn.Parameter(torch.zeros(40))\n",
    "        self.w2 = nn.Parameter(torch.randn(40, 40) / np.sqrt(40))\n",
    "        self.b2 = nn.Parameter(torch.zeros(40))\n",
    "        self.w3 = nn.Parameter(torch.randn(40, 1) / np.sqrt(40))\n",
    "        self.b3 = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x, params=None):\n",
    "        \"\"\"Allows forward pass using either internal params or updated temporary params.\"\"\"\n",
    "        if params is None:\n",
    "            params = [self.w1, self.b1, self.w2, self.b2, self.w3, self.b3]\n",
    "\n",
    "        w1, b1, w2, b2, w3, b3 = params\n",
    "\n",
    "        x = torch.relu(x @ w1 + b1)\n",
    "        x = torch.relu(x @ w2 + b2)\n",
    "        x = torch.sigmoid(x @ w3 + b3)\n",
    "        return x\n",
    "\n",
    "    def get_params(self):\n",
    "        return [self.w1, self.b1, self.w2, self.b2, self.w3, self.b3]\n",
    "\n",
    "\n",
    "# 3. MAML TRAINING LOOP\n",
    "\n",
    "maml_model = SimpleMLP()\n",
    "meta_optimizer = optim.Adam(maml_model.parameters(), lr=META_LR)\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "print(\"Starting MAML Training...\")\n",
    "for epoch in range(EPOCHS):\n",
    "    meta_loss = 0.0\n",
    "\n",
    "    # Outer Loop\n",
    "    for _ in range(TASKS_PER_BATCH):\n",
    "        task_center = get_circle_task()\n",
    "\n",
    "        # Support Set (K=10)\n",
    "        x_supp, y_supp = sample_points(task_center, K_SHOT)\n",
    "        # Query Set\n",
    "        x_qry, y_qry = sample_points(task_center, Q_QUERY)\n",
    "\n",
    "        # Inner Loop (1 Step) [cite: 19]\n",
    "        # 1. Forward on support\n",
    "        params = maml_model.get_params()\n",
    "        preds = maml_model(x_supp, params)\n",
    "        loss = loss_fn(preds, y_supp)\n",
    "\n",
    "        # 2. Compute Gradients\n",
    "        grads = torch.autograd.grad(loss, params, create_graph=True)\n",
    "\n",
    "        # 3. Manual Update (theta_prime = theta - alpha * grad)\n",
    "        updated_params = [p - INNER_LR * g for p, g in zip(params, grads)]\n",
    "\n",
    "        # 4. Compute Loss on Query set using updated params\n",
    "        qry_preds = maml_model(x_qry, updated_params)\n",
    "        task_loss = loss_fn(qry_preds, y_qry)\n",
    "        meta_loss += task_loss\n",
    "\n",
    "    # Meta-Update [cite: 20]\n",
    "    meta_optimizer.zero_grad()\n",
    "    meta_loss /= TASKS_PER_BATCH\n",
    "    meta_loss.backward()\n",
    "    meta_optimizer.step()\n",
    "\n",
    "    if epoch % 200 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {meta_loss.item():.4f}\")\n",
    "\n",
    "# 4. BASELINE TRAINING (JOINT TRAINING)\n",
    "\n",
    "baseline_model = SimpleMLP()\n",
    "baseline_opt = optim.Adam(baseline_model.parameters(), lr=1e-3)\n",
    "\n",
    "print(\"\\nStarting Baseline Training...\")\n",
    "for epoch in range(EPOCHS):\n",
    "    # Sample batch from mixed tasks [cite: 23]\n",
    "    # We simulate \"mixing\" by generating fresh points from random tasks every step\n",
    "    x_batch = []\n",
    "    y_batch = []\n",
    "    for _ in range(32): # Batch size\n",
    "        c = get_circle_task()\n",
    "        x, y = sample_points(c, 1) # 1 point per random task\n",
    "        x_batch.append(x)\n",
    "        y_batch.append(y)\n",
    "\n",
    "    x_batch = torch.cat(x_batch)\n",
    "    y_batch = torch.cat(y_batch)\n",
    "\n",
    "    baseline_opt.zero_grad()\n",
    "    preds = baseline_model(x_batch) # Uses internal params\n",
    "    loss = loss_fn(preds, y_batch)\n",
    "    loss.backward()\n",
    "    baseline_opt.step()\n",
    "\n",
    "    if epoch % 200 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "# 5. EVALUATION & VISUALIZATION\n",
    "\n",
    "\n",
    "# Test Task Generation\n",
    "test_center = get_circle_task()\n",
    "x_test_supp, y_test_supp = sample_points(test_center, K_SHOT) # Support set\n",
    "x_test_eval, y_test_eval = sample_points(test_center, 100)    # Evaluation set\n",
    "\n",
    "#A. Quantitative: Accuracy vs Steps [cite: 29]\n",
    "steps = list(range(11)) # 0 to 10\n",
    "maml_accs = []\n",
    "base_accs = []\n",
    "\n",
    "# Helper for accuracy\n",
    "def get_acc(model, params, x, y):\n",
    "    preds = model(x, params)\n",
    "    return ((preds > 0.5) == y).float().mean().item()\n",
    "\n",
    "# Evaluate MAML\n",
    "curr_params = [p.clone() for p in maml_model.get_params()]\n",
    "for i in range(11):\n",
    "    maml_accs.append(get_acc(maml_model, curr_params, x_test_eval, y_test_eval))\n",
    "    # Update step\n",
    "    loss = loss_fn(maml_model(x_test_supp, curr_params), y_test_supp)\n",
    "    grads = torch.autograd.grad(loss, curr_params)\n",
    "    curr_params = [p - INNER_LR * g for p, g in zip(curr_params, grads)]\n",
    "\n",
    "# Evaluate Baseline (Fine-tuning) [cite: 25]\n",
    "# Need to copy model to avoid overwriting trained weights\n",
    "base_eval_model = copy.deepcopy(baseline_model)\n",
    "base_params = [p for p in base_eval_model.parameters()]\n",
    "# We need a standard optimizer for baseline fine-tuning or manual updates\n",
    "# To keep comparison fair (SGD), we use manual updates same as MAML\n",
    "for i in range(11):\n",
    "    base_accs.append(get_acc(base_eval_model, base_params, x_test_eval, y_test_eval))\n",
    "    # Update step\n",
    "    # Re-compute loss to get grad for current params\n",
    "    output = base_eval_model(x_test_supp, base_params) # Functional call logic applied here\n",
    "    loss = loss_fn(output, y_test_supp)\n",
    "    grads = torch.autograd.grad(loss, base_params)\n",
    "    base_params = [p - INNER_LR * g for p, g in zip(base_params, grads)]\n",
    "\n",
    "# Plot 1: Accuracy Curve\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(steps, maml_accs, marker='o', label='MAML')\n",
    "plt.plot(steps, base_accs, marker='x', label='Baseline (Pre-trained)')\n",
    "plt.title('Test Accuracy vs Gradient Steps (K=10)')\n",
    "plt.xlabel('Gradient Steps')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "#B. Qualitative: Heatmap [cite: 33]\n",
    "\n",
    "# Create Meshgrid\n",
    "xx, yy = np.meshgrid(np.linspace(-5, 5, 100), np.linspace(-5, 5, 100))\n",
    "grid_tensor = torch.tensor(np.c_[xx.ravel(), yy.ravel()], dtype=torch.float32)\n",
    "\n",
    "# Ground Truth\n",
    "dists = np.sqrt((xx - test_center[0])**2 + (yy - test_center[1])**2)\n",
    "gt_mask = dists < RADIUS\n",
    "\n",
    "# MAML Prediction (After 1 step)\n",
    "# Reset params, take 1 step\n",
    "curr_params = [p.clone() for p in maml_model.get_params()]\n",
    "loss = loss_fn(maml_model(x_test_supp, curr_params), y_test_supp)\n",
    "grads = torch.autograd.grad(loss, curr_params)\n",
    "step1_params = [p - INNER_LR * g for p, g in zip(curr_params, grads)]\n",
    "maml_probs = maml_model(grid_tensor, step1_params).detach().numpy().reshape(xx.shape)\n",
    "\n",
    "# Baseline Prediction (After 1 step)\n",
    "base_params = [p for p in baseline_model.parameters()] # Original pre-trained\n",
    "output = baseline_model(x_test_supp, base_params)\n",
    "loss = loss_fn(output, y_test_supp)\n",
    "grads = torch.autograd.grad(loss, base_params)\n",
    "base_step1_params = [p - INNER_LR * g for p, g in zip(base_params, grads)]\n",
    "base_probs = baseline_model(grid_tensor, base_step1_params).detach().numpy().reshape(xx.shape)\n",
    "\n",
    "# Plot Heatmaps\n",
    "fig, ax = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Ground Truth\n",
    "ax[0].contour(xx, yy, gt_mask, levels=[0.5], colors='k', linestyles='--')\n",
    "ax[0].set_title(\"Ground Truth Boundary\")\n",
    "ax[0].scatter(x_test_supp[:,0], x_test_supp[:,1], c=y_test_supp.flatten(), cmap='coolwarm', edgecolors='k')\n",
    "\n",
    "# MAML\n",
    "c1 = ax[1].contourf(xx, yy, maml_probs, levels=20, cmap='RdBu_r', alpha=0.8)\n",
    "ax[1].contour(xx, yy, gt_mask, levels=[0.5], colors='k', linestyles='--') # GT Reference\n",
    "ax[1].scatter(x_test_supp[:,0], x_test_supp[:,1], c=y_test_supp.flatten(), cmap='coolwarm', edgecolors='k')\n",
    "ax[1].set_title(\"MAML (1 Step)\")\n",
    "\n",
    "# Baseline\n",
    "c2 = ax[2].contourf(xx, yy, base_probs, levels=20, cmap='RdBu_r', alpha=0.8)\n",
    "ax[2].contour(xx, yy, gt_mask, levels=[0.5], colors='k', linestyles='--') # GT Reference\n",
    "ax[2].scatter(x_test_supp[:,0], x_test_supp[:,1], c=y_test_supp.flatten(), cmap='coolwarm', edgecolors='k')\n",
    "ax[2].set_title(\"Baseline (1 Step)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1770655576946,
     "user": {
      "displayName": "Pradeep Meena",
      "userId": "07726394773264331103"
     },
     "user_tz": -330
    },
    "id": "dIibRXkqN5hx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN3fQR2cCeL6X9PTxJpbzad",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
