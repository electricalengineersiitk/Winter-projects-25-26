{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**SUBMISSION INSTRUCTIONS**\n",
        "\n",
        "First make a copy of this colab file and then solve the assignment and upload your final notebook on github.\n",
        "\n",
        "**To write an answer to any question, First Add a TEXT block below the question and type it in only one single block**\n",
        "\n",
        "Before uploading your downloaded notebook, RENAME the file as rollno_name.ipynb\n",
        "\n",
        "Submission Deadline : 19/12/2025 Friday EOD i.e before 11:59 PM\n",
        "\n",
        "The deadline is strict and will not be extended, Late submissions are not allowed\n",
        "\n",
        "Note that you have to upload your solution on the github page of the project Vision Transformer and **under Assignment 3**\n",
        "\n",
        "And remember to keep title of your pull request to be ViT_name_rollno_assgn3\n",
        "\n",
        "Github Submission repo -\n",
        "https://github.com/electricalengineersiitk/Winter-projects-25-26/tree/main/Vision%20transformer/Assignment%203"
      ],
      "metadata": {
        "id": "XmTMSeaTLwVp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**THEORETICAL ASSIGNMENT**\n",
        "\n"
      ],
      "metadata": {
        "id": "-TjKxTBEz8i7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This assignment consists of theoretical questions. Objective answers will not be accepted. You need to write detailed answers (200-300 words) for the questions that require more in-depth explanations (you should be able to identify these after reading the question and finding about them). Questions that need less explanation can be answered in 20-100 words. Please ensure the answers are well-written and thorough."
      ],
      "metadata": {
        "id": "AnVa1_8AKqtv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Assume that the inputs **X**\n",
        " to some scalar function **f**\n",
        " are **n x m**\n",
        " matrices. What is the dimensionality of the gradient of **f**\n",
        " with respect to **X**?\n",
        "**Give reasons to justify your answer.**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ERhYz9qV_YXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The resulting dimension of the gradient matrix will be n x m. If f is a scalar function and X is an n by m matrix, then the gradient of f with respect to X also has dimensions n by m. This is because the gradient consists of all partial derivatives of f with respect to every element in X. For each single element of X, there is exactly one partial derivative term. Therefore, the gradient must contain the same number of elements arranged in the same shape. In simple words, every entry of X contributes one entry in the gradient, so both have identical dimensions."
      ],
      "metadata": {
        "id": "3SkROWQKA07L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Define the evaluation metrics Accuracy, Precision, Recall, and F1 score.\n",
        "\n",
        "   Consider the following diagram:\n",
        "   Find the values of Accuracy, Precision, Recall, and F1 score.\n",
        "   |                | Actual: Cancer | Actual: No Cancer |\n",
        "   |----------------|---------------|------------------|\n",
        "   | Predicted: Cancer | 80 | 80 |\n",
        "   | Predicted: No Cancer | 20 | 820 |\n",
        "    \n",
        "   \n",
        "   Find the values of Accuracy, Precision, Recall, and F1 score using the above data.\n"
      ],
      "metadata": {
        "id": "9vnyYHlq2g4B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy: proportion of total correct predictions out of all predictions.\n",
        "\n",
        "Precision: proportion of correctly predicted positive cases among all predicted positives.\n",
        "\n",
        "Recall: proportion of correctly predicted positive cases among all actual positives.\n",
        "\n",
        "F1 score: harmonic mean of precision and recall, useful when classes are imbalanced.\n",
        "\n",
        "TP = 80, FP = 80, FN = 20, TN = 820\n",
        "\n",
        "Accuracy = 900 / 1000 = 0.9\n",
        "\n",
        "Precision = 80 / 160 = 0.5\n",
        "\n",
        "Recall = 80 / 100 = 0.8\n",
        "\n",
        "F1 score ≈ 0.615"
      ],
      "metadata": {
        "id": "gyfwOAGbBq6w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is a confusion matrix ?"
      ],
      "metadata": {
        "id": "h1dJMD0vJKZg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A confusion matrix is a summary of classification results showing how many predictions were correct and incorrect for each class. It contains counts of true positives, true negatives, false positives, and false negatives. It helps understand not just overall accuracy but also the specific types of mistakes a model makes, such as missing positive cases or falsely labeling negatives as positives. It is especially important in medical and fraud detection tasks where different errors have different consequences."
      ],
      "metadata": {
        "id": "XMexz5AgB4qX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is overfitting and underfitting ?"
      ],
      "metadata": {
        "id": "aOqF8Dzw0DdZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overfitting happens when a model learns training data too closely, including noise. It performs very well on training data but poorly on new unseen data. The model becomes too complex.\n",
        "\n",
        "Underfitting happens when a model is too simple and cannot capture underlying patterns. It performs poorly on both training and testing data. The model has high bias and low predictive power."
      ],
      "metadata": {
        "id": "0uqAACnzB8Gk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Explain vanishing and exploding gradients, give reasons why they occur and how they can be prevented.\n"
      ],
      "metadata": {
        "id": "n4KfKc0z0ulm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vanishing and exploding gradients occur during backpropagation in deep networks.\n",
        "\n",
        "Vanishing gradients happen when gradients become extremely small while moving backward through layers. Early layers then receive almost no updates and learning slows or stops. This often occurs with sigmoid or tanh activations and deep networks where many small derivatives are multiplied.\n",
        "\n",
        "Exploding gradients are the opposite problem, where gradients grow very large. This causes unstable training, very large weight updates, and divergence of loss. It usually happens when weights are large or derivatives repeatedly exceed one.\n",
        "\n",
        "They occur because gradients are multiplied many times through layers.\n",
        "\n",
        "Prevention methods include ReLU activations, proper weight initialization, batch normalization, residual connections, and gradient clipping."
      ],
      "metadata": {
        "id": "DEg0-j9qB-tD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is regularization ? Explain L1 and L2 regularization"
      ],
      "metadata": {
        "id": "6yaOipvU1JoD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regularization reduces overfitting by penalizing large model weights.\n",
        "\n",
        "L1 regularization adds a penalty proportional to the absolute value of weights. It encourages sparsity, meaning many weights become exactly zero. This effectively performs feature selection.\n",
        "\n",
        "L2 regularization adds a penalty proportional to the square of weights. It shrinks weights smoothly without forcing them to zero and improves generalization stability.\n",
        "\n",
        "Both modify the loss function to discourage overly complex models."
      ],
      "metadata": {
        "id": "-LikbbHMCrnx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What is Dropout Layer and To which part of a neural network is dropout generally applied? how does it prevent overfitting"
      ],
      "metadata": {
        "id": "NjeRGpCE7QJ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dropout randomly deactivates neurons during training with a fixed probability. It is generally applied to hidden layers of neural networks. By randomly dropping neurons, the model cannot rely on specific pathways and is forced to learn more robust features. Each training pass effectively trains a slightly different network, which prevents co-adaptation of neurons and reduces overfitting. During testing, dropout is turned off but activations are scaled to match expected output."
      ],
      "metadata": {
        "id": "CYXKnxrmCvaA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. A hidden layer of a neural network has the following activations:\n",
        "\n",
        "   \\[\n",
        "   a = [2.0, 5.0, 7.12, 4.5, 6.0]\n",
        "   \\]\n",
        "\n",
        "   Dropout is applied **during training** with dropout probability \\(p = 0.5\\).\n",
        "\n",
        "   A randomly generated dropout mask is:\n",
        "\n",
        "   \\[\n",
        "   m = [1, 0, 0, 1, 1]\n",
        "   \\]\n",
        "\n",
        "   Here,  \n",
        "   - `1` → neuron is **kept**  \n",
        "   - `0` → neuron is **dropped**\n",
        "\n",
        "   ---\n",
        "\n",
        "   **Question:**  \n",
        "   What will be the output activations of this layer **after applying dropout**?\n",
        "   *(Show the final activation vector)*\n"
      ],
      "metadata": {
        "id": "jLZFLyv4-A67"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When dropout is applied, neurons marked with 0 in the mask are removed, and neurons marked with 1 are kept. Since the dropout probability is 0.5, we use inverted dropout during training, which means we scale the kept activations by 2 so that the overall expected activation remains the same.\n",
        "\n",
        "So we first zero out the dropped neurons and then multiply the remaining activations by 2.\n",
        "\n",
        "Final activations after dropout:\n",
        "\n",
        "4, 0, 0, 9, 12"
      ],
      "metadata": {
        "id": "VGGojzNVCy6J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Explain the difference between Gradient Descent, Stochastic Gradient Descent, Mini-Batch Gradient Descent"
      ],
      "metadata": {
        "id": "La3K0HmUGj8t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient Descent\n",
        "Uses the entire dataset to compute the gradient each update. Very accurate but slow and computationally heavy.\n",
        "\n",
        "Stochastic Gradient Descent\n",
        "Updates parameters using one training example at a time. Very fast but noisy and unstable per step, often helps escape local minima.\n",
        "\n",
        "Mini-batch Gradient Descent\n",
        "Uses small groups of samples (for example 32 or 64). Balances speed and stability and is most commonly used in practice."
      ],
      "metadata": {
        "id": "DhXnRmJqDPEK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What are optimizers ? Explain Adam, RMS prop and Momentum in detail"
      ],
      "metadata": {
        "id": "FngV9WtV7JxI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimizers are methods that update neural network weights to minimize loss.\n",
        "\n",
        "Momentum optimizer accumulates past gradients and adds them to current updates. This speeds learning in consistent directions and reduces oscillations, similar to physical momentum.\n",
        "\n",
        "RMSProp adapts individual learning rates for each parameter. It keeps a moving average of squared gradients. Parameters with consistently large gradients get reduced learning rates, while small-gradient ones get increased rates. It works well for recurrent networks and non-stationary problems.\n",
        "\n",
        "Adam combines Momentum and RMSProp. It keeps moving averages of both gradients and squared gradients and applies bias correction. It adapts learning rate automatically and converges quickly in many practical problems. Adam is widely used because it needs little tuning and performs well on large and sparse datasets."
      ],
      "metadata": {
        "id": "5OFjhpBaDZ5R"
      }
    }
  ]
}