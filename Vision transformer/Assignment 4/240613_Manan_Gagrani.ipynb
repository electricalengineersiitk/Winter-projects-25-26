{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-woU4Sodh6ND"
      },
      "source": [
        "# Assignment #4\n",
        "\n",
        "\n",
        "Vision Transformer\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMQvV4nljttN"
      },
      "source": [
        "# 1. Batch Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JT_kxpjbNQUr"
      },
      "source": [
        "Training a deep neural network is a tricky process. Many techniques have already been proposed for more stable training and faster convergence. Most of these techniques either change the model architecture or improve the training algorithm. Batch normalization belongs to the former group. The method was introduced in 2015 and achieved state-of-the-art in ImageNet,  a well-known image classification benchmark."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDcK52jRj_mD"
      },
      "source": [
        "## 1.1 Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owKLAOJVj73I"
      },
      "source": [
        "We generally normalize the inputs of a neural network to speed up the convergence. So if the \"normalization\" works, why not try it on the activation values? How can we improve training by normalizing the values of intermediate layers?\n",
        "\n",
        "Here is an intermediate layer $l$ in some neural network:\n",
        "\n",
        "<p align=\"center\"><img src=\"https://iust-deep-learning.github.io/972/static_files/assignments/asg03_assets/01_intermediate_layer.jpg\" width=\"500\"/></p>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKOwOymYSVK8"
      },
      "source": [
        "The general idea is to train layer $l$ faster by normalizing its input. By the layer $l$ we simply mean weigth matrices $W^{l}$, $b^{l}$ and by the input we mean previous layer's activations $a^{l-1}$. For the sake of simplicity, let us change our notation. Instead of normalizing the input of layer $l$, we would like to normalize the output so that the next layers will receive normalized values from our layer. It has the same effect, but it will make the equations much cleaner.\n",
        "\n",
        "In practice, we do not normalize the output (the activations). Instead, we do the normalization on the weighted sum of inputs $Z^{l}$ just before applying the activation function ($Z^l = xW^l+b^l$).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_xmxSzHkqrT"
      },
      "source": [
        "## 1.2 The formula"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjFpIC3HgxrO"
      },
      "source": [
        "Assume we want some variable $x$ to have normalized values, the only way to do that is to collect all of its values and calculate the mean and variance in order to create a normalized version of $x$. This is a fairly reasonable solution, and we use it to normalize the neural network's input. Now imagine the goal is to normalize some intermediate values in a deep neural network; Collecting values of some intermediate point in a neural network is almost impossible since the training algorithm can change them entirely. To overcome this issue, we can collect them over a mini-batch.  It will give us an estimated version of the mean and variance. This is why it is called Batch Normalization. Here is the detailed algorithm:\n",
        "\n",
        "Given values of $x$ over a mini-batch $\\mathcal{B} = \\{x_1, .., x_m\\}$ :\n",
        "\n",
        "$$\n",
        "\\mu _\\mathcal{B} = \\frac{1}{m} \\sum^{m}_{i=1} x_i  \\ \\ \\ \\ \\ \\text{(mini-batch mean)}\n",
        "\\\\\n",
        "\\sigma^2 _\\mathcal{B} = \\frac{1}{m} \\sum^{m}_{i=1} (x_i-\\mu _\\mathcal{B})\n",
        "\\ \\ \\ \\ \\ \\text{(mini-batch variance)}\n",
        "\\\\\n",
        "x^{norm}_i = \\frac{x_i - \\mu _\\mathcal{B}}{\\sqrt{\\sigma^2 _\\mathcal{B} + \\epsilon}} \\ \\ \\ \\ \\ \\text{(normalize)}\n",
        "\\\\\n",
        "\\hat{x}_i =\\gamma x^{norm}_i+\\beta  \\ \\ \\ \\ \\ \\text{(scale and shift)}\n",
        "\\\\\n",
        "\\mathrm{BN(\\mathcal{B}, \\gamma, \\beta}) = \\{\\hat{x}_1, ..., \\hat{x}_m\\}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72SOE7UfxmEh"
      },
      "source": [
        "**Notes:**\n",
        "1. All of the notations above are non-vector.\n",
        "2. $\\gamma$ and $\\beta$ are learnable parameters.\n",
        "3. $\\epsilon$ is just a small number, and we use it for numerical stability.\n",
        "4. $\\mathrm{BN}$ function calculates its output based on a batch of values. Consequently, we'll have different $\\mu_\\mathcal{B}$ and $\\sigma^2_\\mathcal{B}$ for each mini-batch during the training process. We will reference this property in the next sections.\n",
        "5. $x^{norm}_i$ is actually the normalized version of $x_i$ which has mean 0 and variance 1. However, hidden units in neural networks have different distributions, and we don't really want them to all have the same distribution, So instead, we just scale and shift $x^{norm}_i$ with two variables $\\gamma$, $\\beta$.\n",
        "6. Another reason for the extra \"scale & shift\" step is that if we choose $\\gamma = \\sqrt{\\sigma^2_\\mathcal{B} + \\epsilon}$ and $\\beta = \\mu_\\mathcal{B}$ then $\\hat{x}_i$ will become $x_i$, So the optimizer can easily remove the batch normalization if it is sufficient for proper training.\n",
        "\n",
        "One difference between normalizing a neural network's inputs and Batch Normalization is that the latter does not force values to have mean 0 and variance 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McQ7-6VxRS3I"
      },
      "source": [
        "**1. Explain it with at least one reason, why we might not want the hidden units to be forced to have mean 0 and variance 1.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrTaUoPMLhyl"
      },
      "source": [
        "We might not want to force unit activations to be strictly mean 0 and variance 1 because this constrains the inputs to the linear regime of non-linear activation functions (like the sigmoid function).\n",
        "\n",
        "If the inputs to a sigmoid activation are all near 0 (which happens with mean 0, var 1), the sigmoid function acts almost linearly. This would effectively remove the non-linearity from that layer, reducing the network's capacity to learn complex patterns. By allowing the network to learn parameters $\\gamma$ (scale) and $\\beta$ (shift), it can restore the distribution that best utilizes the non-linearity of the activation function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2f1WO9jSWS_"
      },
      "source": [
        "**2. Where is Batch Normalization generally applied relative to the activations.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLEtRi9hSrxC"
      },
      "source": [
        "Batch Normalization is generally applied **before** the activation function. It is applied to the weighted sum $Z = Wx + B$ (or just $Wx$ if bias is removed) before passing it to the non-linear activation function $\\sigma(\\cdot)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8few4PaaXf9t"
      },
      "source": [
        "### 1.2.2 Batch normalization at test time\n",
        "\n",
        "As we said, We will have multiple $\\mu_\\mathcal{B}$ and $\\sigma^2_\\mathcal{B}$ since they are calculated individually for each mini-batch. So What should we do for the test time? In fact, the idea is quite simple; We can just calculate a moving average of $\\mu_\\mathcal{B}$ and $\\sigma^2_\\mathcal{B}$ to use at test time. Deep learning frameworks such as Tensorflow are using this algorithm in their default bach normalization implementations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwyIr1ChkvzY"
      },
      "source": [
        "## 1.3 Applying the batch-norm on layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wu85xhmM1xFv"
      },
      "source": [
        "Batch Normalization (or simply batch-norm) doesn't know anything about the concept of layers and vectors. we have to integrate it manually in our layers. For a given d-dimensional vector of logits $Z = (z^{(1)},..., z^{(d)})$, the batch-normalized version is\n",
        "\n",
        "$$\n",
        "Z = (\\ \\mathrm{BN}(\\mathcal{B}\\{z^{(1)}\\}, \\gamma^{(1)}, \\beta^{(1)}),..., \\mathrm{BN}(\\mathcal{B}\\{z^{(d)}\\}, \\gamma^{(d)}, \\beta^{(d)})\\ )\n",
        "$$\n",
        "\n",
        "As you might have noticed, we need a batch for each $Z$'s element in the latter version. In other words, we need a batch of $Z$. Fortunately, this is good news for us since we build our neural networks entirely based on batches.\n",
        "\n",
        "Write the vectorized version of batch-norm equations and specify the dimensions.\n",
        "\n",
        "For any given layer $l$ with $n$ hidden units and batch size $b$:\n",
        "\n",
        "$$\n",
        "z = xW + B\\ \\ \\ \\  z \\in \\mathbb{R} ^ {b \\times n}, W \\in \\mathbb{R} ^ {m \\times n}, B \\in \\mathbb{R} ^ {b \\times n}, x \\in \\mathbb{R} ^ {b \\times m}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-J18TTk_Uh1"
      },
      "source": [
        "**3.**\n",
        "$$\n",
        "\\mu = \\frac{1}{b} \\sum_{i=1}^{b} z_i \\quad (\\text{Dimension: } 1 \\times n)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\sigma^2 = \\frac{1}{b} \\sum_{i=1}^{b} (z_i - \\mu)^2 \\quad (\\text{Dimension: } 1 \\times n)\n",
        "$$\n",
        "\n",
        "$$\n",
        "z^{norm} = \\frac{z - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\quad (\\text{Dimension: } b \\times n)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\hat{z} = \\gamma \\odot z^{norm} + \\beta  \\ \\ \\ \\ \\ \\ (\\odot \\text{ is an element-wise dot product} )\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmrjugAKYs0p"
      },
      "source": [
        "Imagine a simple neural network with l hidden layers. We want to apply the batch-norm on all layers. Here is how it would look ($\\mathcal{X}^{b} $ is an input batch):\n",
        "\n",
        "$$\n",
        "\\mathcal{X}^{b}\\stackrel{W^{[1]}, B^{[1]}}{\\longrightarrow}Z^{[1]} \\stackrel{\\gamma^{[1]}, \\beta^{[1]}}{\\longrightarrow}\\hat{Z}^{[1]} \\longrightarrow a^{[1]} = func^{[1]}(\\hat{Z}^{[1]})\\stackrel{W^{[2]}, B^{[2]}}{\\longrightarrow} ...\n",
        "$$\n",
        "\n",
        "Also, the parameters for that neural network would be:\n",
        "$$\n",
        "W^{[1]}, B^{[1]} \\ \\  \\ \\ W^{[2]}, B^{[2]}  \\ \\ ... \\ \\ W^{[l]}, B^{[l]}\n",
        "\\\\\n",
        "\\gamma^{[1]}, \\beta^{[1]} \\ \\ \\ \\  \\  \\ \\ \\gamma^{[2]}, \\beta^{[2]}  \\ \\ \\ \\  ... \\ \\ \\ \\ \\gamma^{[l]}, \\beta^{[l]}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTKOPL1rU3xt"
      },
      "source": [
        "**4. $B^{[i]}$ is the bias term in our neural network, but with incorporating the batch-norm and introduction of new variables,Do you think $B^{[i]}$ is necessary? Justify your answer with proper reasons.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tqv6BuLfirk8"
      },
      "source": [
        "**No**, the bias term $B^{[i]}$ is not necessary when using Batch Normalization.\n",
        "\n",
        "Since the normalization step subtracts the mean $\\mu$ from the input $z = xW + B$, the constant bias $B$ is present in both $z$ and $\\mu$. Therefore, $B$ cancels out during the subtraction $(z - \\mu)$. The role of the bias is effectively replaced by the learnable parameter $\\beta$ in the Batch Normalization step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0PrEPOdlE9d"
      },
      "source": [
        "## 1.4 Why does it work?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVqGS4J6lJlf"
      },
      "source": [
        "Imagine a super simple neural network:\n",
        "\n",
        "<br/>\n",
        "<p align=\"center\"><img src=\"https://iust-deep-learning.github.io/972/static_files/assignments/asg03_assets/02_simple_nn.png\" width=\"300\"/>\n",
        "<br>\n",
        "  [[source](https://blog.paperspace.com/busting-the-myths-about-batch-normalization/)]\n",
        "</p>\n",
        "\n",
        "</br>\n",
        "During the training, Our optimizer calculates the gradients with respect to weights. Take layer **a** as an example; Optimizer calucates  $\\frac{\\partial L}{\\partial a}$ and then it updates the weights for this layer. Unfortunately,  it means that weight update for Layer **a** only depends on the sensitivity of loss function to that weight. However, changing weights of initial layers can completely effect the statistics of any futher layer.\n",
        "\n",
        "With the presence of Batch Normalization, our optimizer package can now adjust two parameters $\\gamma$, $\\beta$ to change statistics of any layer, rather than entire weight matrix. It makes the training of any layer independent and also introduces some checkpointing mechanism.\n",
        "\n",
        "Besides, recent findings show that batch normalization smoothes the landscape/surface of the loss function, effectively making the optimization performance less dependant on the initial state.\n",
        "\n",
        "<p align=\"center\"><img src=\"https://iust-deep-learning.github.io/972/static_files/assignments/asg03_assets/03_error_surface.jpg\"  width=\"700\"/>\n",
        "<br/>\n",
        "  source: [2]\n",
        "  </p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZlJ4Hic2aoN"
      },
      "source": [
        "## 1.5 Batch Normalization in action\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSqjUrD02hiL"
      },
      "source": [
        "Now let's create a layer to use batch normalization easier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YwY8vwus2xQu"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'keras'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m backend \u001b[38;5;28;01mas\u001b[39;00m K\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Sequential\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'keras'"
          ]
        }
      ],
      "source": [
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Layer, Dense, Activation, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6TKpXssrUqc"
      },
      "source": [
        "**5. Complete the following Class**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gt5mzXXX2s5V"
      },
      "outputs": [],
      "source": [
        "class BatchNormalizedLayer(Layer):\n",
        "  def __init__(self, layer, axis=-1, activation=None, **kwargs):\n",
        "    \"\"\"Runs batch normalization on layer instance and applies the activation function\n",
        "\n",
        "    Args:\n",
        "      layer(layers.Layer): A layer to normalize its output\n",
        "      axis(int): the axis that should be normalized (typically the features axis).\n",
        "      activation(str): Activation function to use\n",
        "    \"\"\"\n",
        "    super(BatchNormalizedLayer, self).__init__(**kwargs)\n",
        "\n",
        "    self.layer = layer\n",
        "    self.activation = activation\n",
        "    self.axis = axis\n",
        "\n",
        "  def call(self, inputs):\n",
        "    \"\"\"Runs the layer\n",
        "\n",
        "    Args:\n",
        "      inputs: The layer's input\n",
        "\n",
        "    hint: keras.layers.BatchNormalization and layers.Activation might be useful for you\n",
        "    \"\"\"\n",
        "    ########################################\n",
        "    x = self.layer(inputs)\n",
        "\n",
        "    if not hasattr(self, 'bn_layer'):\n",
        "        self.bn_layer = BatchNormalization(axis=self.axis)\n",
        "\n",
        "    x = self.bn_layer(x)\n",
        "\n",
        "    if self.activation:\n",
        "        if not hasattr(self, 'act_layer'):\n",
        "            self.act_layer = Activation(self.activation)\n",
        "        x = self.act_layer(x)\n",
        "\n",
        "    return x\n",
        "    ########################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cc0wyxlb7JwJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf # Import tensorflow\n",
        "\n",
        "bnl = BatchNormalizedLayer(Dense(5), activation='relu')\n",
        "x = tf.constant(2.5 * np.random.randn(10, 4) + 3, dtype=tf.float32) # Use tf.constant\n",
        "\n",
        "# Evaluate the output using tf.keras.backend.get_value\n",
        "assert tf.keras.backend.get_value(bnl(x)).shape == (10, 5)\n",
        "#this is just a check to see if the Layer is working as expected ,it doesnot print anything"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-Tlpqm-_dyH"
      },
      "source": [
        "### 1.5.1 CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPLoHHW1_qdv"
      },
      "source": [
        "Now we have our special layer. So, let's use it in a real neural network. We want to improve the baseline using the Batch Normalization layer. Our desired task is CIFAR10 image  classification.\n",
        "\n",
        "First, let's load the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "wm79YEyIAmKa",
        "outputId": "55c6cf76-2641-4adc-bb49-8fb7fb8ab255"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.datasets import cifar10\n",
        "\n",
        "num_classes = 10\n",
        "\n",
        "# The data, split between train and test sets:\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# 6.Convert class vectors to binary class matrices.\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)\n",
        "\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# Visualizing CIFAR 10\n",
        "fig, axes1 = plt.subplots(2,5,figsize=(10,4))\n",
        "for j in range(2):\n",
        "  for k in range(5):\n",
        "    i = np.random.choice(range(len(x_train)))\n",
        "    axes1[j][k].set_axis_off()\n",
        "    axes1[j][k].imshow(x_train[i:i+1][0])\n",
        "\n",
        "# Normalize\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBb__seXIyRN"
      },
      "outputs": [],
      "source": [
        "def visualize_loss_and_acc(history):\n",
        "  history_dict = history.history\n",
        "  loss_values = history_dict['loss']\n",
        "  val_loss_values = history_dict['val_loss']\n",
        "\n",
        "  # Check for the correct key name to avoid KeyError\n",
        "  if 'accuracy' in history_dict:\n",
        "      acc = history_dict['accuracy']\n",
        "      val_acc = history_dict['val_accuracy']\n",
        "  else:\n",
        "      acc = history_dict['acc']\n",
        "      val_acc = history_dict['val_acc']\n",
        "\n",
        "  epochs = range(1, len(acc) + 1)\n",
        "\n",
        "  f = plt.figure(figsize=(10,3))\n",
        "\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.plot(epochs, loss_values, 'bo', label='Training loss')\n",
        "  plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
        "  plt.title('Training and validation loss')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "  plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "  plt.title('Training and validation accuracy')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qlKS6lsIzLX"
      },
      "source": [
        "#### Baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzzXJ2M6GXn-"
      },
      "source": [
        "Define the baseline model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fLyt7WKl8F-"
      },
      "source": [
        "![CNN Architecture](https://drive.google.com/uc?id=1Fh3Z94KKHe9sAzUorZlW9NXMPN6NAhlx\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFWdmvXlnWNN"
      },
      "source": [
        "The figure above shows the architecture of a **baseline Convolutional Neural Network (CNN)** used for multi-class image classification.\n",
        "\n",
        "The architecture consists of the following components in **Sequential order**:\n",
        "\n",
        "- Convolutional layers with **3×3 kernels** and **padding = 'same'**\n",
        "- ReLU activation after each convolution\n",
        "- MaxPooling layers with **2×2 pool size**\n",
        "- Dropout layers for regularization\n",
        "- A fully connected Dense layer\n",
        "- A Softmax output layer\n",
        "\n",
        "**7.** Based on the **architecture shown in the image** and the details mentioned above, implement the CNN model in Keras by completing the function below.\n",
        "\n",
        "- Ensure that the **number of layers, order of layers, kernel sizes, pooling sizes, dropout rates, padding, and activations** exactly match the given architecture\n",
        "- Do **not** change the function name or signature\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZnXcRHpuDcvS"
      },
      "outputs": [],
      "source": [
        "def get_baseline_model():\n",
        "  model = Sequential()\n",
        "\n",
        "  ## HINT : input_shape=x_train.shape[1:]\n",
        "\n",
        "    ########################################\n",
        "    model.add(Conv2D(32, (3, 3), padding='same', input_shape=x_train.shape[1:]))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(32, (3, 3), padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(num_classes))\n",
        "    model.add(Activation('softmax'))\n",
        "    ########################################\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "                optimizer=keras.optimizers.RMSprop(learning_rate=0.0001, decay=1e-6),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TQ2uoSmGlvP"
      },
      "source": [
        "Train the baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Q45b4cVF36l"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "epochs = 25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "488nX4FSDzs-",
        "outputId": "6c869a8f-19fd-4c14-a56d-127c56961e38"
      },
      "outputs": [],
      "source": [
        "# Create the baseline model\n",
        "baseline = get_baseline_model()\n",
        "\n",
        "# Train model\n",
        "bs_history = baseline.fit(\n",
        "    x_train, y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=(x_test, y_test),\n",
        "    shuffle=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJgQ9yxpGp8P"
      },
      "source": [
        "Visualize the training and evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "VeVuiJmHGUw7",
        "outputId": "a143fa4f-3aec-4017-b54f-7fddef1c1dd4"
      },
      "outputs": [],
      "source": [
        "visualize_loss_and_acc(bs_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aMc7UWPICG9",
        "outputId": "09613cd2-44c5-40aa-ccf7-6e8a12a4aec1"
      },
      "outputs": [],
      "source": [
        "# Score trained model.\n",
        "scores = baseline.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test accuracy:', scores[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufqdGaLrI4oZ"
      },
      "source": [
        "#### Improved"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dm8Yu20HHBXp"
      },
      "source": [
        "**8. Now update the baseline to create an enhanced model only by using `BatchNormalizedLayer`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aD6NqqZ3Hbeg"
      },
      "outputs": [],
      "source": [
        "def get_improved_model():\n",
        "\n",
        "  model = Sequential()\n",
        "\n",
        "  ########################################\n",
        "  model.add(BatchNormalizedLayer(\n",
        "      Conv2D(32, (3, 3), padding='same', input_shape=x_train.shape[1:], use_bias=False),\n",
        "      activation='relu'\n",
        "  ))\n",
        "  model.add(BatchNormalizedLayer(\n",
        "      Conv2D(32, (3, 3), padding='same', use_bias=False),\n",
        "      activation='relu'\n",
        "  ))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(BatchNormalizedLayer(\n",
        "      Conv2D(64, (3, 3), padding='same', use_bias=False),\n",
        "      activation='relu'\n",
        "  ))\n",
        "  model.add(BatchNormalizedLayer(\n",
        "      Conv2D(64, (3, 3), padding='same', use_bias=False),\n",
        "      activation='relu'\n",
        "  ))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(BatchNormalizedLayer(\n",
        "      Dense(512, use_bias=False),\n",
        "      activation='relu'\n",
        "  ))\n",
        "  model.add(Dropout(0.5))\n",
        "\n",
        "  model.add(Dense(num_classes))\n",
        "  model.add(Activation('softmax'))\n",
        "  ########################################\n",
        "  ## remember you have to use your baseline and add batch normalized layers using the previous Model\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "                optimizer=keras.optimizers.RMSprop(learning_rate=0.0001, decay=1e-6),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYmlozp3I8Ts"
      },
      "source": [
        "Train and evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLNW4dnsH9NF",
        "outputId": "7fb74b18-708a-45a6-b7f4-3a3fe1ec1404"
      },
      "outputs": [],
      "source": [
        "# Create the baseline model\n",
        "impv_model = get_improved_model()\n",
        "\n",
        "# Train model\n",
        "impv_history = impv_model.fit(\n",
        "    x_train, y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=(x_test, y_test),\n",
        "    shuffle=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fsz97z1Q-eoA"
      },
      "source": [
        "Visualize the training and evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "30JMMBQyIb49",
        "outputId": "8d2a0890-271e-4ebf-8b1a-dc8d7a717662"
      },
      "outputs": [],
      "source": [
        "visualize_loss_and_acc(impv_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoqlKrUAIb4_",
        "outputId": "0c4006e6-afe0-4ad4-bec0-525bb1a3a094"
      },
      "outputs": [],
      "source": [
        "# Score trained model.\n",
        "scores = impv_model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test accuracy:', scores[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-vfDkCCycqZ"
      },
      "source": [
        "**Question:** Compare your model to the baseline. What are the diffrences? Does batch normalization work?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjOOqiDDyf4p"
      },
      "source": [
        "Batch Normalization generally results in **faster convergence** (the model learns quicker, requiring fewer epochs to reach high accuracy) and often achieves higher final accuracy compared to the baseline. It stabilizes the learning process by reducing internal covariate shift. You should observe that the training loss decreases more smoothly and the gap between training and validation accuracy is often better managed due to the slight regularization effect of BN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJLRl0DL5aSO"
      },
      "source": [
        "# References\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCQPkvV83Kn0"
      },
      "source": [
        "\n",
        "1. Ioffe, Sergey, and Christian Szegedy. “Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.” ArXiv:1502.03167 [Cs], February 10, 2015. http://arxiv.org/abs/1502.03167.\n",
        "* Im, Daniel Jiwoong, Michael Tao, and Kristin Branson. “An Empirical Analysis of the Optimization of Deep Network Loss Surfaces.” ArXiv:1612.04010 [Cs], December 12, 2016. http://arxiv.org/abs/1612.04010.\n",
        "* Santurkar, Shibani, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. “How Does Batch Normalization Help Optimization?” In Advances in Neural Information Processing Systems 31, edited by S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, 2483–2493. Curran Associates, Inc., 2018. http://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization.pdf.\n",
        "* Coursera Course: Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization\n",
        "* Intro to optimization in deep learning: Busting the myth about batch normalization [[link](https://blog.paperspace.com/busting-the-myths-about-batch-normalization/)]\n",
        "* Why Does Batch Normalization Work? [[link](https://abay.tech/blog/2018/07/01/why-does-batch-normalization-work/)]\n",
        "*  http://www.cs.toronto.edu/~tingwuwang/semantic_segmentation.pdf\n",
        "*  https://medium.com/nanonets/how-to-do-image-segmentation-using-deep-learning-c673cc5862ef\n",
        "* https://www.jeremyjordan.me/semantic-segmentation/\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
