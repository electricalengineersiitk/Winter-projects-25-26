{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "x422GyParjWf"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries for building and training the Vision Transformer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from dataclasses import dataclass\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the SiglipVisionConfig dataclass and CIFAR-10 data transformation functions\n",
        "@dataclass\n",
        "class SiglipVisionConfig:\n",
        "    hidden_layers: int = 6\n",
        "    n_channel: int = 3\n",
        "    img_size: int = 32\n",
        "    patch_size: int = 4\n",
        "    hidden_size: int = 384\n",
        "    num_heads: int = 8\n",
        "\n",
        "    mlp_intermediate_size: int = 1536\n",
        "    num_classes: int = 10\n",
        "    layer_norm_eps: float = 1e-6\n",
        "    general_dropout_prob: float = 0.1\n",
        "    attention_dropout_prob: float = 0.1\n",
        "\n",
        "\n",
        "def cifar_transform():\n",
        "\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.247, 0.243, 0.261])\n",
        "    ])\n",
        "\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.247, 0.243, 0.261])\n",
        "    ])\n",
        "    return train_transform, test_transform"
      ],
      "metadata": {
        "id": "NXIJ94cMTVjq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LI98IRBgrpNR"
      },
      "outputs": [],
      "source": [
        "# Implement the SiglipVisionEmbeddings class for creating patch and position embeddings\n",
        "class SiglipVisionEmbeddings(nn.Module):\n",
        "    def __init__(self, embed_config: SiglipVisionConfig):\n",
        "        super().__init__()\n",
        "        self.embed_config = embed_config\n",
        "\n",
        "        self.input_channels = embed_config.n_channel\n",
        "        self.embedding_dimension = embed_config.hidden_size\n",
        "        self.image_input_size = embed_config.img_size\n",
        "        self.patch_dimensions = embed_config.patch_size\n",
        "\n",
        "        self.patch_convolution_layer = nn.Conv2d(\n",
        "            in_channels=self.input_channels,\n",
        "            out_channels=self.embedding_dimension,\n",
        "            kernel_size=self.patch_dimensions,\n",
        "            stride=self.patch_dimensions,\n",
        "            padding='valid'\n",
        "        )\n",
        "\n",
        "\n",
        "        self.total_patches = (self.image_input_size // self.patch_dimensions) ** 2\n",
        "        self.total_positions = self.total_patches + 1\n",
        "        self.class_token = nn.Parameter(torch.zeros(1, 1, self.embedding_dimension))\n",
        "\n",
        "        self.positional_encoder = nn.Embedding(self.total_positions, self.embedding_dimension)\n",
        "\n",
        "        self.register_buffer(\n",
        "\n",
        "            \"position_indices\",\n",
        "            torch.arange(self.total_positions).expand(1, -1),\n",
        "            persistent=False\n",
        "        )\n",
        "        self.embedding_dropout_layer = nn.Dropout(embed_config.general_dropout_prob)\n",
        "\n",
        "    def forward(self, input_pixel_data: torch.FloatTensor) -> torch.Tensor:\n",
        "\n",
        "        batch_size, channels, height, width = input_pixel_data.shape\n",
        "\n",
        "        patch_features = self.patch_convolution_layer(input_pixel_data);\n",
        "\n",
        "        combined_embeddings = patch_features.flatten(2).transpose(1, 2)\n",
        "\n",
        "\n",
        "        expanded_class_token = self.class_token.expand(batch_size, -1, -1)\n",
        "        combined_embeddings = torch.cat((expanded_class_token, combined_embeddings), dim=1)\n",
        "\n",
        "        combined_embeddings = combined_embeddings + self.positional_encoder(self.position_indices)\n",
        "        return self.embedding_dropout_layer(combined_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jfYKzrizrrZS"
      },
      "outputs": [],
      "source": [
        "# Implement the SiglipMLP (Multi-Layer Perceptron) block\n",
        "class SiglipMLP(nn.Module):\n",
        "    def __init__(self, mlp_config: SiglipVisionConfig):\n",
        "        super().__init__()\n",
        "        self.mlp_config = mlp_config\n",
        "\n",
        "        self.dense_layer1 = nn.Linear(mlp_config.hidden_size, mlp_config.mlp_intermediate_size)\n",
        "\n",
        "        self.dense_layer2 = nn.Linear(mlp_config.mlp_intermediate_size, mlp_config.hidden_size)\n",
        "\n",
        "        self.mlp_dropout_layer = nn.Dropout(mlp_config.general_dropout_prob)\n",
        "\n",
        "    def forward(self, input_features: torch.Tensor) -> torch.Tensor:\n",
        "        processed_features = self.dense_layer1(input_features)\n",
        "\n",
        "        processed_features = F.gelu(processed_features)\n",
        "        processed_features = self.mlp_dropout_layer(processed_features)\n",
        "        processed_features = self.dense_layer2(processed_features)\n",
        "        return processed_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nLgkVbiirtt0"
      },
      "outputs": [],
      "source": [
        "# Implement the SiglipAttention (Multi-head Self-Attention) mechanism\n",
        "class SiglipAttention(nn.Module):\n",
        "    def __init__(self, attn_config: SiglipVisionConfig):\n",
        "        super().__init__();\n",
        "        self.attn_config = attn_config\n",
        "        self.attention_embedding_dim = attn_config.hidden_size\n",
        "        self.attention_heads_count = attn_config.num_heads\n",
        "        self.single_head_dimension = self.attention_embedding_dim // self.attention_heads_count\n",
        "        self.attention_dropout_layer = attn_config.attention_dropout_prob\n",
        "\n",
        "        self.query_projection = nn.Linear(self.attention_embedding_dim, self.attention_embedding_dim, bias=True)\n",
        "        self.key_projection = nn.Linear(self.attention_embedding_dim, self.attention_embedding_dim, bias=True)\n",
        "        self.value_projection = nn.Linear(self.attention_embedding_dim, self.attention_embedding_dim, bias=True)\n",
        "\n",
        "        self.output_projection = nn.Linear(self.attention_embedding_dim, self.attention_embedding_dim, bias=True)\n",
        "\n",
        "    def forward(self, attention_input):\n",
        "        batch_size, sequence_length, feature_dim = attention_input.shape\n",
        "\n",
        "        query_states = self.query_projection(attention_input).view(batch_size, sequence_length, self.attention_heads_count, feature_dim // self.attention_heads_count).transpose(1, 2)\n",
        "        key_states = self.key_projection(attention_input).view(batch_size, sequence_length, self.attention_heads_count, feature_dim // self.attention_heads_count).transpose(1, 2)\n",
        "        value_states = self.value_projection(attention_input).view(batch_size, sequence_length, self.attention_heads_count, feature_dim // self.attention_heads_count).transpose(1, 2)\n",
        "\n",
        "        attention_scores = (query_states @ key_states.transpose(-2, -1)) / math.sqrt(self.single_head_dimension)\n",
        "        attention_scores = F.softmax(attention_scores, dim=-1)\n",
        "\n",
        "        attention_scores = F.dropout(attention_scores, p=self.attention_dropout_layer, training=self.training)\n",
        "\n",
        "        attention_output = attention_scores @ value_states\n",
        "\n",
        "        attention_output = attention_output.transpose(1, 2).reshape(batch_size, sequence_length, feature_dim)\n",
        "        return self.output_projection(attention_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ga3jimi6rwDy"
      },
      "outputs": [],
      "source": [
        "# Implement a single SiglipEncoderLayer composed of self-attention and MLP blocks\n",
        "class SiglipEncoderLayer(nn.Module):\n",
        "    def __init__(self, layer_config: SiglipVisionConfig):\n",
        "        super().__init__()\n",
        "        self.layer_embedding_dim = layer_config.hidden_size\n",
        "        self.self_attention_block = SiglipAttention(layer_config)\n",
        "        self.norm_layer1 = nn.LayerNorm(self.layer_embedding_dim, eps=layer_config.layer_norm_eps)\n",
        "        self.feed_forward_block = SiglipMLP(layer_config)\n",
        "        self.norm_layer2 = nn.LayerNorm(self.layer_embedding_dim, eps=layer_config.layer_norm_eps)\n",
        "\n",
        "    def forward(self, layer_input_states):\n",
        "        skip_connection_input = layer_input_states\n",
        "        current_states = self.norm_layer1(layer_input_states)\n",
        "        current_states = self.self_attention_block(current_states)\n",
        "        current_states += skip_connection_input\n",
        "\n",
        "        skip_connection_input = current_states\n",
        "        current_states = self.norm_layer2(current_states)\n",
        "        current_states = self.feed_forward_block(current_states)\n",
        "        current_states += skip_connection_input\n",
        "        return current_states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0iG9ohdvryHU"
      },
      "outputs": [],
      "source": [
        "# Implement the SiglipEncoder by stacking multiple SiglipEncoderLayer instances\n",
        "class SiglipEncoder(nn.Module):\n",
        "    def __init__(self, encoder_config: SiglipVisionConfig):\n",
        "        super().__init__()\n",
        "        self.encoder_config = encoder_config\n",
        "        self.encoder_blocks = nn.ModuleList([SiglipEncoderLayer(encoder_config) for _ in range(encoder_config.hidden_layers)])\n",
        "\n",
        "    def forward(self, input_hidden_states):\n",
        "        for encoder_block in self.encoder_blocks:\n",
        "            input_hidden_states = encoder_block(input_hidden_states)\n",
        "        return input_hidden_states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "cIZvafuJrd0E"
      },
      "outputs": [],
      "source": [
        "# Combine all components to form the complete CIFAR10VisionTransformer model\n",
        "class CIFAR10VisionTransformer(nn.Module):\n",
        "    def __init__(self, transformer_config: SiglipVisionConfig):\n",
        "        super().__init__()\n",
        "        self.transformer_config = transformer_config\n",
        "        self.vision_embeddings_module = SiglipVisionEmbeddings(transformer_config)\n",
        "        self.transformer_encoder = SiglipEncoder(transformer_config)\n",
        "        self.final_layer_norm = nn.LayerNorm(transformer_config.hidden_size, eps=transformer_config.layer_norm_eps)\n",
        "\n",
        "        self.classification_head = nn.Linear(transformer_config.hidden_size, transformer_config.num_classes)\n",
        "\n",
        "    def forward(self, model_input_pixels):\n",
        "        encoded_features = self.vision_embeddings_module(model_input_pixels)\n",
        "        encoded_features = self.transformer_encoder(encoded_features)\n",
        "        encoded_features = self.final_layer_norm(encoded_features)\n",
        "\n",
        "        classification_token_feature = encoded_features[:, 0]\n",
        "        predicted_logits = self.classification_head(classification_token_feature)\n",
        "        return predicted_logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ROh_fBtMr2kP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd5cbcb0-8798-4787-d6c6-57f7081571c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/25: 100%|██████████| 391/391 [01:20<00:00,  4.85it/s, Loss=1.569, Acc=34.9%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Acc: 34.88%, Test Acc: 46.49%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/25: 100%|██████████| 391/391 [01:22<00:00,  4.74it/s, Loss=1.532, Acc=45.8%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Train Acc: 45.79%, Test Acc: 50.73%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/25: 100%|██████████| 391/391 [01:22<00:00,  4.74it/s, Loss=1.384, Acc=50.4%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Train Acc: 50.36%, Test Acc: 54.97%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/25: 100%|██████████| 391/391 [01:22<00:00,  4.74it/s, Loss=1.260, Acc=54.0%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Train Acc: 53.98%, Test Acc: 56.07%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/25: 100%|██████████| 391/391 [01:22<00:00,  4.74it/s, Loss=1.150, Acc=56.0%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train Acc: 56.00%, Test Acc: 60.14%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/25: 100%|██████████| 391/391 [01:22<00:00,  4.74it/s, Loss=1.196, Acc=58.3%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6: Train Acc: 58.26%, Test Acc: 60.37%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/25: 100%|██████████| 391/391 [01:22<00:00,  4.76it/s, Loss=1.101, Acc=60.5%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7: Train Acc: 60.53%, Test Acc: 63.32%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/25: 100%|██████████| 391/391 [01:22<00:00,  4.76it/s, Loss=1.011, Acc=62.7%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8: Train Acc: 62.72%, Test Acc: 65.18%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/25: 100%|██████████| 391/391 [01:22<00:00,  4.74it/s, Loss=0.950, Acc=65.0%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9: Train Acc: 64.99%, Test Acc: 65.91%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/25: 100%|██████████| 391/391 [01:22<00:00,  4.74it/s, Loss=0.837, Acc=66.5%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10: Train Acc: 66.50%, Test Acc: 68.93%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/25: 100%|██████████| 391/391 [01:22<00:00,  4.75it/s, Loss=0.817, Acc=68.2%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11: Train Acc: 68.18%, Test Acc: 68.74%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/25: 100%|██████████| 391/391 [01:22<00:00,  4.76it/s, Loss=0.998, Acc=69.6%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12: Train Acc: 69.56%, Test Acc: 70.73%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/25: 100%|██████████| 391/391 [01:22<00:00,  4.76it/s, Loss=1.150, Acc=71.0%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13: Train Acc: 70.98%, Test Acc: 70.47%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/25: 100%|██████████| 391/391 [01:22<00:00,  4.74it/s, Loss=0.960, Acc=72.1%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14: Train Acc: 72.12%, Test Acc: 72.41%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/25: 100%|██████████| 391/391 [01:22<00:00,  4.74it/s, Loss=0.805, Acc=73.1%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15: Train Acc: 73.13%, Test Acc: 72.67%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/25: 100%|██████████| 391/391 [01:22<00:00,  4.75it/s, Loss=0.745, Acc=74.1%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16: Train Acc: 74.10%, Test Acc: 73.65%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/25: 100%|██████████| 391/391 [01:22<00:00,  4.76it/s, Loss=0.716, Acc=75.3%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17: Train Acc: 75.25%, Test Acc: 74.15%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/25: 100%|██████████| 391/391 [01:22<00:00,  4.74it/s, Loss=0.664, Acc=76.1%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18: Train Acc: 76.10%, Test Acc: 73.12%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/25: 100%|██████████| 391/391 [01:22<00:00,  4.73it/s, Loss=0.594, Acc=77.0%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19: Train Acc: 76.98%, Test Acc: 74.80%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/25: 100%|██████████| 391/391 [01:22<00:00,  4.75it/s, Loss=0.661, Acc=77.9%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20: Train Acc: 77.94%, Test Acc: 75.66%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21/25: 100%|██████████| 391/391 [01:22<00:00,  4.76it/s, Loss=0.577, Acc=78.9%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21: Train Acc: 78.86%, Test Acc: 76.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22/25: 100%|██████████| 391/391 [01:22<00:00,  4.76it/s, Loss=0.500, Acc=79.7%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22: Train Acc: 79.67%, Test Acc: 77.24%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23/25: 100%|██████████| 391/391 [01:22<00:00,  4.75it/s, Loss=0.521, Acc=80.7%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23: Train Acc: 80.68%, Test Acc: 77.14%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24/25: 100%|██████████| 391/391 [01:22<00:00,  4.76it/s, Loss=0.438, Acc=81.5%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24: Train Acc: 81.48%, Test Acc: 77.31%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25/25: 100%|██████████| 391/391 [01:22<00:00,  4.75it/s, Loss=0.360, Acc=82.5%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25: Train Acc: 82.55%, Test Acc: 78.07%\n",
            "Best Test Accuracy: 78.07%\n"
          ]
        }
      ],
      "source": [
        "# Main function to load data, initialize, train, and evaluate the Vision Transformer model\n",
        "def main():\n",
        "    training_data_transform, testing_data_transform = cifar_transform()\n",
        "\n",
        "    cifar10_train_dataset = torchvision.datasets.CIFAR10(\n",
        "        root='./data',\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=training_data_transform\n",
        "    )\n",
        "    cifar10_test_dataset = torchvision.datasets.CIFAR10(\n",
        "        root='./data',\n",
        "        train=False,\n",
        "        download=True,\n",
        "        transform=testing_data_transform\n",
        "    )\n",
        "\n",
        "    training_dataloader = DataLoader(cifar10_train_dataset, batch_size=128, shuffle=True, num_workers=4, pin_memory=True)\n",
        "    testing_dataloader = DataLoader(cifar10_test_dataset, batch_size=128, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "    cifar10_class_names = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "    model_config = SiglipVisionConfig()\n",
        "    vit_model = CIFAR10VisionTransformer(model_config).cuda()\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "    model_optimizer = optim.AdamW(vit_model.parameters(), lr=3e-4, weight_decay=0.05)\n",
        "    lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(model_optimizer, T_max=50)\n",
        "\n",
        "\n",
        "\n",
        "    max_epochs = 25\n",
        "    best_test_accuracy = 0.0\n",
        "\n",
        "    for current_epoch in range(max_epochs):\n",
        "        vit_model.train()\n",
        "        current_train_loss = 0.0\n",
        "        correct_train_predictions = 0\n",
        "        total_train_samples = 0\n",
        "\n",
        "        progress_bar = tqdm(training_dataloader, desc=f'Epoch {current_epoch+1}/{max_epochs}')\n",
        "        for batch_index, (batch_inputs, batch_targets) in enumerate(progress_bar):\n",
        "            batch_inputs, batch_targets = batch_inputs.cuda(), batch_targets.cuda()\n",
        "\n",
        "            model_optimizer.zero_grad()\n",
        "            model_outputs = vit_model(batch_inputs)\n",
        "            batch_loss = loss_function(model_outputs, batch_targets)\n",
        "            batch_loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(vit_model.parameters(), max_norm=1.0)\n",
        "            model_optimizer.step()\n",
        "\n",
        "            current_train_loss += batch_loss.item()\n",
        "            _, predicted_labels = model_outputs.max(1)\n",
        "            total_train_samples += batch_targets.size(0);\n",
        "            correct_train_predictions += predicted_labels.eq(batch_targets).sum().item()\n",
        "\n",
        "            progress_bar.set_postfix({\n",
        "                'Loss': f'{batch_loss.item():.3f}',\n",
        "                'Acc': f'{100.*correct_train_predictions/total_train_samples:.1f}%'\n",
        "            })\n",
        "\n",
        "        lr_scheduler.step()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        vit_model.eval()\n",
        "        current_test_loss = 0.0\n",
        "        correct_test_predictions = 0\n",
        "        total_test_samples = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_inputs, batch_targets in testing_dataloader:\n",
        "                batch_inputs, batch_targets = batch_inputs.cuda(), batch_targets.cuda()\n",
        "                model_outputs = vit_model(batch_inputs)\n",
        "                batch_loss = loss_function(model_outputs, batch_targets)\n",
        "\n",
        "                current_test_loss += batch_loss.item()\n",
        "                _, predicted_labels = model_outputs.max(1)\n",
        "                total_test_samples += batch_targets.size(0)\n",
        "                correct_test_predictions += predicted_labels.eq(batch_targets).sum().item()\n",
        "\n",
        "        training_accuracy = 100. * correct_train_predictions / total_train_samples\n",
        "        test_accuracy = 100. * correct_test_predictions / total_test_samples\n",
        "\n",
        "\n",
        "        print(f'Epoch {current_epoch+1}: Train Acc: {training_accuracy:.2f}%, Test Acc: {test_accuracy:.2f}%')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        if test_accuracy > best_test_accuracy:\n",
        "            best_test_accuracy = test_accuracy\n",
        "            torch.save(vit_model.state_dict(), 'best_cifar10_vit.pth')\n",
        "\n",
        "    print(f'Best Test Accuracy: {best_test_accuracy:.2f}%')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}