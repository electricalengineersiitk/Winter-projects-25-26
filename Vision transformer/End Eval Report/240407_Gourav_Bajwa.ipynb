{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x422GyParjWf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from dataclasses import dataclass\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vrqGt_frm0p"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class SiglipVisionConfig:\n",
        "    num_hidden_layers: int = 6\n",
        "    num_channels: int = 3       # channels in input image - 3 - RGB\n",
        "    image_size: int = 32        # CIFAR10 dataset images are 32x32\n",
        "    patch_size: int = 4         # 4x4 patches for 32x32 -> 64 patches\n",
        "    num_attention_heads: int = 8# number of attention heads\n",
        "    hidden_size: int = 384\n",
        "    intermediate_size: int = 1536\n",
        "    num_classes: int = 10       # since CIFAR10 has 10 classes\n",
        "    layer_norm_eps: float = 1e-6\n",
        "    attention_dropout: float = 0.1\n",
        "    dropout: float = 0.1\n",
        "\n",
        "def get_cifar10_transforms():\n",
        "\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.ToTensor(),               # converts the images to a tensor\n",
        "        transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.247, 0.243, 0.261])  # normalizes the image tensor\n",
        "    ])\n",
        "\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.247, 0.243, 0.261])\n",
        "    ])\n",
        "    return train_transform, test_transform\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LI98IRBgrpNR"
      },
      "outputs": [],
      "source": [
        "class SiglipVisionEmbeddings(nn.Module):\n",
        "    def __init__(self, config: SiglipVisionConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        # basic parameters -\n",
        "        self.num_channels = config.num_channels\n",
        "        self.embed_dim = config.hidden_size # each image patch is going to be turned into a vector of 384 dimensions\n",
        "        self.image_size = config.image_size # image size is 32x32 pixels\n",
        "        self.patch_size = config.patch_size # each patch has a size of 4x4 pixels\n",
        "\n",
        "        # convolution used to create patch embeddings\n",
        "        self.patch_embedding = nn.Conv2d(\n",
        "            in_channels=self.num_channels,\n",
        "            out_channels=self.embed_dim,\n",
        "            kernel_size=self.patch_size,\n",
        "            stride=self.patch_size,\n",
        "            padding='valid'\n",
        "        )\n",
        "\n",
        "        # calculating the number of patches were going to get-\n",
        "        self.num_patches = (self.image_size // self.patch_size) ** 2  # 32/4 = 8x8 = 64 patches\n",
        "        self.num_positions = self.num_patches + 1                     # +1 for CLS token\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, self.embed_dim))\n",
        "        # creating the positional embedding layer -> creates a lookup table of size num_patches x embed_dim\n",
        "        self.position_embedding = nn.Embedding(self.num_positions, self.embed_dim)\n",
        "\n",
        "        self.register_buffer(\n",
        "            # storing position indices -\n",
        "            \"position_ids\",\n",
        "            torch.arange(self.num_positions).expand(1, -1),\n",
        "            persistent=False\n",
        "        )\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, pixel_values: torch.FloatTensor) -> torch.Tensor:\n",
        "        # getting batch size and image dimensions -\n",
        "        B, C, H, W = pixel_values.shape # assigning the pixel values to Batch, Channel, Height and Width\n",
        "\n",
        "        patch_embeds = self.patch_embedding(pixel_values)\n",
        "        # flattening and reshaping patches\n",
        "        embeddings = patch_embeds.flatten(2).transpose(1, 2)  # B, num_patches, embed_dim\n",
        "\n",
        "        # adding CLS token\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        embeddings = torch.cat((cls_tokens, embeddings), dim=1)\n",
        "\n",
        "        # adding the position embeddings to patch embeddings\n",
        "        embeddings = embeddings + self.position_embedding(self.position_ids)\n",
        "        return self.dropout(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfYKzrizrrZS"
      },
      "outputs": [],
      "source": [
        "# MLP LAYER\n",
        "\n",
        "class SiglipMLP(nn.Module):\n",
        "    def __init__(self, config: SiglipVisionConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        # fully connected layer 1\n",
        "        self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "        # fully connected layer 2\n",
        "        self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "        # using dropout for regularization -\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
        "        hidden_states = self.fc1(hidden_states)\n",
        "        # applying the non linearity activation function -\n",
        "        hidden_states = F.gelu(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.fc2(hidden_states)\n",
        "        return hidden_states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLgkVbiirtt0"
      },
      "outputs": [],
      "source": [
        "# ATTENTION LAYER (part of Encoder Layer)\n",
        "\n",
        "class SiglipAttention(nn.Module):\n",
        "    def __init__(self, config: SiglipVisionConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.embed_dim = config.hidden_size\n",
        "        self.num_heads = config.num_attention_heads\n",
        "        self.head_dim = self.embed_dim // self.num_heads # dimensions per head\n",
        "        self.dropout = config.attention_dropout\n",
        "\n",
        "        # initialising three linear transformations for key, query and value\n",
        "        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=True)\n",
        "        self.k_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=True)\n",
        "        self.v_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=True)\n",
        "\n",
        "        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=True)  # this gives the output projection\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        B, T, C = hidden_states.shape # B = batch, T = tokens, C = channels\n",
        "\n",
        "        q_states = self.q_proj(hidden_states).view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2)\n",
        "        k_states = self.k_proj(hidden_states).view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2)\n",
        "        v_states = self.v_proj(hidden_states).view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2)\n",
        "\n",
        "        # scaled dot product attention -\n",
        "        attn_weights = (q_states @ k_states.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
        "\n",
        "        attn_weights = F.dropout(attn_weights, p=self.dropout, training=self.training)\n",
        "\n",
        "        # multiply attention with values -\n",
        "        attn_outs = attn_weights @ v_states\n",
        "\n",
        "        attn_outs = attn_outs.transpose(1, 2).reshape(B, T, C)\n",
        "        return self.out_proj(attn_outs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ga3jimi6rwDy"
      },
      "outputs": [],
      "source": [
        "# ENCODER LAYER\n",
        "\n",
        "class SiglipEncoderLayer(nn.Module):\n",
        "    def __init__(self, config: SiglipVisionConfig):\n",
        "        super().__init__()\n",
        "        self.embed_dim = config.hidden_size\n",
        "        self.self_attn = SiglipAttention(config)\n",
        "        self.layer_norm1 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n",
        "        self.mlp = SiglipMLP(config)\n",
        "        self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        # first residual block\n",
        "        residual = hidden_states\n",
        "        hidden_states = self.layer_norm1(hidden_states)\n",
        "        hidden_states = self.self_attn(hidden_states)\n",
        "        hidden_states += residual\n",
        "\n",
        "        # second residual block\n",
        "        residual = hidden_states\n",
        "        hidden_states = self.layer_norm2(hidden_states)\n",
        "        hidden_states = self.mlp(hidden_states)\n",
        "        hidden_states += residual\n",
        "        return hidden_states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0iG9ohdvryHU"
      },
      "outputs": [],
      "source": [
        "# FULL ENCODER\n",
        "\n",
        "class SiglipEncoder(nn.Module):\n",
        "    def __init__(self, config: SiglipVisionConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        # stacking multiple encoder layers -\n",
        "        self.layers = nn.ModuleList([SiglipEncoderLayer(config) for _ in range(config.num_hidden_layers)])\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        for layer in self.layers:\n",
        "            hidden_states = layer(hidden_states)\n",
        "        return hidden_states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIZvafuJrd0E"
      },
      "outputs": [],
      "source": [
        "# complete VISION TRANSFORMER for CIFAR10 classification\n",
        "\n",
        "class CIFAR10VisionTransformer(nn.Module):\n",
        "    def __init__(self, config: SiglipVisionConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.embeddings = SiglipVisionEmbeddings(config)\n",
        "        self.encoder = SiglipEncoder(config)\n",
        "        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "\n",
        "        # classification head\n",
        "        self.classifier = nn.Linear(config.hidden_size, config.num_classes)\n",
        "\n",
        "    def forward(self, pixel_values):\n",
        "        hidden_states = self.embeddings(pixel_values)\n",
        "        hidden_states = self.encoder(hidden_states)\n",
        "        hidden_states = self.layer_norm(hidden_states)\n",
        "\n",
        "        # Use CLS token (first token) for classification\n",
        "        cls_token = hidden_states[:, 0]\n",
        "        logits = self.classifier(cls_token)\n",
        "        return logits\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROh_fBtMr2kP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2af80409-2858-463e-de15-da506759071e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 46.1MB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 1/50:  60%|█████▉    | 234/391 [00:47<00:31,  5.00it/s, Loss=1.607, Acc=30.4%]"
          ]
        }
      ],
      "source": [
        "# DATA LOADING AND TRAINING\n",
        "\n",
        "def main():\n",
        "    # loading CIFAR10 dataset\n",
        "    train_transform, test_transform = get_cifar10_transforms()\n",
        "\n",
        "    trainset = torchvision.datasets.CIFAR10(\n",
        "        root='./data',\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=train_transform\n",
        "    )\n",
        "    testset = torchvision.datasets.CIFAR10(\n",
        "        root='./data',\n",
        "        train=False,\n",
        "        download=True,\n",
        "        transform=test_transform\n",
        "    )\n",
        "\n",
        "    trainloader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=4, pin_memory=True)\n",
        "    testloader = DataLoader(testset, batch_size=128, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "    # CIFAR10 classes names\n",
        "    classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "    # Model, loss, optimizer\n",
        "    config = SiglipVisionConfig()\n",
        "    model = CIFAR10VisionTransformer(config).cuda()\n",
        "    criterion = nn.CrossEntropyLoss() # loss function\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.05)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n",
        "\n",
        "\n",
        "\n",
        "    # Training loop\n",
        "    num_epochs = 50\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        pbar = tqdm(trainloader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
        "        for batch_idx, (inputs, targets) in enumerate(pbar):\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "\n",
        "            # gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            train_total += targets.size(0)\n",
        "            train_correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "            pbar.set_postfix({\n",
        "                'Loss': f'{loss.item():.3f}',\n",
        "                'Acc': f'{100.*train_correct/train_total:.1f}%'\n",
        "            })\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Evaluation\n",
        "        model.eval()\n",
        "        test_loss = 0.0\n",
        "        test_correct = 0\n",
        "        test_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in testloader:\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "\n",
        "                test_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                test_total += targets.size(0)\n",
        "                test_correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        train_acc = 100. * train_correct / train_total\n",
        "        test_acc = 100. * test_correct / test_total\n",
        "\n",
        "\n",
        "        print(f'Epoch {epoch+1}: Train Acc: {train_acc:.2f}%, Test Acc: {test_acc:.2f}%')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            torch.save(model.state_dict(), 'best_cifar10_vit.pth')\n",
        "\n",
        "    print(f'Best Test Accuracy: {best_acc:.2f}%')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Vision Transformer implemented from scratch was successfully trained on the CIFAR-10 dataset.\n",
        "\n",
        "The model achieved a ***Training accuracy*** of ***~95%*** and a ***Best Test accuracy*** of ***~79%***.\n",
        "\n",
        "This indicates that the model exhibits a noticeable gap between training and testing performance. This behavior is expected for models trained on small datasets such as CIFAR-10.\n",
        "\n"
      ],
      "metadata": {
        "id": "7Vb3GKYUHwx5"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}