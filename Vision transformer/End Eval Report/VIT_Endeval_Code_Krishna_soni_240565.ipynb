{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "d912c31b"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from dataclasses import dataclass\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "\n",
        "@dataclass\n",
        "class SiglipVisionConfig:\n",
        "    num_transformer_layers: int = 6\n",
        "    input_channels: int = 3\n",
        "    input_image_size: int = 32\n",
        "    image_patch_size: int = 4\n",
        "    attention_heads_count: int = 8\n",
        "    model_hidden_size: int = 384\n",
        "    mlp_intermediate_size: int = 1536\n",
        "    output_classes_count: int = 10\n",
        "    layer_norm_epsilon: float = 1e-6\n",
        "    attention_dropout_rate: float = 0.1\n",
        "    main_dropout_rate: float = 0.1\n",
        "\n",
        "def get_cifar10_transforms():\n",
        "\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.247, 0.243, 0.261])\n",
        "    ])\n",
        "\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.247, 0.243, 0.261])\n",
        "    ])\n",
        "    return train_transform, test_transform"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PBcRHfK7ePgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a648c5e7"
      },
      "source": [
        "class SiglipVisionEmbeddings(nn.Module):\n",
        "    def __init__(self, config: SiglipVisionConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.input_channels = config.input_channels\n",
        "        self.embed_dim = config.model_hidden_size\n",
        "        self.input_image_size = config.input_image_size\n",
        "        self.image_patch_size = config.image_patch_size\n",
        "\n",
        "        self.patch_embedding = nn.Conv2d(\n",
        "            in_channels=self.input_channels,\n",
        "            out_channels=self.embed_dim,\n",
        "            kernel_size=self.image_patch_size,\n",
        "            stride=self.image_patch_size,\n",
        "            padding='valid'\n",
        "        )\n",
        "\n",
        "        self.num_patches = (self.input_image_size // self.image_patch_size) ** 2\n",
        "        self.num_positions = self.num_patches + 1\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, self.embed_dim))\n",
        "        self.position_embedding = nn.Embedding(self.num_positions, self.embed_dim)\n",
        "\n",
        "        self.register_buffer(\n",
        "            \"position_ids\",\n",
        "            torch.arange(self.num_positions).expand(1, -1),\n",
        "            persistent=False\n",
        "        )\n",
        "        self.dropout = nn.Dropout(config.main_dropout_rate)\n",
        "\n",
        "    def forward(self, pixel_values: torch.FloatTensor) -> torch.Tensor:\n",
        "        B, C, H, W = pixel_values.shape\n",
        "\n",
        "        patch_embeds = self.patch_embedding(pixel_values)\n",
        "        embeddings = patch_embeds.flatten(2).transpose(1, 2)\n",
        "\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        embeddings = torch.cat((cls_tokens, embeddings), dim=1)\n",
        "\n",
        "        embeddings = embeddings + self.position_embedding(self.position_ids)\n",
        "        return self.dropout(embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92698be9"
      },
      "source": [
        "class SiglipMLP(nn.Module):\n",
        "    def __init__(self, config: SiglipVisionConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.fc1 = nn.Linear(config.model_hidden_size, config.mlp_intermediate_size)\n",
        "        self.fc2 = nn.Linear(config.mlp_intermediate_size, config.model_hidden_size)\n",
        "        self.dropout = nn.Dropout(config.main_dropout_rate)\n",
        "\n",
        "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
        "        hidden_states = self.fc1(hidden_states)\n",
        "        hidden_states = F.gelu(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.fc2(hidden_states)\n",
        "        return hidden_states"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc030760"
      },
      "source": [
        "class SiglipAttention(nn.Module):\n",
        "    def __init__(self, config: SiglipVisionConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.embed_dim = config.model_hidden_size\n",
        "        self.num_heads = config.attention_heads_count\n",
        "        self.head_dim = self.embed_dim // self.num_heads\n",
        "        self.dropout = config.attention_dropout_rate\n",
        "\n",
        "        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=True)\n",
        "        self.k_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=True)\n",
        "        self.v_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=True)\n",
        "\n",
        "        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=True)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        B, T, C = hidden_states.shape\n",
        "\n",
        "        q_states = self.q_proj(hidden_states).view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2)\n",
        "        k_states = self.k_proj(hidden_states).view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2)\n",
        "        v_states = self.v_proj(hidden_states).view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2)\n",
        "\n",
        "        attn_weights = (q_states @ k_states.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
        "\n",
        "        attn_weights = F.dropout(attn_weights, p=self.dropout, training=self.training)\n",
        "\n",
        "        attn_outs = attn_weights @ v_states\n",
        "\n",
        "        attn_outs = attn_outs.transpose(1, 2).reshape(B, T, C)\n",
        "        return self.out_proj(attn_outs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f259799c"
      },
      "source": [
        "class SiglipEncoderLayer(nn.Module):\n",
        "    def __init__(self, config: SiglipVisionConfig):\n",
        "        super().__init__()\n",
        "        self.embed_dim = config.model_hidden_size\n",
        "        self.self_attn = SiglipAttention(config)\n",
        "        self.layer_norm1 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_epsilon)\n",
        "        self.mlp = SiglipMLP(config)\n",
        "        self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_epsilon)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        residual = hidden_states\n",
        "        hidden_states = self.layer_norm1(hidden_states)\n",
        "        hidden_states = self.self_attn(hidden_states)\n",
        "        hidden_states += residual\n",
        "\n",
        "        residual = hidden_states\n",
        "        hidden_states = self.layer_norm2(hidden_states)\n",
        "        hidden_states = self.mlp(hidden_states)\n",
        "        hidden_states += residual\n",
        "        return hidden_states"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78baca2b"
      },
      "source": [
        "class SiglipEncoder(nn.Module):\n",
        "    def __init__(self, config: SiglipVisionConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.layers = nn.ModuleList([SiglipEncoderLayer(config) for _ in range(config.num_transformer_layers)])\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        for layer in self.layers:\n",
        "            hidden_states = layer(hidden_states)\n",
        "        return hidden_states"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93e0b1df"
      },
      "source": [
        "class CIFAR10VisionTransformer(nn.Module):\n",
        "    def __init__(self, config: SiglipVisionConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.embeddings = SiglipVisionEmbeddings(config)\n",
        "        self.encoder = SiglipEncoder(config)\n",
        "        self.layer_norm = nn.LayerNorm(config.model_hidden_size, eps=config.layer_norm_epsilon)\n",
        "\n",
        "        self.classifier = nn.Linear(config.model_hidden_size, config.output_classes_count)\n",
        "\n",
        "    def forward(self, pixel_values):\n",
        "        hidden_states = self.embeddings(pixel_values)\n",
        "        hidden_states = self.encoder(hidden_states)\n",
        "        hidden_states = self.layer_norm(hidden_states)\n",
        "\n",
        "        cls_token = hidden_states[:, 0]\n",
        "        logits = self.classifier(cls_token)\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7545a3d"
      },
      "source": [
        "class SiglipVisionEmbeddings(nn.Module):\n",
        "    def __init__(self, config: SiglipVisionConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.input_channels = config.input_channels\n",
        "        self.embedding_dimension = config.model_hidden_size\n",
        "        self.input_image_size = config.input_image_size\n",
        "        self.image_patch_size = config.image_patch_size\n",
        "\n",
        "        self.patch_embedding = nn.Conv2d(\n",
        "            in_channels=self.input_channels,\n",
        "            out_channels=self.embedding_dimension,\n",
        "            kernel_size=self.image_patch_size,\n",
        "            stride=self.image_patch_size,\n",
        "            padding='valid'\n",
        "        )\n",
        "\n",
        "        self.total_patches = (self.input_image_size // self.image_patch_size) ** 2\n",
        "        self.total_positions = self.total_patches + 1\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, self.embedding_dimension))\n",
        "        self.positional_embedding_layer = nn.Embedding(self.total_positions, self.embedding_dimension)\n",
        "\n",
        "        self.register_buffer(\n",
        "            \"position_indices\",\n",
        "            torch.arange(self.total_positions).expand(1, -1),\n",
        "            persistent=False\n",
        "        )\n",
        "        self.dropout = nn.Dropout(config.main_dropout_rate)\n",
        "\n",
        "    def forward(self, pixel_values: torch.FloatTensor) -> torch.Tensor:\n",
        "        B, C, H, W = pixel_values.shape\n",
        "\n",
        "        patch_embeds = self.patch_embedding(pixel_values)\n",
        "        embeddings = patch_embeds.flatten(2).transpose(1, 2)\n",
        "\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        embeddings = torch.cat((cls_tokens, embeddings), dim=1)\n",
        "\n",
        "        embeddings = embeddings + self.positional_embedding_layer(self.position_indices)\n",
        "        return self.dropout(embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fed99a8e"
      },
      "source": [
        "class SiglipMLP(nn.Module):\n",
        "    def __init__(self, config: SiglipVisionConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.feed_forward_layer1 = nn.Linear(config.model_hidden_size, config.mlp_intermediate_size)\n",
        "        self.feed_forward_layer2 = nn.Linear(config.mlp_intermediate_size, config.model_hidden_size)\n",
        "        self.dropout = nn.Dropout(config.main_dropout_rate)\n",
        "\n",
        "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
        "        hidden_states = self.feed_forward_layer1(hidden_states)\n",
        "        hidden_states = F.gelu(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.feed_forward_layer2(hidden_states)\n",
        "        return hidden_states"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e6796ec"
      },
      "source": [
        "class SiglipAttention(nn.Module):\n",
        "    def __init__(self, config: SiglipVisionConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.embed_dim = config.model_hidden_size\n",
        "        self.num_heads = config.attention_heads_count\n",
        "        self.attention_head_dimension = self.embed_dim // self.num_heads\n",
        "        self.dropout = config.attention_dropout_rate\n",
        "\n",
        "        self.query_projection = nn.Linear(self.embed_dim, self.embed_dim, bias=True)\n",
        "        self.key_projection = nn.Linear(self.embed_dim, self.embed_dim, bias=True)\n",
        "        self.value_projection = nn.Linear(self.embed_dim, self.embed_dim, bias=True)\n",
        "\n",
        "        self.output_projection = nn.Linear(self.embed_dim, self.embed_dim, bias=True)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        B, T, C = hidden_states.shape\n",
        "\n",
        "        q_states = self.query_projection(hidden_states).view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2)\n",
        "        k_states = self.key_projection(hidden_states).view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2)\n",
        "        v_states = self.value_projection(hidden_states).view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2)\n",
        "\n",
        "        attention_weights = (q_states @ k_states.transpose(-2, -1)) / math.sqrt(self.attention_head_dimension)\n",
        "        attention_weights = F.softmax(attention_weights, dim=-1)\n",
        "\n",
        "        attention_weights = F.dropout(attention_weights, p=self.dropout, training=self.training)\n",
        "\n",
        "        attention_outputs = attention_weights @ v_states\n",
        "\n",
        "        attention_outputs = attention_outputs.transpose(1, 2).reshape(B, T, C)\n",
        "        return self.output_projection(attention_outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aa15cf1"
      },
      "source": [
        "class SiglipEncoderLayer(nn.Module):\n",
        "    def __init__(self, config: SiglipVisionConfig):\n",
        "        super().__init__()\n",
        "        self.embed_dim = config.model_hidden_size\n",
        "        self.self_attention_block = SiglipAttention(config)\n",
        "        self.first_layer_normalization = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_epsilon)\n",
        "        self.mlp = SiglipMLP(config)\n",
        "        self.second_layer_normalization = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_epsilon)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        residual = hidden_states\n",
        "        hidden_states = self.first_layer_normalization(hidden_states)\n",
        "        hidden_states = self.self_attention_block(hidden_states)\n",
        "        hidden_states += residual\n",
        "\n",
        "        residual = hidden_states\n",
        "        hidden_states = self.second_layer_normalization(hidden_states)\n",
        "        hidden_states = self.mlp(hidden_states)\n",
        "        hidden_states += residual\n",
        "        return hidden_states"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6a756356"
      },
      "source": [
        "class SiglipEncoder(nn.Module):\n",
        "    def __init__(self, config: SiglipVisionConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.transformer_layers = nn.ModuleList([SiglipEncoderLayer(config) for _ in range(config.num_transformer_layers)])\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        for layer in self.transformer_layers:\n",
        "            hidden_states = layer(hidden_states)\n",
        "        return hidden_states"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "010684e9"
      },
      "source": [
        "class CIFAR10VisionTransformer(nn.Module):\n",
        "    def __init__(self, config: SiglipVisionConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.embeddings = SiglipVisionEmbeddings(config)\n",
        "        self.encoder = SiglipEncoder(config)\n",
        "        self.layer_norm = nn.LayerNorm(config.model_hidden_size, eps=config.layer_norm_epsilon)\n",
        "\n",
        "        self.classification_head = nn.Linear(config.model_hidden_size, config.output_classes_count)\n",
        "\n",
        "    def forward(self, pixel_values):\n",
        "        hidden_states = self.embeddings(pixel_values)\n",
        "        hidden_states = self.encoder(hidden_states)\n",
        "        hidden_states = self.layer_norm(hidden_states)\n",
        "\n",
        "        cls_token = hidden_states[:, 0]\n",
        "        logits = self.classification_head(cls_token)\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 886
        },
        "id": "b0883b04",
        "outputId": "09b266a1-cbae-4ab0-bebe-5abe242543b7"
      },
      "source": [
        "def main():\n",
        "    train_transform, test_transform = get_cifar10_transforms()\n",
        "\n",
        "    cifar10_train_dataset = torchvision.datasets.CIFAR10(\n",
        "        root='./data',\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=train_transform\n",
        "    )\n",
        "    cifar10_test_dataset = torchvision.datasets.CIFAR10(\n",
        "        root='./data',\n",
        "        train=False,\n",
        "        download=True,\n",
        "        transform=test_transform\n",
        "    )\n",
        "\n",
        "    train_data_loader = DataLoader(cifar10_train_dataset, batch_size=128, shuffle=True, num_workers=4, pin_memory=True)\n",
        "    test_data_loader = DataLoader(cifar10_test_dataset, batch_size=128, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "    classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "    config = SiglipVisionConfig()\n",
        "    model = CIFAR10VisionTransformer(config).cuda()\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "    model_optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.05)\n",
        "    learning_rate_scheduler = optim.lr_scheduler.CosineAnnealingLR(model_optimizer, T_max=50)\n",
        "\n",
        "\n",
        "\n",
        "    total_epochs = 50\n",
        "    best_test_accuracy = 0.0\n",
        "\n",
        "    for epoch in range(total_epochs):\n",
        "        model.train()\n",
        "        current_train_loss = 0.0\n",
        "        correct_train_predictions = 0\n",
        "        total_train_samples = 0\n",
        "\n",
        "        progress_bar = tqdm(train_data_loader, desc=f'Epoch {epoch+1}/{total_epochs}')\n",
        "        for batch_idx, (batch_images, batch_labels) in enumerate(progress_bar):\n",
        "            batch_images, batch_labels = batch_images.cuda(), batch_labels.cuda()\n",
        "\n",
        "            model_optimizer.zero_grad()\n",
        "            model_outputs = model(batch_images)\n",
        "            batch_loss = loss_function(model_outputs, batch_labels)\n",
        "            batch_loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            model_optimizer.step()\n",
        "\n",
        "            current_train_loss += batch_loss.item()\n",
        "            _, predicted_labels = model_outputs.max(1)\n",
        "            total_train_samples += batch_labels.size(0)\n",
        "            correct_train_predictions += predicted_labels.eq(batch_labels).sum().item()\n",
        "\n",
        "            progress_bar.set_postfix({\n",
        "                'Loss': f'{batch_loss.item():.3f}',\n",
        "                'Acc': f'{100.*correct_train_predictions/total_train_samples:.1f}%'\n",
        "            })\n",
        "\n",
        "        learning_rate_scheduler.step()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        model.eval()\n",
        "        current_test_loss = 0.0\n",
        "        correct_test_predictions = 0\n",
        "        total_test_samples = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_images, batch_labels in test_data_loader:\n",
        "                batch_images, batch_labels = batch_images.cuda(), batch_labels.cuda()\n",
        "                model_outputs = model(batch_images)\n",
        "                batch_loss = loss_function(model_outputs, batch_labels)\n",
        "\n",
        "                current_test_loss += batch_loss.item()\n",
        "                _, predicted_labels = model_outputs.max(1)\n",
        "                total_test_samples += batch_labels.size(0)\n",
        "                correct_test_predictions += predicted_labels.eq(batch_labels).sum().item()\n",
        "\n",
        "        current_train_accuracy = 100. * correct_train_predictions / total_train_samples\n",
        "        current_test_accuracy = 100. * correct_test_predictions / total_test_samples\n",
        "\n",
        "\n",
        "        print(f'Epoch {epoch+1}: Train Acc: {current_train_accuracy:.2f}%, Test Acc: {current_test_accuracy:.2f}%')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        if current_test_accuracy > best_test_accuracy:\n",
        "            best_test_accuracy = current_test_accuracy\n",
        "            torch.save(model.state_dict(), 'best_cifar10_vit.pth')\n",
        "\n",
        "    print(f'Best Test Accuracy: {best_test_accuracy:.2f}%')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 42.6MB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 1/50: 100%|██████████| 391/391 [01:14<00:00,  5.26it/s, Loss=1.500, Acc=34.9%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Acc: 34.94%, Test Acc: 46.56%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/50: 100%|██████████| 391/391 [01:21<00:00,  4.82it/s, Loss=1.414, Acc=46.2%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Train Acc: 46.16%, Test Acc: 51.23%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/50: 100%|██████████| 391/391 [01:22<00:00,  4.72it/s, Loss=1.213, Acc=50.8%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Train Acc: 50.80%, Test Acc: 52.67%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/50:   0%|          | 1/391 [00:00<02:24,  2.70it/s, Loss=1.449, Acc=53.1%]Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7d3b2ffed8a0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1654, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1618, in _shutdown_workers\n",
            "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
            "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 149, in join\n",
            "    res = self._popen.wait(timeout)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/multiprocessing/popen_fork.py\", line 40, in wait\n",
            "    if not wait([self.sentinel], timeout):\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 1136, in wait\n",
            "    ready = selector.select(timeout)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/selectors.py\", line 415, in select\n",
            "    fd_event_list = self._selector.poll(timeout)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt: \n",
            "Epoch 4/50:   0%|          | 1/391 [00:00<04:02,  1.61it/s, Loss=1.449, Acc=53.1%]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1983990325.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1983990325.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mmodel_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0mcurrent_train_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mtotal_train_samples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}