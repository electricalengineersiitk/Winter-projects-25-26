{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWJ1gEDNFmjU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import math\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use GPU if available (Colab usually has one)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNhY3fPnItU_",
        "outputId": "ed2876b9-c42e-453b-a5ca-c984ce75c9ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.5, 0.5, 0.5),\n",
        "                         std=(0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.5, 0.5, 0.5),\n",
        "                         std=(0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(\n",
        "    root=\"./data\",\n",
        "    train=True,\n",
        "    transform=train_transform,\n",
        "    download=True\n",
        ")\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(\n",
        "    root=\"./data\",\n",
        "    train=False,\n",
        "    transform=test_transform,\n",
        "    download=True\n",
        ")\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "_28vVmkFH96q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0601b22d-b9ea-4af6-a277-c0c70474a8c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:02<00:00, 83.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, img_size=32, patch_size=4, in_channels=3, embed_dim=64):\n",
        "        super().__init__()\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "\n",
        "        # This layer converts a flattened patch into an embedding\n",
        "        self.projection = nn.Linear(patch_size * patch_size * in_channels,\n",
        "                                    embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x shape: (batch_size, 3, 32, 32)\n",
        "        \"\"\"\n",
        "\n",
        "        B, C, H, W = x.shape\n",
        "\n",
        "        # Step 1: split image into patches\n",
        "        x = x.unfold(2, self.patch_size, self.patch_size)\n",
        "        x = x.unfold(3, self.patch_size, self.patch_size)\n",
        "\n",
        "        # Now shape: (B, C, num_patches_h, num_patches_w, patch_size, patch_size)\n",
        "\n",
        "        x = x.contiguous().view(B, C, -1, self.patch_size, self.patch_size)\n",
        "        x = x.permute(0, 2, 1, 3, 4)\n",
        "\n",
        "        # Shape: (B, num_patches, C, patch_size, patch_size)\n",
        "\n",
        "        x = x.flatten(2)\n",
        "\n",
        "        # Shape: (B, num_patches, patch_dim)\n",
        "\n",
        "        # Step 2: project patches to embedding dimension\n",
        "        x = self.projection(x)\n",
        "\n",
        "        # Final shape: (B, num_patches, embed_dim)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "YUrhp1V4IANA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Embeddings(nn.Module):\n",
        "    def __init__(self, num_patches, embed_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.position_embeddings = nn.Parameter(\n",
        "            torch.zeros(1, num_patches + 1, embed_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, D = x.shape\n",
        "\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "\n",
        "        # Add CLS token at beginning\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "        # Add positional embeddings\n",
        "        x = x + self.position_embeddings\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "1_dWcMzNIQ6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, Q, K, V):\n",
        "        d_k = Q.size(-1)\n",
        "\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "        attention = F.softmax(scores, dim=-1)\n",
        "        output = torch.matmul(attention, V)\n",
        "\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "aIIMTVczIU5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim=64, num_heads=4):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
        "        self.attention = ScaledDotProductAttention()\n",
        "        self.fc_out = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, D = x.shape\n",
        "\n",
        "        qkv = self.qkv(x)\n",
        "        qkv = qkv.reshape(B, N, 3, self.num_heads, self.head_dim)\n",
        "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
        "\n",
        "        Q, K, V = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        out = self.attention(Q, K, V)\n",
        "\n",
        "        out = out.transpose(1, 2).contiguous()\n",
        "        out = out.view(B, N, D)\n",
        "\n",
        "        out = self.fc_out(out)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "tPlLhVdmIZcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderBlock(nn.Module):\n",
        "    def __init__(self, embed_dim=64, num_heads=4, mlp_dim=128):\n",
        "        super().__init__()\n",
        "\n",
        "        self.attention = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        self.ffn = nn.Sequential(\n",
        "    nn.Linear(embed_dim, mlp_dim),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.1),\n",
        "    nn.Linear(mlp_dim, embed_dim)\n",
        ")\n",
        "\n",
        "\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attention(x)\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        x = x + self.ffn(x)\n",
        "        x = self.norm2(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "XGeJg0wdIdqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.patch_embed = PatchEmbedding()\n",
        "        self.embed = Embeddings(num_patches=64, embed_dim=64)\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "    TransformerEncoderBlock(),\n",
        "    TransformerEncoderBlock(),\n",
        "    TransformerEncoderBlock(),\n",
        "    TransformerEncoderBlock()\n",
        ")\n",
        "\n",
        "\n",
        "        self.classifier = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_embed(x)\n",
        "        x = self.embed(x)\n",
        "        x = self.encoder(x)\n",
        "\n",
        "        cls_output = x[:, 0]\n",
        "        out = self.classifier(cls_output)\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "q2Mf2cwXIgl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = VisionTransformer().to(device)\n"
      ],
      "metadata": {
        "id": "oWFIA2thIlQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
        "epochs = 20\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_loader):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzmds4lmIxht",
        "outputId": "deabb640-e3fb-4e32-a51e-9806f39b15f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 1.7135\n",
            "Epoch [2/20], Loss: 1.3352\n",
            "Epoch [3/20], Loss: 1.2071\n",
            "Epoch [4/20], Loss: 1.1329\n",
            "Epoch [5/20], Loss: 1.0724\n",
            "Epoch [6/20], Loss: 1.0238\n",
            "Epoch [7/20], Loss: 0.9825\n",
            "Epoch [8/20], Loss: 0.9482\n",
            "Epoch [9/20], Loss: 0.9152\n",
            "Epoch [10/20], Loss: 0.8897\n",
            "Epoch [11/20], Loss: 0.8624\n",
            "Epoch [12/20], Loss: 0.8377\n",
            "Epoch [13/20], Loss: 0.8163\n",
            "Epoch [14/20], Loss: 0.8003\n",
            "Epoch [15/20], Loss: 0.7787\n",
            "Epoch [16/20], Loss: 0.7604\n",
            "Epoch [17/20], Loss: 0.7407\n",
            "Epoch [18/20], Loss: 0.7300\n",
            "Epoch [19/20], Loss: 0.7122\n",
            "Epoch [20/20], Loss: 0.6963\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n"
      ],
      "metadata": {
        "id": "y46E5b7vKyT1",
        "outputId": "13581d8b-55d7-4732-8b5c-21f60f3bb68c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 70.27%\n"
          ]
        }
      ]
    }
  ]
}