{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "A0YlVl-T4hN7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "IMG_SIZE = 32\n",
        "PATCH_SIZE = 4\n",
        "NUM_PATCHES = (IMG_SIZE // PATCH_SIZE) ** 2\n",
        "EMBED_DIM = 128\n",
        "NUM_HEADS = 8\n",
        "NUM_LAYERS = 4\n",
        "MLP_DIM = 256\n",
        "DROPOUT = 0.1\n",
        "BATCH_SIZE = 64\n",
        "LR = 5e-4\n",
        "EPOCHS = 20"
      ],
      "metadata": {
        "id": "bn3NwjDSFktm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Adding basic augmentation to prevent overfitting\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "testloader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "id": "YwsBXB-3FuCP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2faf4dfc-32d3-4e5e-fc6f-d80be0227714"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 46.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ViTClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        #Patch Embedding:(Convolutional approach)\n",
        "        self.patch_embed = nn.Conv2d(3, EMBED_DIM, kernel_size=PATCH_SIZE, stride=PATCH_SIZE)\n",
        "\n",
        "        #Learnable tokens\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, EMBED_DIM))\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, NUM_PATCHES + 1, EMBED_DIM))\n",
        "        self.dropout = nn.Dropout(DROPOUT)\n",
        "\n",
        "        #Transformer Encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=EMBED_DIM,\n",
        "            nhead=NUM_HEADS,\n",
        "            dim_feedforward=MLP_DIM,\n",
        "            dropout=DROPOUT,\n",
        "            activation='gelu',\n",
        "            batch_first=True,\n",
        "            norm_first=True #Critical for stability\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=NUM_LAYERS)\n",
        "\n",
        "        #Classification Head\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(EMBED_DIM),\n",
        "            nn.Linear(EMBED_DIM, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_embed(x)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "\n",
        "        cls_tokens = self.cls_token.expand(x.shape[0], -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = x + self.pos_embedding\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        return self.mlp_head(x[:, 0])"
      ],
      "metadata": {
        "id": "IG-61d-zFyrh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ViTClassifier().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-2)\n",
        "\n",
        "print(f\"Training on {device}...\")\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in trainloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in testloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{EPOCHS}] - Loss: {running_loss/len(trainloader):.4f} - Acc: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJXDSKNDF4D2",
        "outputId": "dba34bb9-3528-434c-8c5d-ff455df9d54b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on cuda...\n",
            "Epoch [1/20] - Loss: 1.7796 - Acc: 45.99%\n",
            "Epoch [2/20] - Loss: 1.5024 - Acc: 51.05%\n",
            "Epoch [3/20] - Loss: 1.4018 - Acc: 52.63%\n",
            "Epoch [4/20] - Loss: 1.3371 - Acc: 54.91%\n",
            "Epoch [5/20] - Loss: 1.2823 - Acc: 57.10%\n",
            "Epoch [6/20] - Loss: 1.2343 - Acc: 59.25%\n",
            "Epoch [7/20] - Loss: 1.1934 - Acc: 61.41%\n",
            "Epoch [8/20] - Loss: 1.1557 - Acc: 61.13%\n",
            "Epoch [9/20] - Loss: 1.1218 - Acc: 62.43%\n",
            "Epoch [10/20] - Loss: 1.0884 - Acc: 63.79%\n",
            "Epoch [11/20] - Loss: 1.0567 - Acc: 65.35%\n",
            "Epoch [12/20] - Loss: 1.0215 - Acc: 64.92%\n",
            "Epoch [13/20] - Loss: 1.0016 - Acc: 66.41%\n",
            "Epoch [14/20] - Loss: 0.9782 - Acc: 66.92%\n",
            "Epoch [15/20] - Loss: 0.9461 - Acc: 67.86%\n",
            "Epoch [16/20] - Loss: 0.9349 - Acc: 68.53%\n",
            "Epoch [17/20] - Loss: 0.9090 - Acc: 68.82%\n",
            "Epoch [18/20] - Loss: 0.8864 - Acc: 69.04%\n",
            "Epoch [19/20] - Loss: 0.8664 - Acc: 71.02%\n",
            "Epoch [20/20] - Loss: 0.8502 - Acc: 70.64%\n"
          ]
        }
      ]
    }
  ]
}