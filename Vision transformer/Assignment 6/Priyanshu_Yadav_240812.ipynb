{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Assignment #6\n",
        "\n",
        "Vision Transformer\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "5CDBR-mth6ey"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SUBMISSION INSTRUCTIONS**\n",
        "\n",
        "First make a copy of this colab file and then solve the assignment and upload your final notebook on github.\n",
        "\n",
        "Before uploading your downloaded notebook, RENAME the file as rollno_name.ipynb\n",
        "\n",
        "Submission Deadline : 31/01/2026 Saturday EOD i.e before 11:59 PM\n",
        "\n",
        "The deadline is strict, Late submissions will incur penalty\n",
        "\n",
        "Note that you have to upload your solution on the github page of the project Vision Transformer and **under Assignment 6**\n",
        "\n",
        "And remember to keep title of your pull request to be ViT_name_rollno_assgn6\n",
        "\n",
        "Github Submission repo -\n",
        "https://github.com/electricalengineersiitk/Winter-projects-25-26/tree/main/Vision%20transformer/Assignment%206"
      ],
      "metadata": {
        "id": "z8HhWU2zjYu8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part A - Data and tokens"
      ],
      "metadata": {
        "id": "K7PUweRobXFq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Build a tiny toy dataset with pandas\n",
        "Create a pandas DataFrame with columns text and label.\n",
        "- Include at least 12 short sentences (3-10 words each).\n",
        "- The label is 0/1 (e.g., positive vs negative sentiment).\n",
        "- Shuffle rows and split into train/test (80/20) using a fixed random seed.\n",
        "Return: df_train, df_test."
      ],
      "metadata": {
        "id": "mSacHQ0CblC-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8LzSCSbMarwT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "def make_toy_dataset(seed: int = 42):\n",
        "   # 1. Define a list of at least 12 short sentences (3-10 words each).\n",
        "    sentences = [\n",
        "        \"This movie is absolutely fantastic and thrilling\",\n",
        "        \"I really enjoyed this book, it was great\",\n",
        "        \"What a wonderful day for a picnic\",\n",
        "        \"The service was excellent, very friendly staff\",\n",
        "        \"Absolutely loved the food at that restaurant\",\n",
        "        \"Terrible experience, would not recommend this place\",\n",
        "        \"This product broke after only one use\",\n",
        "        \"I found the plot quite boring and slow\",\n",
        "        \"The customer support was unhelpful and rude\",\n",
        "        \"Never buying from them again, very disappointed\",\n",
        "        \"A decent effort, but could be much better\",\n",
        "        \"It was okay, nothing special really\",\n",
        "        \"Highly inspiring, truly a masterpiece\",\n",
        "        \"Simply the best decision I ever made\"\n",
        "    ]\n",
        "\n",
        "    # 2. Define a corresponding list of binary labels (0 or 1) for each sentence.\n",
        "    # 1 for positive, 0 for negative/neutral\n",
        "    labels = [\n",
        "        1, 1, 1, 1, 1,\n",
        "        0, 0, 0, 0, 0,\n",
        "        0, 0,\n",
        "        1, 1\n",
        "    ]\n",
        "\n",
        "    # 3. Create a pandas DataFrame named df with two columns: 'text' and 'label'.\n",
        "    df = pd.DataFrame({\n",
        "        'text': sentences,\n",
        "        'label': labels\n",
        "    })\n",
        "\n",
        "    # 4. Shuffle the rows of the DataFrame df in-place using df.sample(frac=1, random_state=seed).\n",
        "    # Make sure to reset the index after shuffling using reset_index(drop=True).\n",
        "    df = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
        "\n",
        "    # 5. Calculate the split index for 80% training data.\n",
        "    train_size = int(0.8 * len(df))\n",
        "\n",
        "    # 6. Split the shuffled DataFrame df into df_train (first 80% of rows) and df_test (remaining 20% of rows).\n",
        "    df_train = df.iloc[:train_size]\n",
        "    df_test = df.iloc[train_size:]\n",
        "\n",
        "    return df_train, df_test"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. Clean and tokenize text\n",
        "\n",
        "Implement a basic cleaner: lowercase, strip, replace multiple spaces with one, and remove punctuation\n",
        "(.,!?;:).\n",
        "Tokenize by whitespace.\n",
        "Add a new column tokens that stores a list of tokens per row.\n",
        "Return the updated DataFrame."
      ],
      "metadata": {
        "id": "GkMRbbJDbsc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "def clean_text(s: str) -> str:\n",
        "  # 1a. Convert to lowercase\n",
        "    s = s.lower()\n",
        "    # 1b. Remove leading/trailing whitespace\n",
        "    s = s.strip()\n",
        "    # 1c. Replace multiple spaces with a single space\n",
        "    s = re.sub(r'\\s+', ' ', s)\n",
        "    # 1d. Remove punctuation characters (.,!?;:)\n",
        "    s = re.sub(r'[.,!?;:]', '', s)\n",
        "    return s\n",
        "\n",
        "def add_tokens_column(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Adds df['tokens'] = list[str].\"\"\"\n",
        "    # 2a. Apply the clean_text function to the 'text' column\n",
        "    df['cleaned_text'] = df['text'].apply(clean_text)\n",
        "    # 2b. Tokenize each cleaned sentence by splitting it by whitespace\n",
        "    df['tokens'] = df['cleaned_text'].apply(lambda x: x.split())\n",
        "    # Drop the temporary 'cleaned_text' column if not needed further\n",
        "    df = df.drop(columns=['cleaned_text'])\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "4siOVxuCbnqs"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. Build a vocabulary + token/id mappings\n",
        "\n",
        "Build token2id and id2token using the training tokens.\n",
        "Include special tokens: [PAD], [UNK], [BOS], [EOS] at the beginning.\n",
        "Add tokens that occur at least min_freq times.\n",
        "Return: token2id (dict), id2token (list)."
      ],
      "metadata": {
        "id": "mJpgICVAbu2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from typing import Dict, List\n",
        "SPECIALS = ['[PAD]', '[UNK]', '[BOS]', '[EOS]']\n",
        "def build_vocab(list_of_token_lists, min_freq: int = 1):\n",
        "  # 1. Initialize token2id as an empty dictionary and id2token as an empty list.\n",
        "    token2id: Dict[str, int] = {}\n",
        "    id2token: List[str] = []\n",
        "\n",
        "    # 2. Iterate through the SPECIALS list. For each special token, add it to id2token\n",
        "    #    and map it to its index in token2id. Ensure these special tokens are at the\n",
        "    #    beginning of your vocabulary.\n",
        "    for token in SPECIALS:\n",
        "        token2id[token] = len(id2token)\n",
        "        id2token.append(token)\n",
        "\n",
        "    # 3. Flatten the list_of_token_lists (e.g., from df_train['tokens']) into a single list of all tokens.\n",
        "    all_tokens = [token for sublist in list_of_token_lists for token in sublist]\n",
        "\n",
        "    # 4. Use collections.Counter to count the frequency of each token in the flattened list.\n",
        "    token_counts = Counter(all_tokens)\n",
        "\n",
        "    # 5. Iterate through the token counts. For each token, if its frequency is greater\n",
        "    #    than or equal to min_freq and it is not already in token2id (to avoid adding\n",
        "    #    special tokens again), add it to id2token and map it to its new index in token2id.\n",
        "    for token, count in token_counts.items():\n",
        "        if count >= min_freq and token not in token2id:\n",
        "            token2id[token] = len(id2token)\n",
        "            id2token.append(token)\n",
        "\n",
        "            return token2id, id2token\n"
      ],
      "metadata": {
        "id": "Oh8qMuzOa0-M"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. Convert tokens to ids + pad to a batch\n",
        "\n",
        "Implement tokens_to_ids for one sequence.\n",
        "Implement pad_batch that takes a list of id sequences and returns:\n",
        "- X: int array (B,T) padded with pad_id\n",
        "- pad_mask: bool array (B,T) where True means 'real token' and False means padding"
      ],
      "metadata": {
        "id": "tZpBXyaKb69F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def tokens_to_ids(tokens, token2id, unk_token='[UNK]'):\n",
        "    \"\"\"Converts a list of tokens to a list of their corresponding integer IDs.\"\"\"\n",
        "    ids = []\n",
        "    unk_id = token2id[unk_token] # Get ID for unknown token once\n",
        "    for token in tokens:\n",
        "        ids.append(token2id.get(token, unk_id))\n",
        "    return ids\n",
        "\n",
        "def pad_batch(id_seqs, pad_id: int):\n",
        "    \"\"\"Return X (B,T) and pad_mask (B,T)\"\"\"\n",
        "    # Determine batch size B and max sequence length T\n",
        "    B = len(id_seqs)\n",
        "    T = max(len(seq) for seq in id_seqs)\n",
        "\n",
        "    # Initialize X with pad_id\n",
        "    X = np.full((B, T), pad_id, dtype=int)\n",
        "\n",
        "    # Initialize pad_mask with False\n",
        "    pad_mask = np.full((B, T), False, dtype=bool)\n",
        "\n",
        "    # Populate X and pad_mask\n",
        "    for i, id_sequence in enumerate(id_seqs):\n",
        "        seq_len = len(id_sequence)\n",
        "        X[i, :seq_len] = id_sequence\n",
        "        pad_mask[i, :seq_len] = True\n",
        "\n",
        "        return X, pad_mask\n"
      ],
      "metadata": {
        "id": "IAHXRbwBa5gx"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part B - Core Transformer math"
      ],
      "metadata": {
        "id": "5CSuz9Nab_O4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. Embedding lookup\n",
        "\n",
        "Implement an embedding table E of shape (V,D) initialized from a normal distribution (mean 0, std 0.02).\n",
        "Given token ids X (B,T), return embeddings of shape (B,T,D) using NumPy indexing.\n"
      ],
      "metadata": {
        "id": "VMCKexcUcCIG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def init_embeddings(vocab_size: int, d_model: int, seed: int = 0):\n",
        "   # 1. Initialize the random seed.\n",
        "    np.random.seed(seed)\n",
        "    # 2. Create a NumPy array E of shape (vocab_size, d_model).\n",
        "    #    Values should be sampled from a normal distribution with mean=0 and std=0.02.\n",
        "    E = np.random.normal(loc=0.0, scale=0.02, size=(vocab_size, d_model))\n",
        "    return E\n",
        "def embed(X: np.ndarray, E: np.ndarray):\n",
        "    \"\"\"X: (B,T) int, E: (V,D) -> out: (B,T,D).\"\"\"\n",
        "    # 1. Use NumPy advanced indexing to lookup the embeddings for each token in X.\n",
        "    #    X contains token IDs. E is the embedding table.\n",
        "    out = E[X]\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "MjcboKYZcGFy"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. Sinusoidal positional encoding\n",
        "\n",
        "Implement the classic sinusoidal positional encoding PE of shape (T,D).\n",
        "Then add it to token embeddings (B,T,D).\n",
        "Make sure your implementation works for both even and odd D."
      ],
      "metadata": {
        "id": "roaKv_LLcH9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def sinusoidal_positional_encoding(T: int, D: int):\n",
        "    # 1a. Create a NumPy array PE of shape (T, D) initialized with zeros.\n",
        "    PE = np.zeros((T, D))\n",
        "\n",
        "    # 1b. Generate a position array representing token positions from 0 to T-1.\n",
        "    position = np.arange(T)[:, np.newaxis] # Shape (T, 1)\n",
        "\n",
        "    # 1c. Generate a div_term array for the denominator.\n",
        "    # For each dimension index k from 0 to D-1, the denominator term is 10000^(2i/D),\n",
        "    # where i = k // 2. Use np.exp(np.arange(0, D, 2) * - (np.log(10000.0) / D))\n",
        "    # for the 1 / (10000^(2i/D)) part to handle the 2i exponent.\n",
        "    div_term = np.exp(np.arange(0, D, 2) * -(np.log(10000.0) / D)) # Shape (D/2,)\n",
        "\n",
        "    # 1d. Apply the sinusoidal functions:\n",
        "    # i. For even dimension indices (0, 2, 4, ...), compute np.sin(position[:, np.newaxis] / div_term).\n",
        "    #    Assign these values to PE[:, 0::2].\n",
        "    PE[:, 0::2] = np.sin(position * div_term) # Broadcasting (T,1) * (1, D/2) = (T, D/2)\n",
        "\n",
        "    # ii. For odd dimension indices (1, 3, 5, ...), compute np.cos(position[:, np.newaxis] / div_term).\n",
        "    #     Assign these values to PE[:, 1::2].\n",
        "    # Handle odd dimensions if D is odd: If D is odd, div_term still has D/2 elements (integer division).\n",
        "    # The last odd column will use the last element of div_term.\n",
        "    # np.arange(0, D, 2) creates indices 0, 2, ..., up to D-2 (or D-1 if D is odd, but still stepping by 2).\n",
        "    # The div_term correctly corresponds to i in 2i for D/2 terms.\n",
        "    # This means div_term *should* be applied to both sin and cos parts. There are D/2 pairs of (sin, cos).\n",
        "    PE[:, 1::2] = np.cos(position * div_term)\n",
        "\n",
        "    return PE\n",
        "\n",
        "def add_positional_encoding(X_emb: np.ndarray, PE: np.ndarray):\n",
        "    \"\"\"X_emb: (B,T,D), PE: (T,D) -> (B,T,D).\"\"\"\n",
        "    # 2a. Add the PE matrix to the X_emb tensor.\n",
        "    # Ensure broadcasting works correctly, as X_emb has shape (B, T, D) and PE has shape (T, D).\n",
        "    X_emb_pe = X_emb + PE\n",
        "    return X_emb_pe\n"
      ],
      "metadata": {
        "id": "owZSRIeocLtS"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. Scaled dot-product attention with masking\n",
        "\n",
        "Implement scaled dot-product attention:\n",
        "Attention(Q,K,V) = softmax((Q @ K^T) / sqrt(dk) + mask) @ V\n",
        "Inputs: Q,K,V are (B,H,T,Dh). Mask is boolean broadcastable to (B,H,T,T) where False means 'mask out'.\n",
        "Requirements:\n",
        "- Use a numerically stable softmax (subtract max).\n",
        "- Convert boolean mask to large negative values before softmax.\n",
        "Return: context (B,H,T,Dh) and attention weights (B,H,T,T)."
      ],
      "metadata": {
        "id": "k85DsUTgcQbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def softmax(x: np.ndarray, axis: int = -1):\n",
        "  # 1b. Subtract the maximum value along the specified axis for numerical stability\n",
        "    x_stable = x - np.max(x, axis=axis, keepdims=True)\n",
        "    # 1c. Compute the exponentiated values\n",
        "    exp_x = np.exp(x_stable)\n",
        "    # 1d. Divide the exponentiated values by their sum along the specified axis\n",
        "    sum_exp_x = np.sum(exp_x, axis=axis, keepdims=True)\n",
        "    y = exp_x / sum_exp_x\n",
        "    return y\n",
        "\n",
        "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
        "    \"\"\"Q,K,V: (B,H,T,Dh), mask: bool broadcastable to (B,H,T,T).\"\"\"\n",
        "    # 2a. Calculate the dot product of Q and K transpose\n",
        "    # K.transpose(0, 1, 3, 2) changes (B,H,T,Dh) to (B,H,Dh,T)\n",
        "    scores = Q @ K.transpose(0, 1, 3, 2)\n",
        "\n",
        "    # 2b. Scale the scores by dividing by the square root of Dh\n",
        "    Dh = K.shape[-1]\n",
        "    scores = scores / np.sqrt(Dh)\n",
        "\n",
        "    # 2c. If a mask is provided\n",
        "    if mask is not None:\n",
        "        # Convert boolean mask to numerical mask: False -> -1e9, True -> 0\n",
        "        # Using np.where to apply -1e9 where mask is False (masked out)\n",
        "        # The mask is typically (B,T) or (B,1,1,T) for padding, or (B,1,T,T) for causality.\n",
        "        # It needs to be broadcastable to (B,H,T,T).\n",
        "        numerical_mask = np.where(mask, 0.0, -1e9)\n",
        "        scores = scores + numerical_mask # Addition handles broadcasting\n",
        "\n",
        "    # 2d. Apply the softmax function to the modified scores along the last axis\n",
        "    attention_weights = softmax(scores, axis=-1)\n",
        "\n",
        "    # 2e. Compute the context vector\n",
        "    context = attention_weights @ V\n",
        "\n",
        "    # 2f. Return context and attention weights\n",
        "    return context, attention_weights\n"
      ],
      "metadata": {
        "id": "bMWieoyZcTSE"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. Multi-head self-attention (MHA)\n",
        "\n",
        "Implement multi-head self-attention for input X (B,T,D).\n",
        "- Project to Q,K,V using weight matrices Wq,Wk,Wv each (D,D).\n",
        "- Reshape/split into heads -> (B,H,T,Dh) where Dh=D/H.\n",
        "- Apply scaled dot-product attention with a pad mask (B,T) (broadcast it appropriately).\n",
        "- Concatenate heads and apply output projection Wo (D,D).\n",
        "Return: out (B,T,D) and attention weights (B,H,T,T)."
      ],
      "metadata": {
        "id": "gM5ZKq_jcTEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def linear(x: np.ndarray, W: np.ndarray, b=None):\n",
        " # 1. Perform matrix multiplication: x @ W.\n",
        "    y = x @ W\n",
        "    # 2. If bias b is provided, add it to the result.\n",
        "    if b is not None:\n",
        "        y = y + b # Bias is (D_out,) and will broadcast correctly\n",
        "    return y\n",
        "\n",
        "def split_heads(x: np.ndarray, n_heads: int): # (B,T,D) -> (B,H,T,Dh)\n",
        "    # Get batch size, sequence length, and model dimension\n",
        "    B, T, D = x.shape\n",
        "    # Calculate dimension per head\n",
        "    Dh = D // n_heads\n",
        "    # 1. Reshape x from (B, T, D) to (B, T, n_heads, Dh).\n",
        "    # 2. Transpose the dimensions to get (B, n_heads, T, Dh).\n",
        "    xh = x.reshape(B, T, n_heads, Dh).transpose(0, 2, 1, 3)\n",
        "    return xh\n",
        "\n",
        "def combine_heads(xh: np.ndarray): # (B,H,T,Dh) -> (B,T,D)\n",
        "    # Get batch size, number of heads, sequence length, and dimension per head\n",
        "    B, n_heads, T, Dh = xh.shape\n",
        "    # 1. Transpose back to (B, T, n_heads, Dh).\n",
        "    # 2. Reshape to (B, T, D) where D = n_heads * Dh.\n",
        "    x = xh.transpose(0, 2, 1, 3).reshape(B, T, n_heads * Dh)\n",
        "    return x\n",
        "\n",
        "def mha_self_attention(X, Wq, Wk, Wv, Wo, n_heads: int, pad_mask=None): # X: (B,T,D)\n",
        "    # D is the model dimension\n",
        "    D = X.shape[-1]\n",
        "\n",
        "    # 4a. Project X to Q, K, V using linear layers\n",
        "    Q = linear(X, Wq)\n",
        "    K = linear(X, Wk)\n",
        "    V = linear(X, Wv)\n",
        "\n",
        "    # 4b. Split Q, K, V into multiple heads\n",
        "    Qh = split_heads(Q, n_heads)\n",
        "    Kh = split_heads(K, n_heads)\n",
        "    Vh = split_heads(V, n_heads)\n",
        "\n",
        "    # 4c. Apply scaled dot-product attention with pad_mask\n",
        "    # Create an attention mask from pad_mask (B, T) to (B, 1, T, T)\n",
        "    # If pad_mask is (B,T), we want to mask out interactions where either query or key is padded.\n",
        "    # (B, T, 1) * (B, 1, T) -> (B, T, T) then expand to (B, 1, T, T) for broadcasting with (B,H,T,T)\n",
        "    attention_mask_sps = None\n",
        "    if pad_mask is not None:\n",
        "        # Make sure that if a query token is padding (False), it cannot attend to anything\n",
        "        # and if a key token is padding (False), no query can attend to it.\n",
        "        # `pad_mask` is (B,T) where True means 'real token' and False means 'padding'\n",
        "        # Construct a boolean mask for (B, T, T) where True means 'keep' and False means 'mask out'.\n",
        "        # (B, T, 1) & (B, 1, T) -> (B, T, T)\n",
        "        seq_len_mask = pad_mask[:, np.newaxis, :] & pad_mask[:, :, np.newaxis]\n",
        "        # Expand to (B, 1, T, T) for broadcasting across heads\n",
        "        attention_mask_sps = seq_len_mask[:, np.newaxis, :, :]\n",
        "\n",
        "    context_h, attn = scaled_dot_product_attention(Qh, Kh, Vh, mask=attention_mask_sps)\n",
        "\n",
        "    # 4d. Concatenate heads\n",
        "    context = combine_heads(context_h)\n",
        "\n",
        "    # 4e. Apply final output projection Wo\n",
        "    out = linear(context, Wo)\n",
        "\n",
        "    # 5. Return output and attention weights\n",
        "    return out, attn"
      ],
      "metadata": {
        "id": "Vzr0fP7HcYeY"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9. LayerNorm + residual connection\n",
        "\n",
        "Implement LayerNorm for X (B,T,D) using learnable gamma and beta of shape (D,).\n",
        "Then implement residual_add_and_norm(Y, X, gamma, beta) that returns LayerNorm(X + Y)."
      ],
      "metadata": {
        "id": "e3w9akBdcX__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def layer_norm(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, eps: float = 1e-5):\n",
        "  # 1a. Calculate the mean of X along the last dimension (D), keeping dimensions for broadcasting.\n",
        "    mean = np.mean(X, axis=-1, keepdims=True)\n",
        "    # 1b. Calculate the variance of X along the last dimension (D), keeping dimensions for broadcasting.\n",
        "    variance = np.var(X, axis=-1, keepdims=True)\n",
        "\n",
        "    # 1c. Normalize X by subtracting the mean and dividing by the standard deviation\n",
        "    #     (square root of variance + epsilon).\n",
        "    # This results in a tensor of the same shape as X.\n",
        "    normalized_X = (X - mean) / np.sqrt(variance + eps)\n",
        "\n",
        "    # 1d. Scale the normalized X by gamma and shift by beta.\n",
        "    #     gamma and beta are (D,) shaped arrays and will broadcast correctly across (B, T) dimensions.\n",
        "    Y = normalized_X * gamma + beta\n",
        "\n",
        "    # 1e. Return the normalized and scaled output Y.\n",
        "    return Y\n",
        "\n",
        "def residual_add_and_norm(Y: np.ndarray, X: np.ndarray, gamma: np.ndarray, beta: np.ndarray):\n",
        "    # 2a. Add Y to X (element-wise addition, implicitly handling batch, sequence length, and feature dimensions).\n",
        "    added_output = X + Y\n",
        "\n",
        "    # 2b. Apply the layer_norm function to the result of the addition,\n",
        "    #     using the provided gamma and beta parameters.\n",
        "    Z = layer_norm(added_output, gamma, beta)\n",
        "\n",
        "    # 2c. Return the final normalized output Z.\n",
        "    return Z"
      ],
      "metadata": {
        "id": "JFfAjriHccnz"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10. Position-wise FeedForward network\n",
        "\n",
        "Implement FFN: FFN(X) = relu(X @ W1 + b1) @ W2 + b2\n",
        "Shapes: X (B,T,D), W1 (D,Dff), b1 (Dff,), W2 (Dff,D), b2 (D,)\n",
        "Return: (B,T,D)."
      ],
      "metadata": {
        "id": "vLI0APBzceTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def relu(x: np.ndarray):\n",
        " # 1. Apply the Rectified Linear Unit (ReLU) activation function.\n",
        "    #    This means returning x if x > 0, and 0 otherwise.\n",
        "    y = np.maximum(0, x)\n",
        "    return y\n",
        "def feed_forward(X: np.ndarray, W1: np.ndarray, b1: np.ndarray, W2: np.ndarray, b2: np.ndarray):\n",
        "    # X: (B,T,D)\n",
        "    # W1: (D,Dff), b1: (Dff,)\n",
        "    # W2: (Dff,D), b2: (D,)\n",
        "\n",
        "    # 1. First linear transformation: X @ W1 + b1\n",
        "    #    X is (B,T,D), W1 is (D,Dff). Result is (B,T,Dff).\n",
        "    #    b1 is (Dff,) and broadcasts correctly.\n",
        "    linear1_output = X @ W1 + b1\n",
        "\n",
        "    # 2. Apply ReLU activation to the output of the first linear transformation.\n",
        "    relu_output = relu(linear1_output)\n",
        "\n",
        "    # 3. Second linear transformation: relu_output @ W2 + b2\n",
        "    #    relu_output is (B,T,Dff), W2 is (Dff,D). Result is (B,T,D).\n",
        "    #    b2 is (D,) and broadcasts correctly.\n",
        "    Y = relu_output @ W2 + b2\n",
        "\n",
        "    # 4. Return the final output of shape (B,T,D).\n",
        "    return Y"
      ],
      "metadata": {
        "id": "oYRKtUR1chXF"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part C - Putting it together"
      ],
      "metadata": {
        "id": "8JUgk3YyclkR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q11. One Transformer encoder block (forward)\n",
        "\n",
        "Implement a single encoder block forward pass:\n",
        "1) MHA = MultiHeadSelfAttention(X) with pad_mask\n",
        "2) X1 = LayerNorm(X + MHA)\n",
        "3) FFN = FeedForward(X1)\n",
        "4) X2 = LayerNorm(X1 + FFN)\n",
        "Return X2.\n",
        "You may pass all parameters explicitly (weights, gamma/beta)."
      ],
      "metadata": {
        "id": "g67wT-WmcoEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encoder_block_forward(X, params, n_heads: int, pad_mask=None):\n",
        "    # Extract parameters for MHA\n",
        "    Wq = params['Wq']\n",
        "    Wk = params['Wk']\n",
        "    Wv = params['Wv']\n",
        "    Wo = params['Wo']\n",
        "\n",
        "    # Extract parameters for LayerNorm after MHA\n",
        "    gamma_mha = params['gamma_mha']\n",
        "    beta_mha = params['beta_mha']\n",
        "\n",
        "    # Extract parameters for FFN\n",
        "    W1 = params['W1']\n",
        "    b1 = params['b1']\n",
        "    W2 = params['W2']\n",
        "    b2 = params['b2']\n",
        "\n",
        "    # Extract parameters for LayerNorm after FFN\n",
        "    gamma_ffn = params['gamma_ffn']\n",
        "    beta_ffn = params['beta_ffn']\n",
        "\n",
        "    # 1) MHA = MultiHeadSelfAttention(X) with pad_mask\n",
        "    mha_output, _ = mha_self_attention(X, Wq, Wk, Wv, Wo, n_heads, pad_mask=pad_mask)\n",
        "\n",
        "    # 2) X1 = LayerNorm(X + MHA) -- using residual_add_and_norm\n",
        "    X1 = residual_add_and_norm(mha_output, X, gamma_mha, beta_mha)\n",
        "\n",
        "    # 3) FFN = FeedForward(X1)\n",
        "    ffn_output = feed_forward(X1, W1, b1, W2, b2)\n",
        "\n",
        "    # 4) X2 = LayerNorm(X1 + FFN) -- using residual_add_and_norm\n",
        "    X2 = residual_add_and_norm(ffn_output, X1, gamma_ffn, beta_ffn)\n",
        "\n",
        "    return X2"
      ],
      "metadata": {
        "id": "ppMLKzXIcsfv"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q12. Sequence classification head + end-to-end demo\n",
        "\n",
        "Create an end-to-end forward pass for a tiny classifier:\n",
        "- Input ids -> embeddings + positional enc\n",
        "- One encoder block\n",
        "- Pooling: take the [BOS] position (t=0) as the sequence representation\n",
        "- Linear head: logits = h0 @ Wcls + bcls with Wcls (D,2), bcls (2,)\n",
        "- Softmax to probabilities\n",
        "Write predict_proba that takes a batch of texts and returns probs (B,2).\n",
        "Include simple sanity checks: shapes, probabilities sum to 1, and masking doesn't crash for different\n",
        "lengths.\n"
      ],
      "metadata": {
        "id": "g-lT714IcunS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_proba(texts, token2id, E, PE, params, Wcls, bcls, n_heads: int):\n",
        "    # 2. Convert the input `texts` to token IDs.\n",
        "    # This involves using clean_text, then tokenizing, then tokens_to_ids.\n",
        "    # Create a temporary DataFrame for easy processing using add_tokens_column logic.\n",
        "    df_temp = pd.DataFrame({'text': texts})\n",
        "    df_temp['cleaned_text'] = df_temp['text'].apply(clean_text)\n",
        "    df_temp['tokens'] = df_temp['cleaned_text'].apply(lambda x: x.split())\n",
        "\n",
        "    # Add [BOS] and [EOS] tokens to each sequence\n",
        "    # BOS_ID and EOS_ID must be retrieved from token2id\n",
        "    bos_id = token2id['[BOS]']\n",
        "    eos_id = token2id['[EOS]']\n",
        "\n",
        "    # Prepend [BOS] and append [EOS] to each token list, then convert to IDs\n",
        "    id_sequences = []\n",
        "    for tokens in df_temp['tokens']:\n",
        "        sequence_with_specials = [token2id['[BOS]']] + tokens + [token2id['[EOS]']]\n",
        "        id_sequences.append(tokens_to_ids(sequence_with_specials, token2id))\n",
        "\n",
        "    # 3. Pad the sequences of token IDs to create X and pad_mask.\n",
        "    pad_id = token2id['[PAD]']\n",
        "    X, pad_mask = pad_batch(id_sequences, pad_id)\n",
        "\n",
        "    # 4. Perform embedding lookup.\n",
        "    X_emb = embed(X, E)\n",
        "\n",
        "    # 5. Add positional encoding.\n",
        "    # Ensure PE matches the current sequence length T\n",
        "    T_current = X.shape[1]\n",
        "    PE_current = PE[:T_current, :]\n",
        "    X_emb_pe = add_positional_encoding(X_emb, PE_current)\n",
        "\n",
        "    # 6. Pass through one encoder block.\n",
        "    # The params dict is expected to contain all necessary MHA, FFN, and LayerNorm weights and biases.\n",
        "    encoder_output = encoder_block_forward(X_emb_pe, params, n_heads, pad_mask=pad_mask)\n",
        "\n",
        "    # 7. Perform pooling: take the [BOS] position (t=0) as the sequence representation\n",
        "    h0 = encoder_output[:, 0, :] # Shape (B, D)\n",
        "\n",
        "    # 8. Apply the linear classification head.\n",
        "    logits = h0 @ Wcls + bcls # Wcls: (D,2), bcls: (2,)\n",
        "\n",
        "    # 9. Apply Softmax to probabilities\n",
        "    probs = softmax(logits, axis=-1)\n",
        "\n",
        "    # 10. Return the probabilities.\n",
        "    return probs\n",
        "\n"
      ],
      "metadata": {
        "id": "sVgU0WAYcxVW"
      },
      "execution_count": 20,
      "outputs": []
    }
  ]
}