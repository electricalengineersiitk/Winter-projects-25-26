{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Assignment #6\n",
        "\n",
        "Vision Transformer\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "mIR6a8dFe2e4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part A - Data and tokens"
      ],
      "metadata": {
        "id": "rsPfvp6Te4KB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_n52Y22fe7X4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Build a tiny toy dataset with pandas\n",
        "Create a pandas DataFrame with columns text and label.\n",
        "- Include at least 12 short sentences (3-10 words each).\n",
        "- The label is 0/1 (e.g., positive vs negative sentiment).\n",
        "- Shuffle rows and split into train/test (80/20) using a fixed random seed.\n",
        "Return: df_train, df_test."
      ],
      "metadata": {
        "id": "mSacHQ0CblC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def make_toy_dataset(seed: int = 42):\n",
        "    \"\"\"Return df_train, df_test with columns: text (str), label (int).\"\"\"\n",
        "\n",
        "    # Step 1: Create toy dataset (12 sentences)\n",
        "    data = {\n",
        "        \"text\": [\n",
        "            \"I love this movie\",\n",
        "            \"This food tastes amazing\",\n",
        "            \"What a wonderful day\",\n",
        "            \"I enjoy learning new things\",\n",
        "            \"The service was excellent\",\n",
        "            \"This product is very good\",\n",
        "            \"I hate this weather\",\n",
        "            \"This is a terrible mistake\",\n",
        "            \"The experience was bad\",\n",
        "            \"I feel very sad today\",\n",
        "            \"This movie is boring\",\n",
        "            \"The food was disgusting\"\n",
        "        ],\n",
        "        \"label\": [\n",
        "            1, 1, 1, 1, 1, 1,   # Positive = 1\n",
        "            0, 0, 0, 0, 0, 0    # Negative = 0\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # Step 2: Create DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Step 3: Shuffle dataset\n",
        "    df = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
        "\n",
        "    # Step 4: Train/Test split (80/20)\n",
        "    split_idx = int(0.8 * len(df))\n",
        "    df_train = df.iloc[:split_idx].reset_index(drop=True)\n",
        "    df_test = df.iloc[split_idx:].reset_index(drop=True)\n",
        "\n",
        "    return df_train, df_test\n"
      ],
      "metadata": {
        "id": "pcUh__z8e-28"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. Clean and tokenize text\n",
        "\n",
        "Implement a basic cleaner: lowercase, strip, replace multiple spaces with one, and remove punctuation\n",
        "(.,!?;:).\n",
        "Tokenize by whitespace.\n",
        "Add a new column tokens that stores a list of tokens per row.\n",
        "Return the updated DataFrame."
      ],
      "metadata": {
        "id": "GkMRbbJDbsc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "def clean_text(s: str) -> str:\n",
        "    \"\"\"Basic cleaner: lowercase, remove punctuation, normalize spaces.\"\"\"\n",
        "\n",
        "    # Lowercase\n",
        "    s = s.lower()\n",
        "\n",
        "    # Remove punctuation (.,!?;:)\n",
        "    s = re.sub(r\"[.,!?;:]\", \"\", s)\n",
        "\n",
        "    # Strip leading/trailing spaces\n",
        "    s = s.strip()\n",
        "\n",
        "    # Replace multiple spaces with single space\n",
        "    s = re.sub(r\"\\s+\", \" \", s)\n",
        "\n",
        "    return s\n",
        "\n",
        "\n",
        "def add_tokens_column(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Adds df['tokens'] = list[str].\"\"\"\n",
        "\n",
        "    df = df.copy()\n",
        "\n",
        "    # Clean + tokenize\n",
        "    df[\"tokens\"] = df[\"text\"].apply(lambda x: clean_text(x).split())\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "c0QYNAObe-Z7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. Build a vocabulary + token/id mappings\n",
        "\n",
        "Build token2id and id2token using the training tokens.\n",
        "Include special tokens: [PAD], [UNK], [BOS], [EOS] at the beginning.\n",
        "Add tokens that occur at least min_freq times.\n",
        "Return: token2id (dict), id2token (list)."
      ],
      "metadata": {
        "id": "mJpgICVAbu2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from typing import Dict, List\n",
        "\n",
        "SPECIALS = ['[PAD]', '[UNK]', '[BOS]', '[EOS]']\n",
        "\n",
        "def build_vocab(list_of_token_lists, min_freq: int = 1):\n",
        "    \"\"\"Return token2id (dict) and id2token (list).\"\"\"\n",
        "\n",
        "    # Count token frequencies\n",
        "    counter = Counter()\n",
        "    for tokens in list_of_token_lists:\n",
        "        counter.update(tokens)\n",
        "\n",
        "    # Start vocab with special tokens\n",
        "    id2token = SPECIALS.copy()\n",
        "\n",
        "    # Add tokens meeting min_freq condition\n",
        "    for token, freq in counter.items():\n",
        "        if freq >= min_freq:\n",
        "            id2token.append(token)\n",
        "\n",
        "    # Create token2id mapping\n",
        "    token2id = {token: idx for idx, token in enumerate(id2token)}\n",
        "\n",
        "    return token2id, id2token\n"
      ],
      "metadata": {
        "id": "KPsdkStdfQpZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. Convert tokens to ids + pad to a batch\n",
        "\n",
        "Implement tokens_to_ids for one sequence.\n",
        "Implement pad_batch that takes a list of id sequences and returns:\n",
        "- X: int array (B,T) padded with pad_id\n",
        "- pad_mask: bool array (B,T) where True means 'real token' and False means padding"
      ],
      "metadata": {
        "id": "tZpBXyaKb69F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def tokens_to_ids(tokens, token2id, unk_token='[UNK]'):\n",
        "    \"\"\"Convert one token list into list of ids.\"\"\"\n",
        "\n",
        "    unk_id = token2id[unk_token]\n",
        "\n",
        "    ids = [token2id.get(tok, unk_id) for tok in tokens]\n",
        "\n",
        "    return ids\n",
        "\n",
        "\n",
        "def pad_batch(id_seqs, pad_id: int):\n",
        "    \"\"\"\n",
        "    Return:\n",
        "    - X: padded int array (B,T)\n",
        "    - pad_mask: bool array (B,T)\n",
        "    \"\"\"\n",
        "\n",
        "    batch_size = len(id_seqs)\n",
        "    max_len = max(len(seq) for seq in id_seqs)\n",
        "\n",
        "    # Initialize padded array\n",
        "    X = np.full((batch_size, max_len), pad_id, dtype=int)\n",
        "\n",
        "    # Mask: True for real tokens\n",
        "    pad_mask = np.zeros((batch_size, max_len), dtype=bool)\n",
        "\n",
        "    for i, seq in enumerate(id_seqs):\n",
        "        length = len(seq)\n",
        "        X[i, :length] = seq\n",
        "        pad_mask[i, :length] = True\n",
        "\n",
        "    return X, pad_mask\n"
      ],
      "metadata": {
        "id": "YIIgn3MVfQk-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part B - Core Transformer math"
      ],
      "metadata": {
        "id": "5CSuz9Nab_O4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. Embedding lookup\n",
        "\n",
        "Implement an embedding table E of shape (V,D) initialized from a normal distribution (mean 0, std 0.02).\n",
        "Given token ids X (B,T), return embeddings of shape (B,T,D) using NumPy indexing.\n"
      ],
      "metadata": {
        "id": "VMCKexcUcCIG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def init_embeddings(vocab_size: int, d_model: int, seed: int = 0):\n",
        "    \"\"\"Initialize embedding table E: (V,D) from N(0, 0.02).\"\"\"\n",
        "\n",
        "    rng = np.random.RandomState(seed)\n",
        "    E = rng.normal(loc=0.0, scale=0.02, size=(vocab_size, d_model))\n",
        "\n",
        "    return E\n",
        "\n",
        "\n",
        "def embed(X: np.ndarray, E: np.ndarray):\n",
        "    \"\"\"\n",
        "    X: (B,T) token ids\n",
        "    E: (V,D) embedding matrix\n",
        "    Return: out (B,T,D)\n",
        "    \"\"\"\n",
        "\n",
        "    out = E[X]   # NumPy indexing\n",
        "\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "uIDHLUUGglA4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. Sinusoidal positional encoding\n",
        "\n",
        "Implement the classic sinusoidal positional encoding PE of shape (T,D).\n",
        "Then add it to token embeddings (B,T,D).\n",
        "Make sure your implementation works for both even and odd D."
      ],
      "metadata": {
        "id": "roaKv_LLcH9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def sinusoidal_positional_encoding(T: int, D: int):\n",
        "    \"\"\"Return sinusoidal PE: (T,D).\"\"\"\n",
        "\n",
        "    PE = np.zeros((T, D))\n",
        "\n",
        "    for pos in range(T):\n",
        "        for i in range(0, D, 2):\n",
        "            div_term = np.exp(-np.log(10000.0) * i / D)\n",
        "\n",
        "            PE[pos, i] = np.sin(pos * div_term)\n",
        "\n",
        "            # Only apply cosine if dimension exists (odd D safe)\n",
        "            if i + 1 < D:\n",
        "                PE[pos, i + 1] = np.cos(pos * div_term)\n",
        "\n",
        "    return PE\n",
        "\n",
        "\n",
        "def add_positional_encoding(X_emb: np.ndarray, PE: np.ndarray):\n",
        "    \"\"\"\n",
        "    X_emb: (B,T,D)\n",
        "    PE: (T,D)\n",
        "    Return: (B,T,D)\n",
        "    \"\"\"\n",
        "\n",
        "    X_emb_pe = X_emb + PE[None, :, :]   # broadcast across batch\n",
        "\n",
        "    return X_emb_pe\n"
      ],
      "metadata": {
        "id": "RFPbjbn4glwa"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. Scaled dot-product attention with masking\n",
        "\n",
        "Implement scaled dot-product attention:\n",
        "Attention(Q,K,V) = softmax((Q @ K^T) / sqrt(dk) + mask) @ V\n",
        "Inputs: Q,K,V are (B,H,T,Dh). Mask is boolean broadcastable to (B,H,T,T) where False means 'mask out'.\n",
        "Requirements:\n",
        "- Use a numerically stable softmax (subtract max).\n",
        "- Convert boolean mask to large negative values before softmax.\n",
        "Return: context (B,H,T,Dh) and attention weights (B,H,T,T)."
      ],
      "metadata": {
        "id": "k85DsUTgcQbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def softmax(x: np.ndarray, axis: int = -1):\n",
        "    \"\"\"Stable softmax.\"\"\"\n",
        "\n",
        "    x_max = np.max(x, axis=axis, keepdims=True)\n",
        "    exp_x = np.exp(x - x_max)\n",
        "    y = exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
        "\n",
        "    return y\n",
        "\n",
        "\n",
        "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
        "    \"\"\"\n",
        "    Q,K,V: (B,H,T,Dh)\n",
        "    mask: bool broadcastable to (B,H,T,T)\n",
        "    Return:\n",
        "      context: (B,H,T,Dh)\n",
        "      attn: (B,H,T,T)\n",
        "    \"\"\"\n",
        "\n",
        "    dk = Q.shape[-1]\n",
        "\n",
        "    # Compute attention scores: (B,H,T,T)\n",
        "    scores = (Q @ K.transpose(0, 1, 3, 2)) / np.sqrt(dk)\n",
        "\n",
        "    # Apply mask (False means masked out)\n",
        "    if mask is not None:\n",
        "        scores = np.where(mask, scores, -1e9)\n",
        "\n",
        "    # Softmax over last dimension (keys)\n",
        "    attn = softmax(scores, axis=-1)\n",
        "\n",
        "    # Weighted sum: (B,H,T,Dh)\n",
        "    context = attn @ V\n",
        "\n",
        "    return context, attn\n"
      ],
      "metadata": {
        "id": "s1yxLVfxgmmB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. Multi-head self-attention (MHA)\n",
        "\n",
        "Implement multi-head self-attention for input X (B,T,D).\n",
        "- Project to Q,K,V using weight matrices Wq,Wk,Wv each (D,D).\n",
        "- Reshape/split into heads -> (B,H,T,Dh) where Dh=D/H.\n",
        "- Apply scaled dot-product attention with a pad mask (B,T) (broadcast it appropriately).\n",
        "- Concatenate heads and apply output projection Wo (D,D).\n",
        "Return: out (B,T,D) and attention weights (B,H,T,T)."
      ],
      "metadata": {
        "id": "gM5ZKq_jcTEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def linear(x: np.ndarray, W: np.ndarray, b=None):\n",
        "    \"\"\"Linear layer: x @ W + b\"\"\"\n",
        "\n",
        "    y = x @ W\n",
        "    if b is not None:\n",
        "        y = y + b\n",
        "    return y\n",
        "\n",
        "\n",
        "def split_heads(x: np.ndarray, n_heads: int):\n",
        "    \"\"\"\n",
        "    (B,T,D) -> (B,H,T,Dh)\n",
        "    \"\"\"\n",
        "\n",
        "    B, T, D = x.shape\n",
        "    Dh = D // n_heads\n",
        "\n",
        "    x = x.reshape(B, T, n_heads, Dh)\n",
        "    xh = x.transpose(0, 2, 1, 3)\n",
        "\n",
        "    return xh\n",
        "\n",
        "\n",
        "def combine_heads(xh: np.ndarray):\n",
        "    \"\"\"\n",
        "    (B,H,T,Dh) -> (B,T,D)\n",
        "    \"\"\"\n",
        "\n",
        "    B, H, T, Dh = xh.shape\n",
        "\n",
        "    xh = xh.transpose(0, 2, 1, 3)\n",
        "    x = xh.reshape(B, T, H * Dh)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def mha_self_attention(X, Wq, Wk, Wv, Wo, n_heads: int, pad_mask=None):\n",
        "    \"\"\"\n",
        "    X: (B,T,D)\n",
        "    pad_mask: (B,T) True=real token, False=padding\n",
        "    Return:\n",
        "      out: (B,T,D)\n",
        "      attn: (B,H,T,T)\n",
        "    \"\"\"\n",
        "\n",
        "    # Project inputs\n",
        "    Q = linear(X, Wq)\n",
        "    K = linear(X, Wk)\n",
        "    V = linear(X, Wv)\n",
        "\n",
        "    # Split into heads\n",
        "    Qh = split_heads(Q, n_heads)\n",
        "    Kh = split_heads(K, n_heads)\n",
        "    Vh = split_heads(V, n_heads)\n",
        "\n",
        "    # Prepare attention mask\n",
        "    mask = None\n",
        "    if pad_mask is not None:\n",
        "        # (B,T) -> (B,1,1,T)\n",
        "        mask = pad_mask[:, None, None, :]\n",
        "\n",
        "    # Attention\n",
        "    context, attn = scaled_dot_product_attention(Qh, Kh, Vh, mask)\n",
        "\n",
        "    # Combine heads back\n",
        "    context_combined = combine_heads(context)\n",
        "\n",
        "    # Final projection\n",
        "    out = linear(context_combined, Wo)\n",
        "\n",
        "    return out, attn\n"
      ],
      "metadata": {
        "id": "TYTTv0z0gnHT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9. LayerNorm + residual connection\n",
        "\n",
        "Implement LayerNorm for X (B,T,D) using learnable gamma and beta of shape (D,).\n",
        "Then implement residual_add_and_norm(Y, X, gamma, beta) that returns LayerNorm(X + Y)."
      ],
      "metadata": {
        "id": "e3w9akBdcX__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def layer_norm(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, eps: float = 1e-5):\n",
        "    \"\"\"\n",
        "    X: (B,T,D)\n",
        "    gamma, beta: (D,)\n",
        "    Return: normalized output\n",
        "    \"\"\"\n",
        "\n",
        "    mean = np.mean(X, axis=-1, keepdims=True)\n",
        "    var = np.var(X, axis=-1, keepdims=True)\n",
        "\n",
        "    X_norm = (X - mean) / np.sqrt(var + eps)\n",
        "\n",
        "    Y = gamma * X_norm + beta\n",
        "\n",
        "    return Y\n",
        "\n",
        "\n",
        "def residual_add_and_norm(Y: np.ndarray, X: np.ndarray, gamma: np.ndarray, beta: np.ndarray):\n",
        "    \"\"\"Return LayerNorm(X + Y).\"\"\"\n",
        "\n",
        "    Z = layer_norm(X + Y, gamma, beta)\n",
        "\n",
        "    return Z\n"
      ],
      "metadata": {
        "id": "_CyLURIZgnnn"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10. Position-wise FeedForward network\n",
        "\n",
        "Implement FFN: FFN(X) = relu(X @ W1 + b1) @ W2 + b2\n",
        "Shapes: X (B,T,D), W1 (D,Dff), b1 (Dff,), W2 (Dff,D), b2 (D,)\n",
        "Return: (B,T,D)."
      ],
      "metadata": {
        "id": "vLI0APBzceTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def relu(x: np.ndarray):\n",
        "    \"\"\"ReLU activation.\"\"\"\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "\n",
        "def feed_forward(X: np.ndarray, W1: np.ndarray, b1: np.ndarray,\n",
        "                 W2: np.ndarray, b2: np.ndarray):\n",
        "    \"\"\"\n",
        "    X: (B,T,D)\n",
        "    Return: (B,T,D)\n",
        "    \"\"\"\n",
        "\n",
        "    hidden = relu(X @ W1 + b1)\n",
        "    Y = hidden @ W2 + b2\n",
        "\n",
        "    return Y\n"
      ],
      "metadata": {
        "id": "NCiHYO3MgoND"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part C - Putting it together"
      ],
      "metadata": {
        "id": "8JUgk3YyclkR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q11. One Transformer encoder block (forward)\n",
        "\n",
        "Implement a single encoder block forward pass:\n",
        "1) MHA = MultiHeadSelfAttention(X) with pad_mask\n",
        "2) X1 = LayerNorm(X + MHA)\n",
        "3) FFN = FeedForward(X1)\n",
        "4) X2 = LayerNorm(X1 + FFN)\n",
        "Return X2.\n",
        "You may pass all parameters explicitly (weights, gamma/beta)."
      ],
      "metadata": {
        "id": "g67wT-WmcoEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encoder_block_forward(X, params, n_heads: int, pad_mask=None):\n",
        "    \"\"\"\n",
        "    X: (B,T,D)\n",
        "    params: dictionary containing all weights + gamma/beta\n",
        "    pad_mask: (B,T) boolean mask\n",
        "    Return: X2 (B,T,D)\n",
        "    \"\"\"\n",
        "\n",
        "    # ---- Multi-Head Self Attention ----\n",
        "    mha_out, attn = mha_self_attention(\n",
        "        X,\n",
        "        params[\"Wq\"],\n",
        "        params[\"Wk\"],\n",
        "        params[\"Wv\"],\n",
        "        params[\"Wo\"],\n",
        "        n_heads=n_heads,\n",
        "        pad_mask=pad_mask\n",
        "    )\n",
        "\n",
        "    # ---- Residual + LayerNorm 1 ----\n",
        "    X1 = residual_add_and_norm(\n",
        "        mha_out,\n",
        "        X,\n",
        "        params[\"gamma1\"],\n",
        "        params[\"beta1\"]\n",
        "    )\n",
        "\n",
        "    # ---- Feed Forward Network ----\n",
        "    ffn_out = feed_forward(\n",
        "        X1,\n",
        "        params[\"W1\"],\n",
        "        params[\"b1\"],\n",
        "        params[\"W2\"],\n",
        "        params[\"b2\"]\n",
        "    )\n",
        "\n",
        "    # ---- Residual + LayerNorm 2 ----\n",
        "    X2 = residual_add_and_norm(\n",
        "        ffn_out,\n",
        "        X1,\n",
        "        params[\"gamma2\"],\n",
        "        params[\"beta2\"]\n",
        "    )\n",
        "\n",
        "    return X2\n"
      ],
      "metadata": {
        "id": "CbdfrwKkg4Cw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q12. Sequence classification head + end-to-end demo\n",
        "\n",
        "Create an end-to-end forward pass for a tiny classifier:\n",
        "- Input ids -> embeddings + positional enc\n",
        "- One encoder block\n",
        "- Pooling: take the [BOS] position (t=0) as the sequence representation\n",
        "- Linear head: logits = h0 @ Wcls + bcls with Wcls (D,2), bcls (2,)\n",
        "- Softmax to probabilities\n",
        "Write predict_proba that takes a batch of texts and returns probs (B,2).\n",
        "Include simple sanity checks: shapes, probabilities sum to 1, and masking doesn't crash for different\n",
        "lengths."
      ],
      "metadata": {
        "id": "CVp7BZHKf-1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_proba(texts, token2id, E, PE, params, Wcls, bcls, n_heads: int):\n",
        "    \"\"\"\n",
        "    texts: list[str]\n",
        "    Return probs: (B,2)\n",
        "    \"\"\"\n",
        "\n",
        "    # ---------- Step 1: Tokenize texts ----------\n",
        "    tokenized = []\n",
        "    for text in texts:\n",
        "        clean = clean_text(text)\n",
        "        tokens = clean.split()\n",
        "\n",
        "        # Add BOS and EOS\n",
        "        tokens = [\"[BOS]\"] + tokens + [\"[EOS]\"]\n",
        "        tokenized.append(tokens)\n",
        "\n",
        "    # ---------- Step 2: Convert tokens â†’ ids ----------\n",
        "    id_seqs = [tokens_to_ids(seq, token2id) for seq in tokenized]\n",
        "\n",
        "    # ---------- Step 3: Pad batch ----------\n",
        "    pad_id = token2id[\"[PAD]\"]\n",
        "    X_ids, pad_mask = pad_batch(id_seqs, pad_id)\n",
        "\n",
        "    # Sanity check\n",
        "    B, T = X_ids.shape\n",
        "    assert pad_mask.shape == (B, T)\n",
        "\n",
        "    # ---------- Step 4: Embedding Lookup ----------\n",
        "    X_emb = embed(X_ids, E)  # (B,T,D)\n",
        "\n",
        "    # ---------- Step 5: Add Positional Encoding ----------\n",
        "    X_emb_pe = add_positional_encoding(X_emb, PE[:T])\n",
        "\n",
        "    # ---------- Step 6: Encoder Block ----------\n",
        "    X_enc = encoder_block_forward(\n",
        "        X_emb_pe,\n",
        "        params,\n",
        "        n_heads=n_heads,\n",
        "        pad_mask=pad_mask\n",
        "    )\n",
        "\n",
        "    # ---------- Step 7: Pooling (BOS token at t=0) ----------\n",
        "    h0 = X_enc[:, 0, :]   # (B,D)\n",
        "\n",
        "    # ---------- Step 8: Classification Head ----------\n",
        "    logits = h0 @ Wcls + bcls   # (B,2)\n",
        "\n",
        "    # ---------- Step 9: Softmax Probabilities ----------\n",
        "    probs = softmax(logits, axis=-1)\n",
        "\n",
        "    # ---------- Sanity Checks ----------\n",
        "    assert probs.shape == (B, 2)\n",
        "    assert np.allclose(np.sum(probs, axis=1), 1.0)\n",
        "\n",
        "    return probs\n"
      ],
      "metadata": {
        "id": "dov-sp-Rg4oF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FcIkMmrafQjl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}