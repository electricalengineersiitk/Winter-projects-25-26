{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Assignment #6\n",
        "\n",
        "Vision Transformer\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "5CDBR-mth6ey"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SUBMISSION INSTRUCTIONS**\n",
        "\n",
        "First make a copy of this colab file and then solve the assignment and upload your final notebook on github.\n",
        "\n",
        "Before uploading your downloaded notebook, RENAME the file as rollno_name.ipynb\n",
        "\n",
        "Submission Deadline : 31/01/2026 Saturday EOD i.e before 11:59 PM\n",
        "\n",
        "The deadline is strict, Late submissions will incur penalty\n",
        "\n",
        "Note that you have to upload your solution on the github page of the project Vision Transformer and **under Assignment 6**\n",
        "\n",
        "And remember to keep title of your pull request to be ViT_name_rollno_assgn6\n",
        "\n",
        "Github Submission repo -\n",
        "https://github.com/electricalengineersiitk/Winter-projects-25-26/tree/main/Vision%20transformer/Assignment%206"
      ],
      "metadata": {
        "id": "z8HhWU2zjYu8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part A - Data and tokens"
      ],
      "metadata": {
        "id": "K7PUweRobXFq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Build a tiny toy dataset with pandas\n",
        "Create a pandas DataFrame with columns text and label.\n",
        "- Include at least 12 short sentences (3-10 words each).\n",
        "- The label is 0/1 (e.g., positive vs negative sentiment).\n",
        "- Shuffle rows and split into train/test (80/20) using a fixed random seed.\n",
        "Return: df_train, df_test."
      ],
      "metadata": {
        "id": "mSacHQ0CblC-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8LzSCSbMarwT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def make_toy_dataset(seed: int = 42):\n",
        "    \"\"\"Return df_train, df_test with columns: text (str), label (int).\"\"\"\n",
        "\n",
        "    data = {\n",
        "        'text': [\n",
        "            \"I absolutely love this new coffee machine.\",\n",
        "            \"The movie was a complete waste of time.\",\n",
        "            \"This software is incredibly intuitive and fast.\",\n",
        "            \"I am very disappointed with the service.\",\n",
        "            \"The weather today is absolutely perfect.\",\n",
        "            \"The book's plot was boring and predictable.\",\n",
        "            \"Everything about this meal was delicious.\",\n",
        "            \"I will never buy from this brand again.\",\n",
        "            \"The customer support team was very helpful.\",\n",
        "            \"The battery life is much shorter than advertised.\",\n",
        "            \"The performance of this laptop is outstanding.\",\n",
        "            \"I hate how complicated this remote control is.\"\n",
        "        ],\n",
        "        'label': [1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
        "    }\n",
        "\n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Shuffle the entire dataset\n",
        "    df_shuffled = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
        "\n",
        "    # Calculate split index (80%)\n",
        "    split_idx = int(len(df_shuffled) * 0.8)\n",
        "\n",
        "    # Split into train and test\n",
        "    df_train = df_shuffled.iloc[:split_idx]\n",
        "    df_test = df_shuffled.iloc[split_idx:]\n",
        "\n",
        "    return df_train, df_test\n",
        "\n",
        "# Example usage:\n",
        "# df_train, df_test = make_toy_dataset(42)\n",
        "# print(f\"Train size: {len(df_train)}, Test size: {len(df_test)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. Clean and tokenize text\n",
        "\n",
        "Implement a basic cleaner: lowercase, strip, replace multiple spaces with one, and remove punctuation\n",
        "(.,!?;:).\n",
        "Tokenize by whitespace.\n",
        "Add a new column tokens that stores a list of tokens per row.\n",
        "Return the updated DataFrame."
      ],
      "metadata": {
        "id": "GkMRbbJDbsc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "def clean_text(s: str) -> str:\n",
        "    \"\"\"Lowercase text and remove non-alphanumeric characters.\"\"\"\n",
        "    # Convert to lowercase\n",
        "    s = s.lower()\n",
        "    # Remove special characters and punctuation (keep spaces)\n",
        "    s = re.sub(r'[^a-z0-9\\s]', '', s)\n",
        "    # Remove extra whitespace\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()\n",
        "    return s\n",
        "\n",
        "def add_tokens_column(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Adds df['tokens'] = list[str] by cleaning and splitting text.\"\"\"\n",
        "    # Apply cleaning first, then split on whitespace to create a list\n",
        "    df['tokens'] = df['text'].apply(lambda x: clean_text(x).split())\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "4siOVxuCbnqs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. Build a vocabulary + token/id mappings\n",
        "\n",
        "Build token2id and id2token using the training tokens.\n",
        "Include special tokens: [PAD], [UNK], [BOS], [EOS] at the beginning.\n",
        "Add tokens that occur at least min_freq times.\n",
        "Return: token2id (dict), id2token (list)."
      ],
      "metadata": {
        "id": "mJpgICVAbu2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "SPECIALS = ['[PAD]', '[UNK]', '[BOS]', '[EOS]']\n",
        "\n",
        "def build_vocab(list_of_token_lists: List[List[str]], min_freq: int = 1) -> Tuple[Dict[str, int], Dict[int, str]]:\n",
        "    \"\"\"Builds mapping dictionaries for tokens to IDs and vice versa.\"\"\"\n",
        "\n",
        "    # 1. Count all tokens across all documents\n",
        "    counter = Counter()\n",
        "    for tokens in list_of_token_lists:\n",
        "        counter.update(tokens)\n",
        "\n",
        "    # 2. Initialize mappings with special tokens\n",
        "    token2id = {token: i for i, token in enumerate(SPECIALS)}\n",
        "\n",
        "    # 3. Add tokens that meet the minimum frequency threshold\n",
        "    for token, count in counter.items():\n",
        "        if count >= min_freq and token not in token2id:\n",
        "            token2id[token] = len(token2id)\n",
        "\n",
        "    # 4. Create the reverse mapping\n",
        "    id2token = {idx: token for token, idx in token2id.items()}\n",
        "\n",
        "    return token2id, id2token\n"
      ],
      "metadata": {
        "id": "Oh8qMuzOa0-M"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. Convert tokens to ids + pad to a batch\n",
        "\n",
        "Implement tokens_to_ids for one sequence.\n",
        "Implement pad_batch that takes a list of id sequences and returns:\n",
        "- X: int array (B,T) padded with pad_id\n",
        "- pad_mask: bool array (B,T) where True means 'real token' and False means padding"
      ],
      "metadata": {
        "id": "tZpBXyaKb69F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokens_to_ids(tokens, token2id, unk_token='[UNK]'):\n",
        "    \"\"\"Convert a list of tokens to a list of integer IDs.\"\"\"\n",
        "    unk_id = token2id.get(unk_token)\n",
        "    # Use .get() to default to the [UNK] ID if a word isn't in our vocab\n",
        "    return [token2id.get(token, unk_id) for token in tokens]\n",
        "\n",
        "def pad_batch(id_seqs, pad_id: int):\n",
        "    batch_size = len(id_seqs)\n",
        "    max_len = max(len(seq) for seq in id_seqs)\n",
        "\n",
        "    # Initialize X with the pad_id\n",
        "    X = np.full((batch_size, max_len), pad_id, dtype=np.int64)\n",
        "    # Initialize mask with False\n",
        "    pad_mask = np.zeros((batch_size, max_len), dtype=bool)\n",
        "\n",
        "    for i, seq in enumerate(id_seqs):\n",
        "        length = len(seq)\n",
        "        X[i, :length] = seq\n",
        "        pad_mask[i, :length] = True\n",
        "\n",
        "    return X, pad_mask\n"
      ],
      "metadata": {
        "id": "IAHXRbwBa5gx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part B - Core Transformer math"
      ],
      "metadata": {
        "id": "5CSuz9Nab_O4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. Embedding lookup\n",
        "\n",
        "Implement an embedding table E of shape (V,D) initialized from a normal distribution (mean 0, std 0.02).\n",
        "Given token ids X (B,T), return embeddings of shape (B,T,D) using NumPy indexing.\n"
      ],
      "metadata": {
        "id": "VMCKexcUcCIG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def init_embeddings(vocab_size: int, d_model: int, seed: int = 0):\n",
        "    \"\"\"\n",
        "    Initializes the embedding matrix E.\n",
        "    V = vocab_size, D = d_model.\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "    # Common practice: initialize with mean 0 and small variance\n",
        "    # or use Xavier/He initialization logic.\n",
        "    E = np.random.randn(vocab_size, d_model) * 0.01\n",
        "    return E\n",
        "\n",
        "def embed(X: np.ndarray, E: np.ndarray):\n",
        "    \"\"\"\n",
        "    X: (B, T) - Batch of token IDs\n",
        "    E: (V, D) - Embedding weight matrix\n",
        "    out: (B, T, D) - Embedded representations\n",
        "    \"\"\"\n",
        "    # NumPy fancy indexing: for every index in X,\n",
        "    # grab the corresponding row from E.\n",
        "    out = E[X]\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "MjcboKYZcGFy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. Sinusoidal positional encoding\n",
        "\n",
        "Implement the classic sinusoidal positional encoding PE of shape (T,D).\n",
        "Then add it to token embeddings (B,T,D).\n",
        "Make sure your implementation works for both even and odd D."
      ],
      "metadata": {
        "id": "roaKv_LLcH9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def sinusoidal_positional_encoding(T: int, D: int):\n",
        "    \"\"\"\n",
        "    T: Sequence length (number of positions)\n",
        "    D: Model dimension (must be even for this implementation)\n",
        "    Returns PE: (T, D)\n",
        "    \"\"\"\n",
        "    # 1. Initialize the PE matrix\n",
        "    PE = np.zeros((T, D))\n",
        "\n",
        "    # 2. Create a column vector for positions (0 to T-1)\n",
        "    pos = np.arange(T)[:, np.newaxis]\n",
        "\n",
        "    # 3. Create a row vector for the denominator indices (0, 2, 4... D-2)\n",
        "    # We use exp and log for numerical stability/efficiency\n",
        "    div_term = np.exp(np.arange(0, D, 2) * -(np.log(10000.0) / D))\n",
        "\n",
        "    # 4. Apply sin to even indices and cos to odd indices\n",
        "    PE[:, 0::2] = np.sin(pos * div_term)\n",
        "    PE[:, 1::2] = np.cos(pos * div_term)\n",
        "\n",
        "    return PE\n",
        "\n",
        "def add_positional_encoding(X_emb: np.ndarray, PE: np.ndarray):\n",
        "    \"\"\"\n",
        "    X_emb: (B, T, D) - Embedded input\n",
        "    PE: (T, D) - Positional encodings\n",
        "    \"\"\"\n",
        "    # NumPy broadcasting handles the batch dimension (B) automatically\n",
        "    X_emb_pe = X_emb + PE\n",
        "    return X_emb_pe"
      ],
      "metadata": {
        "id": "owZSRIeocLtS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. Scaled dot-product attention with masking\n",
        "\n",
        "Implement scaled dot-product attention:\n",
        "Attention(Q,K,V) = softmax((Q @ K^T) / sqrt(dk) + mask) @ V\n",
        "Inputs: Q,K,V are (B,H,T,Dh). Mask is boolean broadcastable to (B,H,T,T) where False means 'mask out'.\n",
        "Requirements:\n",
        "- Use a numerically stable softmax (subtract max).\n",
        "- Convert boolean mask to large negative values before softmax.\n",
        "Return: context (B,H,T,Dh) and attention weights (B,H,T,T)."
      ],
      "metadata": {
        "id": "k85DsUTgcQbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def softmax(x: np.ndarray, axis: int = -1):\n",
        "    \"\"\"Computes stable softmax along the specified axis.\"\"\"\n",
        "    # Subtract max for numerical stability\n",
        "    e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
        "    return e_x / np.sum(e_x, axis=axis, keepdims=True)\n",
        "\n",
        "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
        "    \"\"\"\n",
        "    Q, K, V: (B, H, T, Dh)\n",
        "    mask: Optional boolean mask (B, H, T, T) where True means 'keep' and False means 'mask'\n",
        "    \"\"\"\n",
        "    d_k = Q.shape[-1]\n",
        "\n",
        "    # 1. Compute scores: (B, H, T, Dh) @ (B, H, Dh, T) -> (B, H, T, T)\n",
        "    # Using swapaxes to transpose the last two dimensions of K\n",
        "    scores = np.matmul(Q, K.swapaxes(-2, -1)) / np.sqrt(d_k)\n",
        "\n",
        "    # 2. Apply mask if provided\n",
        "    if mask is not None:\n",
        "        # Fill masked positions with a very large negative number\n",
        "        scores = np.where(mask, scores, -1e9)\n",
        "\n",
        "    # 3. Get attention weights\n",
        "    attn = softmax(scores, axis=-1)\n",
        "\n",
        "    # 4. Compute context: (B, H, T, T) @ (B, H, T, Dh) -> (B, H, T, Dh)\n",
        "    context = np.matmul(attn, V)\n",
        "\n",
        "    return context, attn"
      ],
      "metadata": {
        "id": "bMWieoyZcTSE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. Multi-head self-attention (MHA)\n",
        "\n",
        "Implement multi-head self-attention for input X (B,T,D).\n",
        "- Project to Q,K,V using weight matrices Wq,Wk,Wv each (D,D).\n",
        "- Reshape/split into heads -> (B,H,T,Dh) where Dh=D/H.\n",
        "- Apply scaled dot-product attention with a pad mask (B,T) (broadcast it appropriately).\n",
        "- Concatenate heads and apply output projection Wo (D,D).\n",
        "Return: out (B,T,D) and attention weights (B,H,T,T)."
      ],
      "metadata": {
        "id": "gM5ZKq_jcTEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def linear(x: np.ndarray, W: np.ndarray, b=None):\n",
        "    \"\"\"Standard linear transformation: y = xW + b.\"\"\"\n",
        "    y = np.matmul(x, W)\n",
        "    if b is not None:\n",
        "        y += b\n",
        "    return y\n",
        "\n",
        "def split_heads(x: np.ndarray, n_heads: int):\n",
        "    \"\"\"\n",
        "    Reshapes (B, T, D) into (B, H, T, Dh) where Dh = D / H.\n",
        "    \"\"\"\n",
        "    B, T, D = x.shape\n",
        "    Dh = D // n_heads\n",
        "    return x.reshape(B, T, n_heads, Dh).transpose(0, 2, 1, 3)\n",
        "\n",
        "def combine_heads(xh: np.ndarray):\n",
        "\n",
        "    B, H, T, Dh = xh.shape\n",
        "    D = H * Dh\n",
        "    return xh.transpose(0, 2, 1, 3).reshape(B, T, D)\n",
        "\n",
        "def mha_self_attention(X, Wq, Wk, Wv, Wo, n_heads: int, pad_mask=None):\n",
        "    Q = linear(X, Wq)\n",
        "    K = linear(X, Wk)\n",
        "    V = linear(X, Wv)\n",
        "\n",
        "    Qs = split_heads(Q, n_heads)\n",
        "    Ks = split_heads(K, n_heads)\n",
        "    Vs = split_heads(V, n_heads)\n",
        "\n",
        "    context, attn = scaled_dot_product_attention(Qs, Ks, Vs, mask=pad_mask)\n",
        "\n",
        "    combined = combine_heads(context)\n",
        "    out = linear(combined, Wo)\n",
        "\n",
        "    return out, attn"
      ],
      "metadata": {
        "id": "Vzr0fP7HcYeY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9. LayerNorm + residual connection\n",
        "\n",
        "Implement LayerNorm for X (B,T,D) using learnable gamma and beta of shape (D,).\n",
        "Then implement residual_add_and_norm(Y, X, gamma, beta) that returns LayerNorm(X + Y)."
      ],
      "metadata": {
        "id": "e3w9akBdcX__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def layer_norm(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, eps: float = 1e-5):\n",
        "\n",
        "    mean = np.mean(X, axis=-1, keepdims=True)\n",
        "    var = np.var(X, axis=-1, keepdims=True)\n",
        "\n",
        "    X_hat = (X - mean) / np.sqrt(var + eps)\n",
        "\n",
        "    Y = gamma * X_hat + beta\n",
        "    return Y\n",
        "\n",
        "def residual_add_and_norm(Y: np.ndarray, X: np.ndarray, gamma: np.ndarray, beta: np.ndarray):\n",
        "\n",
        "    Z = layer_norm(X + Y, gamma, beta)\n",
        "    return Z"
      ],
      "metadata": {
        "id": "JFfAjriHccnz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10. Position-wise FeedForward network\n",
        "\n",
        "Implement FFN: FFN(X) = relu(X @ W1 + b1) @ W2 + b2\n",
        "Shapes: X (B,T,D), W1 (D,Dff), b1 (Dff,), W2 (Dff,D), b2 (D,)\n",
        "Return: (B,T,D)."
      ],
      "metadata": {
        "id": "vLI0APBzceTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def relu(x: np.ndarray):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def feed_forward(X: np.ndarray, W1: np.ndarray, b1: np.ndarray, W2: np.ndarray, b2: np.ndarray):\n",
        "\n",
        "    h = np.matmul(X, W1) + b1\n",
        "\n",
        "    a = relu(h)\n",
        "\n",
        "    Y = np.matmul(a, W2) + b2\n",
        "\n",
        "    return Y"
      ],
      "metadata": {
        "id": "oYRKtUR1chXF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part C - Putting it together"
      ],
      "metadata": {
        "id": "8JUgk3YyclkR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q11. One Transformer encoder block (forward)\n",
        "\n",
        "Implement a single encoder block forward pass:\n",
        "1) MHA = MultiHeadSelfAttention(X) with pad_mask\n",
        "2) X1 = LayerNorm(X + MHA)\n",
        "3) FFN = FeedForward(X1)\n",
        "4) X2 = LayerNorm(X1 + FFN)\n",
        "Return X2.\n",
        "You may pass all parameters explicitly (weights, gamma/beta)."
      ],
      "metadata": {
        "id": "g67wT-WmcoEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def encoder_block_forward(X, params, n_heads: int, pad_mask=None):\n",
        "\n",
        "    attn_out, _ = mha_self_attention(\n",
        "        X,\n",
        "        params['Wq'], params['Wk'], params['Wv'], params['Wo'],\n",
        "        n_heads,\n",
        "        pad_mask\n",
        "    )\n",
        "\n",
        "\n",
        "    X1 = residual_add_and_norm(\n",
        "        attn_out, X,\n",
        "        params['gamma1'], params['beta1']\n",
        "    )\n",
        "\n",
        "    ffn_out = feed_forward(\n",
        "        X1,\n",
        "        params['W1'], params['b1'],\n",
        "        params['W2'], params['b2']\n",
        "    )\n",
        "\n",
        "    X2 = residual_add_and_norm(\n",
        "        ffn_out, X1,\n",
        "        params['gamma2'], params['beta2']\n",
        "    )\n",
        "\n",
        "    return X2"
      ],
      "metadata": {
        "id": "ppMLKzXIcsfv"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q12. Sequence classification head + end-to-end demo\n",
        "\n",
        "Create an end-to-end forward pass for a tiny classifier:\n",
        "- Input ids -> embeddings + positional enc\n",
        "- One encoder block\n",
        "- Pooling: take the [BOS] position (t=0) as the sequence representation\n",
        "- Linear head: logits = h0 @ Wcls + bcls with Wcls (D,2), bcls (2,)\n",
        "- Softmax to probabilities\n",
        "Write predict_proba that takes a batch of texts and returns probs (B,2).\n",
        "Include simple sanity checks: shapes, probabilities sum to 1, and masking doesn't crash for different\n",
        "lengths.\n"
      ],
      "metadata": {
        "id": "g-lT714IcunS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def predict_proba(texts, token2id, E, PE, params, Wcls, bcls, n_heads: int):\n",
        "\n",
        "    token_ids = [[token2id.get(t, 0) for t in text.split()] for text in texts]\n",
        "    T_max = max(len(t) for t in token_ids)\n",
        "    B = len(texts)\n",
        "\n",
        "    X_ids = np.zeros((B, T_max), dtype=int)\n",
        "    for i, tokens in enumerate(token_ids):\n",
        "        X_ids[i, :len(tokens)] = tokens\n",
        "    X_emb = embed(X_ids, E)\n",
        "    X_pe = add_positional_encoding(X_emb, PE[:T_max, :])\n",
        "\n",
        "    X_hidden = encoder_block_forward(X_pe, params, n_heads)\n",
        "\n",
        "    sentence_rep = X_hidden[:, 0, :]\n",
        "\n",
        "    logits = linear(sentence_rep, Wcls, bcls)\n",
        "\n",
        "    probs = softmax(logits, axis=-1)\n",
        "\n",
        "    return probs"
      ],
      "metadata": {
        "id": "sVgU0WAYcxVW"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}