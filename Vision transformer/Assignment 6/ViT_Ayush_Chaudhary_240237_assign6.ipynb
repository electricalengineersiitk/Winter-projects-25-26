{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Assignment #6\n",
        "\n",
        "Vision Transformer\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "5CDBR-mth6ey"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SUBMISSION INSTRUCTIONS**\n",
        "\n",
        "First make a copy of this colab file and then solve the assignment and upload your final notebook on github.\n",
        "\n",
        "Before uploading your downloaded notebook, RENAME the file as rollno_name.ipynb\n",
        "\n",
        "Submission Deadline : 31/01/2026 Saturday EOD i.e before 11:59 PM\n",
        "\n",
        "The deadline is strict, Late submissions will incur penalty\n",
        "\n",
        "Note that you have to upload your solution on the github page of the project Vision Transformer and **under Assignment 6**\n",
        "\n",
        "And remember to keep title of your pull request to be ViT_name_rollno_assgn6\n",
        "\n",
        "Github Submission repo -\n",
        "https://github.com/electricalengineersiitk/Winter-projects-25-26/tree/main/Vision%20transformer/Assignment%206"
      ],
      "metadata": {
        "id": "z8HhWU2zjYu8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part A - Data and tokens"
      ],
      "metadata": {
        "id": "K7PUweRobXFq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Build a tiny toy dataset with pandas\n",
        "Create a pandas DataFrame with columns text and label.\n",
        "- Include at least 12 short sentences (3-10 words each).\n",
        "- The label is 0/1 (e.g., positive vs negative sentiment).\n",
        "- Shuffle rows and split into train/test (80/20) using a fixed random seed.\n",
        "Return: df_train, df_test."
      ],
      "metadata": {
        "id": "mSacHQ0CblC-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8LzSCSbMarwT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def make_toy_dataset(seed: int = 42):\n",
        "    data = {\n",
        "        \"text\": [\n",
        "            \"i love this movie\", \"this is a bad film\", \"it was a great day\",\n",
        "            \"i hate this weather\", \"the food was delicious\", \"i feel very sad\",\n",
        "            \"the sun is shining\", \"everything is terrible\", \"what a wonderful experience\",\n",
        "            \"i am not happy\", \"best day of my life\", \"worst service ever\"\n",
        "        ],\n",
        "        \"label\": [1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
        "    }\n",
        "    df = pd.DataFrame(data)\n",
        "    df = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
        "    df_train, df_test = train_test_split(df, test_size=0.2, random_state=seed)\n",
        "    return df_train, df_test"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. Clean and tokenize text\n",
        "\n",
        "Implement a basic cleaner: lowercase, strip, replace multiple spaces with one, and remove punctuation\n",
        "(.,!?;:).\n",
        "Tokenize by whitespace.\n",
        "Add a new column tokens that stores a list of tokens per row.\n",
        "Return the updated DataFrame."
      ],
      "metadata": {
        "id": "GkMRbbJDbsc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_text(s: str) -> str:\n",
        "    s = s.lower().strip()\n",
        "    s = re.sub(r'[.,!?;:]', '', s)\n",
        "    s = re.sub(r'\\s+', ' ', s)\n",
        "    return s\n",
        "\n",
        "def add_tokens_column(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df['tokens'] = df['text'].apply(lambda x: clean_text(x).split())\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "4siOVxuCbnqs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. Build a vocabulary + token/id mappings\n",
        "\n",
        "Build token2id and id2token using the training tokens.\n",
        "Include special tokens: [PAD], [UNK], [BOS], [EOS] at the beginning.\n",
        "Add tokens that occur at least min_freq times.\n",
        "Return: token2id (dict), id2token (list)."
      ],
      "metadata": {
        "id": "mJpgICVAbu2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "SPECIALS = ['[PAD]', '[UNK]', '[BOS]', '[EOS]']\n",
        "\n",
        "def build_vocab(list_of_token_lists, min_freq: int = 1):\n",
        "    counter = Counter(tk for tokens in list_of_token_lists for tk in tokens)\n",
        "    id2token = SPECIALS + [tk for tk, count in counter.items() if count >= min_freq]\n",
        "    token2id = {tk: i for i, tk in enumerate(id2token)}\n",
        "    return token2id, id2token\n"
      ],
      "metadata": {
        "id": "Oh8qMuzOa0-M"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. Convert tokens to ids + pad to a batch\n",
        "\n",
        "Implement tokens_to_ids for one sequence.\n",
        "Implement pad_batch that takes a list of id sequences and returns:\n",
        "- X: int array (B,T) padded with pad_id\n",
        "- pad_mask: bool array (B,T) where True means 'real token' and False means padding"
      ],
      "metadata": {
        "id": "tZpBXyaKb69F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def tokens_to_ids(tokens, token2id, unk_token='[UNK]'):\n",
        "    unk_id = token2id.get(unk_token)\n",
        "    return [token2id.get(t, unk_id) for t in tokens]\n",
        "\n",
        "def pad_batch(id_seqs, pad_id: int):\n",
        "    max_len = max(len(s) for s in id_seqs)\n",
        "    B = len(id_seqs)\n",
        "    X = np.full((B, max_len), pad_id, dtype=int)\n",
        "    pad_mask = np.zeros((B, max_len), dtype=bool)\n",
        "\n",
        "    for i, seq in enumerate(id_seqs):\n",
        "        X[i, :len(seq)] = seq\n",
        "        pad_mask[i, :len(seq)] = True\n",
        "    return X, pad_mask\n"
      ],
      "metadata": {
        "id": "IAHXRbwBa5gx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part B - Core Transformer math"
      ],
      "metadata": {
        "id": "5CSuz9Nab_O4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. Embedding lookup\n",
        "\n",
        "Implement an embedding table E of shape (V,D) initialized from a normal distribution (mean 0, std 0.02).\n",
        "Given token ids X (B,T), return embeddings of shape (B,T,D) using NumPy indexing.\n"
      ],
      "metadata": {
        "id": "VMCKexcUcCIG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_embeddings(vocab_size: int, d_model: int, seed: int = 0):\n",
        "    np.random.seed(seed)\n",
        "    return np.random.normal(0, 0.02, (vocab_size, d_model))\n",
        "\n",
        "def embed(X: np.ndarray, E: np.ndarray):\n",
        "    return E[X]\n"
      ],
      "metadata": {
        "id": "MjcboKYZcGFy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. Sinusoidal positional encoding\n",
        "\n",
        "Implement the classic sinusoidal positional encoding PE of shape (T,D).\n",
        "Then add it to token embeddings (B,T,D).\n",
        "Make sure your implementation works for both even and odd D."
      ],
      "metadata": {
        "id": "roaKv_LLcH9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sinusoidal_positional_encoding(T: int, D: int):\n",
        "    PE = np.zeros((T, D))\n",
        "    position = np.arange(T)[:, np.newaxis]\n",
        "    div_term = np.exp(np.arange(0, D, 2) * -(np.log(10000.0) / D))\n",
        "    PE[:, 0::2] = np.sin(position * div_term)\n",
        "    PE[:, 1::2] = np.cos(position * div_term)\n",
        "    return PE\n",
        "\n",
        "def add_positional_encoding(X_emb: np.ndarray, PE: np.ndarray):\n",
        "    return X_emb + PE[:X_emb.shape[1], :]\n"
      ],
      "metadata": {
        "id": "owZSRIeocLtS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. Scaled dot-product attention with masking\n",
        "\n",
        "Implement scaled dot-product attention:\n",
        "Attention(Q,K,V) = softmax((Q @ K^T) / sqrt(dk) + mask) @ V\n",
        "Inputs: Q,K,V are (B,H,T,Dh). Mask is boolean broadcastable to (B,H,T,T) where False means 'mask out'.\n",
        "Requirements:\n",
        "- Use a numerically stable softmax (subtract max).\n",
        "- Convert boolean mask to large negative values before softmax.\n",
        "Return: context (B,H,T,Dh) and attention weights (B,H,T,T)."
      ],
      "metadata": {
        "id": "k85DsUTgcQbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x: np.ndarray, axis: int = -1):\n",
        "    exps = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
        "    return exps / np.sum(exps, axis=axis, keepdims=True)\n",
        "\n",
        "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
        "    dk = Q.shape[-1]\n",
        "    scores = np.matmul(Q, K.transpose(0, 1, 3, 2)) / np.sqrt(dk)\n",
        "    if mask is not None:\n",
        "        # mask is (B, 1, 1, T) or (B, 1, T, T)\n",
        "        scores = np.where(mask, scores, -1e9)\n",
        "    attn = softmax(scores, axis=-1)\n",
        "    context = np.matmul(attn, V)\n",
        "    return context, attn"
      ],
      "metadata": {
        "id": "bMWieoyZcTSE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. Multi-head self-attention (MHA)\n",
        "\n",
        "Implement multi-head self-attention for input X (B,T,D).\n",
        "- Project to Q,K,V using weight matrices Wq,Wk,Wv each (D,D).\n",
        "- Reshape/split into heads -> (B,H,T,Dh) where Dh=D/H.\n",
        "- Apply scaled dot-product attention with a pad mask (B,T) (broadcast it appropriately).\n",
        "- Concatenate heads and apply output projection Wo (D,D).\n",
        "Return: out (B,T,D) and attention weights (B,H,T,T)."
      ],
      "metadata": {
        "id": "gM5ZKq_jcTEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linear(x: np.ndarray, W: np.ndarray, b=None):\n",
        "    y = np.matmul(x, W)\n",
        "    return y + b if b is not None else y\n",
        "\n",
        "def split_heads(x: np.ndarray, n_heads: int):\n",
        "    B, T, D = x.shape\n",
        "    Dh = D // n_heads\n",
        "    return x.reshape(B, T, n_heads, Dh).transpose(0, 2, 1, 3)\n",
        "\n",
        "def combine_heads(xh: np.ndarray):\n",
        "    B, H, T, Dh = xh.shape\n",
        "    return xh.transpose(0, 2, 1, 3).reshape(B, T, H * Dh)\n",
        "\n",
        "def mha_self_attention(X, Wq, Wk, Wv, Wo, n_heads: int, pad_mask=None):\n",
        "    Q = split_heads(linear(X, Wq), n_heads)\n",
        "    K = split_heads(linear(X, Wk), n_heads)\n",
        "    V = split_heads(linear(X, Wv), n_heads)\n",
        "\n",
        "    m = None\n",
        "    if pad_mask is not None:\n",
        "        m = pad_mask[:, np.newaxis, np.newaxis, :]\n",
        "\n",
        "    context, attn = scaled_dot_product_attention(Q, K, V, mask=m)\n",
        "    out = linear(combine_heads(context), Wo)\n",
        "    return out, attn"
      ],
      "metadata": {
        "id": "Vzr0fP7HcYeY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9. LayerNorm + residual connection\n",
        "\n",
        "Implement LayerNorm for X (B,T,D) using learnable gamma and beta of shape (D,).\n",
        "Then implement residual_add_and_norm(Y, X, gamma, beta) that returns LayerNorm(X + Y)."
      ],
      "metadata": {
        "id": "e3w9akBdcX__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def layer_norm(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, eps: float = 1e-5):\n",
        "    mean = X.mean(axis=-1, keepdims=True)\n",
        "    std = X.std(axis=-1, keepdims=True)\n",
        "    return gamma * (X - mean) / (std + eps) + beta\n",
        "\n",
        "def residual_add_and_norm(Y: np.ndarray, X: np.ndarray, gamma: np.ndarray, beta: np.ndarray):\n",
        "    return layer_norm(X + Y, gamma, beta)"
      ],
      "metadata": {
        "id": "JFfAjriHccnz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10. Position-wise FeedForward network\n",
        "\n",
        "Implement FFN: FFN(X) = relu(X @ W1 + b1) @ W2 + b2\n",
        "Shapes: X (B,T,D), W1 (D,Dff), b1 (Dff,), W2 (Dff,D), b2 (D,)\n",
        "Return: (B,T,D)."
      ],
      "metadata": {
        "id": "vLI0APBzceTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(x: np.ndarray):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def feed_forward(X: np.ndarray, W1: np.ndarray, b1: np.ndarray, W2: np.ndarray, b2: np.ndarray):\n",
        "    h = relu(linear(X, W1, b1))\n",
        "    return linear(h, W2, b2)"
      ],
      "metadata": {
        "id": "oYRKtUR1chXF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part C - Putting it together"
      ],
      "metadata": {
        "id": "8JUgk3YyclkR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q11. One Transformer encoder block (forward)\n",
        "\n",
        "Implement a single encoder block forward pass:\n",
        "1) MHA = MultiHeadSelfAttention(X) with pad_mask\n",
        "2) X1 = LayerNorm(X + MHA)\n",
        "3) FFN = FeedForward(X1)\n",
        "4) X2 = LayerNorm(X1 + FFN)\n",
        "Return X2.\n",
        "You may pass all parameters explicitly (weights, gamma/beta)."
      ],
      "metadata": {
        "id": "g67wT-WmcoEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encoder_block_forward(X, params, n_heads: int, pad_mask=None):\n",
        "    # params expected to have keys like 'Wq', 'gamma1', etc.\n",
        "    attn_out, _ = mha_self_attention(X, params['Wq'], params['Wk'], params['Wv'],\n",
        "                                     params['Wo'], n_heads, pad_mask)\n",
        "    X1 = residual_add_and_norm(attn_out, X, params['gamma1'], params['beta1'])\n",
        "\n",
        "    ff_out = feed_forward(X1, params['W1'], params['b1'], params['W2'], params['b2'])\n",
        "    X2 = residual_add_and_norm(ff_out, X1, params['gamma2'], params['beta2'])\n",
        "    return X2"
      ],
      "metadata": {
        "id": "ppMLKzXIcsfv"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q12. Sequence classification head + end-to-end demo\n",
        "\n",
        "Create an end-to-end forward pass for a tiny classifier:\n",
        "- Input ids -> embeddings + positional enc\n",
        "- One encoder block\n",
        "- Pooling: take the [BOS] position (t=0) as the sequence representation\n",
        "- Linear head: logits = h0 @ Wcls + bcls with Wcls (D,2), bcls (2,)\n",
        "- Softmax to probabilities\n",
        "Write predict_proba that takes a batch of texts and returns probs (B,2).\n",
        "Include simple sanity checks: shapes, probabilities sum to 1, and masking doesn't crash for different\n",
        "lengths.\n"
      ],
      "metadata": {
        "id": "g-lT714IcunS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_proba(texts, token2id, E, PE, params, Wcls, bcls, n_heads: int):\n",
        "    # Pre-process\n",
        "    cleaned = [clean_text(t).split() for t in texts]\n",
        "    # Add [BOS] and [EOS]\n",
        "    token_lists = [['[BOS]'] + tokens + ['[EOS]'] for tokens in cleaned]\n",
        "    id_seqs = [tokens_to_ids(tokens, token2id) for tokens in token_lists]\n",
        "    X_batch, mask = pad_batch(id_seqs, token2id['[PAD]'])\n",
        "\n",
        "    # Forward Pass\n",
        "    X_emb = embed(X_batch, E)\n",
        "    X_pe = add_positional_encoding(X_emb, PE)\n",
        "    X_enc = encoder_block_forward(X_pe, params, n_heads, mask)\n",
        "\n",
        "    # Pooling: take [BOS] token at index 0\n",
        "    h0 = X_enc[:, 0, :]\n",
        "    logits = linear(h0, Wcls, bcls)\n",
        "    probs = softmax(logits, axis=-1)\n",
        "\n",
        "    return probs"
      ],
      "metadata": {
        "id": "sVgU0WAYcxVW"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}