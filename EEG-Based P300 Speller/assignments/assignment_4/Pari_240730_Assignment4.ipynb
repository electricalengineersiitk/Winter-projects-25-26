{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MCcS4xKuUdQ",
        "outputId": "2b397ca8-7adc-46c3-b025-9f772bca1811"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import scipy.io as sio\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data = sio.loadmat('/content/drive/My Drive/BCI_Comp_III_Wads_2004/Subject_A_Train.mat')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FT2Rme4vuqCp"
      },
      "outputs": [],
      "source": [
        "# TODO: Identify which variables correspond to\n",
        "# 1. Continuous EEG signal\n",
        "# 2. Stimulus onset information\n",
        "# 3. Target vs non-target labels\n",
        "# 1. Continuous EEG signal: Reshape from (85, 7794, 64) to (662490, 64)\n",
        "# The dataset stores Signal as (85, 7794, 64)\n",
        "# 85 characters, 7794 time samples per character, 64 channels.\n",
        "raw_signal = data['Signal']\n",
        "\n",
        "# RESHAPE IS MANDATORY: Flatten characters and time into one long timeline\n",
        "# New shape: (662490, 64)\n",
        "eeg_signal = raw_signal.reshape(-1, raw_signal.shape[-1])\n",
        "\n",
        "# Flatten these so their indices (0 to 662489) match the reshaped signal\n",
        "flashing = data['Flashing'].flatten()\n",
        "stimulus_type = data['StimulusType'].flatten()\n",
        "\n",
        "fs = 240\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qbXhiaPuyrS",
        "outputId": "441d7d31-f8c6-43bf-ae77-7e56e4f8661b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X samples: 15299, y samples: 15299\n"
          ]
        }
      ],
      "source": [
        "def extract_epochs(signal, stimulus_onsets, labels, fs, t_start=0.0, t_end=0.8):\n",
        "    \"\"\"\n",
        "    Extract EEG epochs around each stimulus onset.\n",
        "\n",
        "    Parameters:\n",
        "    - signal: continuous EEG array of shape (time, channels)\n",
        "    - stimulus_onsets: indices where stimuli occur\n",
        "    - labels: target/non-target labels per stimulus\n",
        "    - fs: sampling frequency in Hz\n",
        "    - t_start: start time (seconds) relative to stimulus\n",
        "    - t_end: end time (seconds) relative to stimulus\n",
        "\n",
        "    Returns:\n",
        "    - epochs: array of shape (num_trials, channels, time)\n",
        "    - y: corresponding labels\n",
        "    \"\"\"\n",
        "    # Ensure 1D arrays\n",
        "    stimulus_onsets = stimulus_onsets.flatten()\n",
        "    labels = labels.flatten()\n",
        "\n",
        "    start_samp = int(t_start * fs)\n",
        "    end_samp = int(t_end * fs)\n",
        "    expected_len = end_samp - start_samp\n",
        "\n",
        "    # Find where the flash begins\n",
        "    onsets = np.where(np.diff(stimulus_onsets.astype(int)) == 1)[0] + 1\n",
        "\n",
        "    epochs = []\n",
        "    y = []\n",
        "\n",
        "    for idx in onsets:\n",
        "        start = idx + start_samp\n",
        "        end = idx + end_samp\n",
        "\n",
        "        # This check failed before because signal.shape[0] was 85.\n",
        "        # Now signal.shape[0] is 662,490, so it will PASS.\n",
        "        if start >= 0 and end <= signal.shape[0]:\n",
        "            epoch = signal[start:end, :].T # Result: (Channels, Time)\n",
        "\n",
        "            if epoch.shape[1] == expected_len:\n",
        "                epochs.append(epoch)\n",
        "                y.append(labels[idx])\n",
        "\n",
        "    return np.array(epochs), np.array(y)\n",
        "\n",
        "# Execute extraction\n",
        "X, y = extract_epochs(eeg_signal, flashing, stimulus_type, fs)\n",
        "\n",
        "# Verify counts before moving to Part 4\n",
        "print(f\"X samples: {len(X)}, y samples: {len(y)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_I_ftgpuzT7",
        "outputId": "134069a7-e63f-4c3f-bfc8-0df582c703e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New X_prepared shape: (15299, 64, 192, 1)\n"
          ]
        }
      ],
      "source": [
        "def prepare_for_eegnet(epochs):\n",
        "    \"\"\"\n",
        "    Prepare EEG epochs for input into EEGNet.\n",
        "\n",
        "    Expected input shape: (trials, channels, time)\n",
        "    Expected output shape: (trials, 1, channels, time)\n",
        "    \"\"\"\n",
        "    # TODO: Add singleton dimension required by Conv2D\n",
        "    # Add the singleton dimension (kernels/channels_first)\n",
        "    # Resulting shape: (trials, 1, channels, time)\n",
        "    return np.expand_dims(epochs, axis=-1)\n",
        "\n",
        "X_prepared = prepare_for_eegnet(X)\n",
        "print(f\"New X_prepared shape: {X_prepared.shape}\")\n",
        "# Hint: Use numpy.expand_dims"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTGrqc-_u5Zf"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Conv2D, DepthwiseConv2D, SeparableConv2D,\n",
        "    BatchNormalization, AveragePooling2D, Dropout, Flatten, Dense, Activation\n",
        ")\n",
        "from tensorflow.keras.constraints import max_norm\n",
        "\n",
        "# Set the image data format globally for Keras backend\n",
        "tf.keras.backend.set_image_data_format('channels_first')\n",
        "\n",
        "def EEGNet(nb_classes, Chans, Samples, F1=8, D=2, F2=16, dropoutRate=0.5):\n",
        "    \"\"\"\n",
        "    EEGNet architecture.\n",
        "\n",
        "    Parameters:\n",
        "    - nb_classes: number of output classes\n",
        "    - Chans: number of EEG channels\n",
        "    - Samples: number of time samples per epoch\n",
        "    - F1: number of temporal filters\n",
        "    - D: depth multiplier for spatial filters\n",
        "    - F2: number of pointwise filters\n",
        "    \"\"\"\n",
        "    # Switching to channels_last to solve the NHWC error\n",
        "    input_format = 'channels_last'\n",
        "    bn_axis = -1 # Matches the last dimension\n",
        "\n",
        "    # New input shape: (64, 192, 1)\n",
        "    inputs = Input(shape=(Chans, Samples, 1))\n",
        "\n",
        "    # Block 1: Temporal Convolution\n",
        "    # We convolve across the 'Samples' (192) dimension\n",
        "    block1 = Conv2D(F1, (1, 120), padding='same', use_bias=False, data_format=input_format)(inputs)\n",
        "    block1 = BatchNormalization(axis=bn_axis)(block1)\n",
        "\n",
        "    # Block 1: Spatial Convolution (Depthwise)\n",
        "    # We convolve across the 'Chans' (64) dimension\n",
        "    block1 = DepthwiseConv2D((Chans, 1), use_bias=False,\n",
        "                               depth_multiplier=D,\n",
        "                               depthwise_constraint=max_norm(1.),\n",
        "                               data_format=input_format)(block1)\n",
        "    block1 = BatchNormalization(axis=bn_axis)(block1)\n",
        "    block1 = Activation('elu')(block1)\n",
        "    block1 = AveragePooling2D((1, 4), data_format=input_format)(block1)\n",
        "    block1 = Dropout(dropoutRate)(block1)\n",
        "\n",
        "    # Block 2: Separable Convolution\n",
        "    block2 = SeparableConv2D(F2, (1, 16), use_bias=False, padding='same', data_format=input_format)(block1)\n",
        "    block2 = BatchNormalization(axis=bn_axis)(block2)\n",
        "    block2 = Activation('elu')(block2)\n",
        "    block2 = AveragePooling2D((1, 8), data_format=input_format)(block2)\n",
        "    block2 = Dropout(dropoutRate)(block2)\n",
        "\n",
        "    # Classification\n",
        "    flatten = Flatten()(block2)\n",
        "    dense = Dense(nb_classes, name='dense', kernel_constraint=max_norm(0.25))(flatten)\n",
        "    softmax = Activation('softmax', name='softmax')(dense)\n",
        "\n",
        "    return Model(inputs=inputs, outputs=softmax)\n",
        "\n",
        "# Instantiate with the same parameters\n",
        "model = EEGNet(nb_classes=2, Chans=64, Samples=192)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aV0G_QkOu6Br",
        "outputId": "8afd4ae0-6a6a-4361-e11c-7cd3bd099217"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (12239, 64, 192, 1)\n",
            "y_train_cat shape: (12239, 2)\n",
            "X_train shape: (12239, 64, 192, 1)\n",
            "Model expected input: (None, 64, 192, 1)\n",
            "Epoch 1/50\n",
            "\u001b[1m765/765\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m365s\u001b[0m 473ms/step - accuracy: 0.8046 - loss: 0.5045 - val_accuracy: 0.8363 - val_loss: 0.4251\n",
            "Epoch 2/50\n",
            "\u001b[1m765/765\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m377s\u001b[0m 467ms/step - accuracy: 0.8339 - loss: 0.4207 - val_accuracy: 0.8350 - val_loss: 0.4093\n",
            "Epoch 3/50\n",
            "\u001b[1m765/765\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m360s\u001b[0m 471ms/step - accuracy: 0.8385 - loss: 0.3999 - val_accuracy: 0.8389 - val_loss: 0.4048\n",
            "Epoch 4/50\n",
            "\u001b[1m765/765\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m381s\u001b[0m 470ms/step - accuracy: 0.8429 - loss: 0.3935 - val_accuracy: 0.8382 - val_loss: 0.4049\n",
            "Epoch 5/50\n",
            "\u001b[1m765/765\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m349s\u001b[0m 456ms/step - accuracy: 0.8386 - loss: 0.3979 - val_accuracy: 0.8444 - val_loss: 0.3961\n",
            "Epoch 6/50\n",
            "\u001b[1m765/765\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m358s\u001b[0m 468ms/step - accuracy: 0.8326 - loss: 0.4010 - val_accuracy: 0.8402 - val_loss: 0.3972\n",
            "Epoch 7/50\n",
            "\u001b[1m765/765\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m380s\u001b[0m 466ms/step - accuracy: 0.8356 - loss: 0.3954 - val_accuracy: 0.8402 - val_loss: 0.3923\n",
            "Epoch 8/50\n",
            "\u001b[1m765/765\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m359s\u001b[0m 469ms/step - accuracy: 0.8441 - loss: 0.3828 - val_accuracy: 0.8438 - val_loss: 0.3852\n",
            "Epoch 9/50\n",
            "\u001b[1m765/765\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m372s\u001b[0m 455ms/step - accuracy: 0.8416 - loss: 0.3798 - val_accuracy: 0.8418 - val_loss: 0.3907\n",
            "Epoch 10/50\n",
            "\u001b[1m765/765\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m358s\u001b[0m 467ms/step - accuracy: 0.8450 - loss: 0.3706 - val_accuracy: 0.8428 - val_loss: 0.3921\n",
            "Epoch 11/50\n",
            "\u001b[1m765/765\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m375s\u001b[0m 459ms/step - accuracy: 0.8439 - loss: 0.3757 - val_accuracy: 0.8461 - val_loss: 0.3807\n",
            "Epoch 12/50\n",
            "\u001b[1m765/765\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m360s\u001b[0m 470ms/step - accuracy: 0.8478 - loss: 0.3778 - val_accuracy: 0.8448 - val_loss: 0.3886\n",
            "Epoch 13/50\n",
            "\u001b[1m765/765\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m381s\u001b[0m 469ms/step - accuracy: 0.8397 - loss: 0.3733 - val_accuracy: 0.8382 - val_loss: 0.3882\n",
            "Epoch 14/50\n",
            "\u001b[1m765/765\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m359s\u001b[0m 469ms/step - accuracy: 0.8427 - loss: 0.3834 - val_accuracy: 0.8428 - val_loss: 0.3830\n",
            "Epoch 15/50\n",
            "\u001b[1m765/765\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m382s\u001b[0m 469ms/step - accuracy: 0.8453 - loss: 0.3776 - val_accuracy: 0.8441 - val_loss: 0.3807\n",
            "Epoch 16/50\n",
            "\u001b[1m765/765\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m384s\u001b[0m 471ms/step - accuracy: 0.8453 - loss: 0.3692 - val_accuracy: 0.8402 - val_loss: 0.3813\n",
            "Epoch 17/50\n",
            "\u001b[1m765/765\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m349s\u001b[0m 456ms/step - accuracy: 0.8479 - loss: 0.3638 - val_accuracy: 0.8435 - val_loss: 0.3800\n",
            "Epoch 18/50\n",
            "\u001b[1m765/765\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m360s\u001b[0m 471ms/step - accuracy: 0.8547 - loss: 0.3602 - val_accuracy: 0.8458 - val_loss: 0.3801\n",
            "Epoch 19/50\n",
            "\u001b[1m765/765\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m381s\u001b[0m 470ms/step - accuracy: 0.8502 - loss: 0.3640 - val_accuracy: 0.8444 - val_loss: 0.3825\n",
            "Epoch 20/50\n",
            "\u001b[1m765/765\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m360s\u001b[0m 470ms/step - accuracy: 0.8493 - loss: 0.3593 - val_accuracy: 0.8438 - val_loss: 0.3795\n",
            "Epoch 21/50\n",
            "\u001b[1m765/765\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m349s\u001b[0m 455ms/step - accuracy: 0.8463 - loss: 0.3638 - val_accuracy: 0.8458 - val_loss: 0.3742\n",
            "Epoch 22/50\n",
            "\u001b[1m765/765\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m359s\u001b[0m 470ms/step - accuracy: 0.8514 - loss: 0.3600 - val_accuracy: 0.8458 - val_loss: 0.3789\n",
            "Epoch 23/50\n",
            "\u001b[1m765/765\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m357s\u001b[0m 467ms/step - accuracy: 0.8411 - loss: 0.3757 - val_accuracy: 0.8458 - val_loss: 0.3814\n",
            "Epoch 24/50\n",
            "\u001b[1m765/765\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m350s\u001b[0m 458ms/step - accuracy: 0.8448 - loss: 0.3689 - val_accuracy: 0.8425 - val_loss: 0.3845\n",
            "Epoch 25/50\n",
            "\u001b[1m765/765\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m386s\u001b[0m 464ms/step - accuracy: 0.8472 - loss: 0.3651 - val_accuracy: 0.8435 - val_loss: 0.3804\n",
            "Epoch 26/50\n",
            "\u001b[1m765/765\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m350s\u001b[0m 458ms/step - accuracy: 0.8494 - loss: 0.3659 - val_accuracy: 0.8441 - val_loss: 0.3787\n",
            "Epoch 27/50\n",
            "\u001b[1m765/765\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m388s\u001b[0m 466ms/step - accuracy: 0.8505 - loss: 0.3554 - val_accuracy: 0.8451 - val_loss: 0.3797\n",
            "Epoch 28/50\n",
            "\u001b[1m765/765\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m372s\u001b[0m 454ms/step - accuracy: 0.8460 - loss: 0.3652 - val_accuracy: 0.8435 - val_loss: 0.3760\n",
            "Epoch 29/50\n",
            "\u001b[1m765/765\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m389s\u001b[0m 464ms/step - accuracy: 0.8469 - loss: 0.3603 - val_accuracy: 0.8415 - val_loss: 0.3796\n",
            "Epoch 30/50\n",
            "\u001b[1m765/765\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m347s\u001b[0m 454ms/step - accuracy: 0.8492 - loss: 0.3638 - val_accuracy: 0.8451 - val_loss: 0.3766\n",
            "Epoch 31/50\n",
            "\u001b[1m765/765\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m357s\u001b[0m 467ms/step - accuracy: 0.8500 - loss: 0.3564 - val_accuracy: 0.8484 - val_loss: 0.3726\n",
            "Epoch 32/50\n",
            "\u001b[1m765/765\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m347s\u001b[0m 454ms/step - accuracy: 0.8583 - loss: 0.3543 - val_accuracy: 0.8461 - val_loss: 0.3797\n",
            "Epoch 33/50\n",
            "\u001b[1m765/765\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m348s\u001b[0m 454ms/step - accuracy: 0.8509 - loss: 0.3590 - val_accuracy: 0.8438 - val_loss: 0.3759\n",
            "Epoch 34/50\n",
            "\u001b[1m765/765\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m381s\u001b[0m 454ms/step - accuracy: 0.8563 - loss: 0.3519 - val_accuracy: 0.8435 - val_loss: 0.3755\n",
            "Epoch 35/50\n",
            "\u001b[1m765/765\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m356s\u001b[0m 466ms/step - accuracy: 0.8498 - loss: 0.3675 - val_accuracy: 0.8431 - val_loss: 0.3782\n",
            "Epoch 36/50\n",
            "\u001b[1m765/765\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m383s\u001b[0m 466ms/step - accuracy: 0.8476 - loss: 0.3682 - val_accuracy: 0.8418 - val_loss: 0.3788\n",
            "Epoch 37/50\n",
            "\u001b[1m765/765\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m348s\u001b[0m 455ms/step - accuracy: 0.8463 - loss: 0.3685 - val_accuracy: 0.8435 - val_loss: 0.3814\n",
            "Epoch 38/50\n",
            "\u001b[1m765/765\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m391s\u001b[0m 467ms/step - accuracy: 0.8504 - loss: 0.3614 - val_accuracy: 0.8477 - val_loss: 0.3752\n",
            "Epoch 39/50\n",
            "\u001b[1m765/765\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m351s\u001b[0m 459ms/step - accuracy: 0.8513 - loss: 0.3576 - val_accuracy: 0.8441 - val_loss: 0.3743\n",
            "Epoch 40/50\n",
            "\u001b[1m765/765\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m360s\u001b[0m 471ms/step - accuracy: 0.8596 - loss: 0.3497 - val_accuracy: 0.8474 - val_loss: 0.3715\n",
            "Epoch 41/50\n",
            "\u001b[1m765/765\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m348s\u001b[0m 455ms/step - accuracy: 0.8470 - loss: 0.3669 - val_accuracy: 0.8451 - val_loss: 0.3770\n",
            "Epoch 42/50\n",
            "\u001b[1m765/765\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m383s\u001b[0m 457ms/step - accuracy: 0.8559 - loss: 0.3537 - val_accuracy: 0.8441 - val_loss: 0.3767\n",
            "Epoch 43/50\n",
            "\u001b[1m765/765\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m359s\u001b[0m 470ms/step - accuracy: 0.8547 - loss: 0.3522 - val_accuracy: 0.8435 - val_loss: 0.3754\n",
            "Epoch 44/50\n",
            "\u001b[1m765/765\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m358s\u001b[0m 468ms/step - accuracy: 0.8515 - loss: 0.3549 - val_accuracy: 0.8451 - val_loss: 0.3750\n",
            "Epoch 45/50\n",
            "\u001b[1m765/765\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m359s\u001b[0m 469ms/step - accuracy: 0.8626 - loss: 0.3430 - val_accuracy: 0.8454 - val_loss: 0.3770\n",
            "Epoch 46/50\n",
            "\u001b[1m765/765\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m379s\u001b[0m 466ms/step - accuracy: 0.8509 - loss: 0.3559 - val_accuracy: 0.8500 - val_loss: 0.3783\n",
            "Epoch 47/50\n",
            "\u001b[1m765/765\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m350s\u001b[0m 457ms/step - accuracy: 0.8537 - loss: 0.3478 - val_accuracy: 0.8415 - val_loss: 0.3845\n",
            "Epoch 48/50\n",
            "\u001b[1m765/765\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m390s\u001b[0m 468ms/step - accuracy: 0.8538 - loss: 0.3585 - val_accuracy: 0.8464 - val_loss: 0.3755\n",
            "Epoch 49/50\n",
            "\u001b[1m765/765\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m385s\u001b[0m 471ms/step - accuracy: 0.8539 - loss: 0.3585 - val_accuracy: 0.8490 - val_loss: 0.3733\n",
            "Epoch 50/50\n",
            "\u001b[1m765/765\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m370s\u001b[0m 456ms/step - accuracy: 0.8601 - loss: 0.3416 - val_accuracy: 0.8428 - val_loss: 0.3743\n"
          ]
        }
      ],
      "source": [
        "# TODO: Split the dataset into training and validation sets\n",
        "# TODO: Compile the model with an appropriate loss and optimizer\n",
        "# Hint: Use categorical cross-entropy and Adam optimizer\n",
        "# TODO: Train the model and store the training history\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Split data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_prepared, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train_cat = to_categorical(y_train, num_classes=2)\n",
        "y_val_cat = to_categorical(y_val, num_classes=2)\n",
        "# Double check shapes\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"y_train_cat shape: {y_train_cat.shape}\")\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"Model expected input: {model.input_shape}\")\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Use a smaller batch size initially to isolate memory errors\n",
        "history = model.fit(X_train, y_train_cat,\n",
        "                    batch_size=16,\n",
        "                    epochs=50,\n",
        "                    validation_data=(X_val, y_val_cat),\n",
        "                    verbose=1)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}