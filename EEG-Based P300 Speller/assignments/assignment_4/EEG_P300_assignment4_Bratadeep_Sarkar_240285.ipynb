{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bratadeepsarkar123/Winter-projects-25-26/blob/main/EEG-Based%20P300%20Speller/assignments/assignment_4/EEG_P300_assignment4_Bratadeep_Sarkar_240285.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25207d05"
      },
      "source": [
        "# Assignment: Training EEGNet on P300 EEG Data\n",
        "\n",
        "In this assignment, you will work with real EEG data from a P300 speller experiment and implement the EEGNet architecture to detect P300 responses. The emphasis of this assignment is on understanding and implementing the EEGNet model rather than extensive signal preprocessing.\n",
        "\n",
        "**Instructions:**\n",
        "- Complete the provided code scaffolding\n",
        "- Fill in missing logic where indicated\n",
        "- Focus especially on the EEGNet architecture and training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cebbfa87"
      },
      "source": [
        "## Part 1: Loading and Inspecting the Dataset\n",
        "\n",
        "In this section, you will load the EEG dataset and inspect its basic structure. The dataset contains continuous EEG recordings along with stimulus and label information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "c5e1539f",
        "outputId": "3fe429e9-812f-4ba4-ccf2-cc368eb9c88f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'Subject_A_Train.mat'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/io/matlab/_mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Subject_A_Train.mat'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3704154587.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# TODO: Update the path if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Subject_A_Train.mat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/io/matlab/_mio.py\u001b[0m in \u001b[0;36mloadmat\u001b[0;34m(file_name, mdict, appendmat, spmatrix, **kwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m     \"\"\"\n\u001b[1;32m    232\u001b[0m     \u001b[0mvariable_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'variable_names'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m         \u001b[0mMR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmat_reader_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mmatfile_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/io/matlab/_mio.py\u001b[0m in \u001b[0;36m_open_file_context\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/io/matlab/_mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mappendmat\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.mat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mfile_like\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'.mat'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             raise OSError(\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Subject_A_Train.mat'"
          ]
        }
      ],
      "source": [
        "import scipy.io as sio\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "# TODO: Update the path if needed\n",
        "data = sio.loadmat('Subject_A_Train.mat')\n",
        "\n",
        "\n",
        "print(data.keys())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb60b42b"
      },
      "source": [
        "## Part 2: Understanding the Experimental Design\n",
        "\n",
        "The P300 speller paradigm is based on detecting brain responses to rare target stimuli. In this section, you will identify how stimulus timing and labels are encoded in the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mW_rcsF3AjHu"
      },
      "outputs": [],
      "source": [
        "if 'data' in locals():\n",
        "    # 1. Continuous EEG signal\n",
        "\n",
        "    raw_eeg = data['Signal']\n",
        "\n",
        "    # 2. Stimulus onset information\n",
        "    flashing = data['Flashing']\n",
        "\n",
        "    # 3. Target vs non-target labels\n",
        "    stimulus_type = data['StimulusType']\n",
        "\n",
        "\n",
        "    fs = 240.0\n",
        "\n",
        "    print(f\"EEG Shape: {raw_eeg.shape}\")\n",
        "    print(f\"Flashing Shape: {flashing.shape}\")\n",
        "    print(f\"Stimulus Type Shape: {stimulus_type.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0b2e84b"
      },
      "source": [
        "## Part 3: EEG Epoch Extraction\n",
        "\n",
        "EEGNet does not operate on continuous EEG. Instead, the signal must be segmented into short epochs following each stimulus. This step converts raw EEG into trials suitable for supervised learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DmjqGOHAkbS"
      },
      "outputs": [],
      "source": [
        "def extract_epochs(signal, flashing, stimulus_type, fs, t_start=0.0, t_end=0.8):\n",
        "    \"\"\"\n",
        "    Extract epochs from 3D BCI Competition data (Trials x Time x Channels).\n",
        "    \"\"\"\n",
        "\n",
        "    t_min_samples = int(t_start * fs)\n",
        "    t_max_samples = int(t_end * fs)\n",
        "\n",
        "    epochs = []\n",
        "    labels = []\n",
        "\n",
        "\n",
        "    if signal.ndim == 3:\n",
        "        num_runs = signal.shape[0]\n",
        "        print(f\"Processing {num_runs} runs...\")\n",
        "\n",
        "        for run_idx in range(num_runs):\n",
        "\n",
        "            run_signal = signal[run_idx, :, :]\n",
        "            run_flashing = flashing[run_idx, :]\n",
        "            run_stim_type = stimulus_type[run_idx, :]\n",
        "\n",
        "\n",
        "            diff = np.diff(run_flashing, prepend=0)\n",
        "            starts = np.where(diff == 1)[0]\n",
        "\n",
        "            for start_idx in starts:\n",
        "                end_idx = start_idx + t_max_samples\n",
        "\n",
        "                if end_idx < run_signal.shape[0]:\n",
        "\n",
        "                    epoch = run_signal[start_idx + t_min_samples : end_idx, :]\n",
        "\n",
        "\n",
        "                    epoch = epoch.T\n",
        "\n",
        "                    epochs.append(epoch)\n",
        "                    labels.append(run_stim_type[start_idx])\n",
        "\n",
        "    elif signal.ndim == 2:\n",
        "\n",
        "        diff = np.diff(flashing.flatten(), prepend=0)\n",
        "        starts = np.where(diff == 1)[0]\n",
        "        for start_idx in starts:\n",
        "            if start_idx + t_max_samples < signal.shape[0]:\n",
        "                epoch = signal[start_idx + t_min_samples : start_idx + t_max_samples, :].T\n",
        "                epochs.append(epoch)\n",
        "                labels.append(stimulus_type.flatten()[start_idx])\n",
        "\n",
        "    return np.array(epochs), np.array(labels)\n",
        "\n",
        "\n",
        "if 'data' in locals():\n",
        "    X, Y = extract_epochs(raw_eeg, flashing, stimulus_type, fs)\n",
        "    print(f\"Extracted X Shape: {X.shape}\")\n",
        "    print(f\"Extracted Y Shape: {Y.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24965719"
      },
      "source": [
        "## Part 4: Preparing Data for EEGNet\n",
        "\n",
        "In this section, you will perform minimal preprocessing to make the data compatible with EEGNet. Extensive signal processing is not required."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ju9HiaQAmN3"
      },
      "outputs": [],
      "source": [
        "def prepare_for_eegnet(epochs):\n",
        "    \"\"\"\n",
        "    Prepare EEG epochs for input into EEGNet.\n",
        "\n",
        "    Input: (Trials, Channels, Time)\n",
        "    Output: (Trials, Channels, Time, 1) -- Standard Image Format\n",
        "    \"\"\"\n",
        "\n",
        "    return np.expand_dims(epochs, axis=-1)\n",
        "\n",
        "\n",
        "if 'X' in locals():\n",
        "    X_prepared = prepare_for_eegnet(X)\n",
        "    print(f\"Prepared Data Shape (Channels Last): {X_prepared.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c11d7e4a"
      },
      "source": [
        "## Part 5: Implementing EEGNet\n",
        "\n",
        "This is the core part of the assignment. You will implement the EEGNet architecture as discussed in class. Focus on matching the block structure and understanding the role of each layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQ-SlB-8Anhd"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (Input, Conv2D, DepthwiseConv2D,\n",
        "                                     SeparableConv2D, BatchNormalization,\n",
        "                                     AveragePooling2D, Dropout, Flatten, Dense, Activation)\n",
        "from tensorflow.keras.constraints import max_norm\n",
        "\n",
        "def EEGNet(nb_classes, Chans, Samples, F1=8, D=2, F2=16, dropoutRate=0.5):\n",
        "    \"\"\"\n",
        "    EEGNet architecture (Channels Last Version).\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Input Shape Change: (Chans, Samples, 1)\n",
        "    input1 = Input(shape=(Chans, Samples, 1))\n",
        "\n",
        "\n",
        "    block1 = Conv2D(F1, (1, 64), padding='same',\n",
        "                    input_shape=(Chans, Samples, 1),\n",
        "                    use_bias=False)(input1)\n",
        "\n",
        "\n",
        "    block1 = BatchNormalization(axis=-1)(block1)\n",
        "\n",
        "\n",
        "    # Block 1: Spatial Convolution (Depthwise)\n",
        "\n",
        "\n",
        "    block1 = DepthwiseConv2D((Chans, 1), use_bias=False,\n",
        "                             depth_multiplier=D,\n",
        "                             depthwise_constraint=max_norm(1.))(block1)\n",
        "\n",
        "    block1 = BatchNormalization(axis=-1)(block1)\n",
        "    block1 = Activation('elu')(block1)\n",
        "\n",
        "\n",
        "    block1 = AveragePooling2D((1, 4))(block1)\n",
        "    block1 = Dropout(dropoutRate)(block1)\n",
        "\n",
        "\n",
        "    # Block 2: Separable Convolution\n",
        "\n",
        "    block2 = SeparableConv2D(F2, (1, 16), use_bias=False,\n",
        "                             padding='same')(block1)\n",
        "\n",
        "    block2 = BatchNormalization(axis=-1)(block2)\n",
        "    block2 = Activation('elu')(block2)\n",
        "\n",
        "    # Pooling\n",
        "    block2 = AveragePooling2D((1, 8))(block2)\n",
        "    block2 = Dropout(dropoutRate)(block2)\n",
        "\n",
        "\n",
        "    # Classification\n",
        "\n",
        "    flatten = Flatten(name='flatten')(block2)\n",
        "\n",
        "    # Keep kernel_constraint (Keras 3 fix)\n",
        "    dense = Dense(nb_classes, name='dense', kernel_constraint=max_norm(0.25))(flatten)\n",
        "    softmax = Activation('softmax', name='softmax')(dense)\n",
        "\n",
        "    return Model(inputs=input1, outputs=softmax)\n",
        "\n",
        "\n",
        "if 'X_prepared' in locals():\n",
        "    # Check shape (N, C, T, 1)\n",
        "    if X_prepared.shape[-1] == 1:\n",
        "        trials, chans, samples, kernels = X_prepared.shape\n",
        "        print(f\"Building Model (Channels Last): Chans={chans}, Samples={samples}\")\n",
        "\n",
        "        model = EEGNet(nb_classes=2, Chans=chans, Samples=samples)\n",
        "        model.summary()\n",
        "    else:\n",
        "        print(\"Error: X_prepared shape is wrong. Please run the NEW Part 4 code above.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3b44b13"
      },
      "source": [
        "## Part 6: Training the Model\n",
        "\n",
        "In this section, you will train EEGNet to distinguish between P300 and non-P300 EEG epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56yasYVjApGf"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.utils import class_weight\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if 'X_prepared' in locals():\n",
        "    # 1. Prepare Labels\n",
        "    Y_cat = to_categorical(Y)\n",
        "\n",
        "    # 2. Split Data\n",
        "    x_train, x_val, y_train, y_val = train_test_split(\n",
        "        X_prepared, Y_cat, test_size=0.2, random_state=42, stratify=Y_cat\n",
        "    )\n",
        "\n",
        "    print(f\"Training Shape: {x_train.shape}\") # Should be (N, 64, 192, 1)\n",
        "\n",
        "    # 3. Calculate Class Weights\n",
        "    y_integers = np.argmax(Y_cat, axis=1)\n",
        "    class_weights_vals = class_weight.compute_class_weight(\n",
        "        class_weight='balanced',\n",
        "        classes=np.unique(y_integers),\n",
        "        y=y_integers\n",
        "    )\n",
        "    class_weights = dict(enumerate(class_weights_vals))\n",
        "\n",
        "    # 4. Compile\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=Adam(learning_rate=0.001),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # 5. Train\n",
        "    print(\"Starting training...\")\n",
        "    history = model.fit(\n",
        "        x_train, y_train,\n",
        "        batch_size=16,\n",
        "        epochs=20,\n",
        "        validation_data=(x_val, y_val),\n",
        "        class_weight=class_weights,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # 6. Plot\n",
        "    plt.plot(history.history['accuracy'], label='Train')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation')\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iLfwAI3eCAY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}